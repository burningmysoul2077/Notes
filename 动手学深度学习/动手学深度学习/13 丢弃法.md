* [前言](#前言)
* [动机](#动机)
  * [重新审视过拟合](#重新审视过拟合)
  * [扰动的稳健性](#扰动的稳健性)
    * [什么是一个“好”的预测模型](#什么是一个好的预测模型)
* [丢弃法](#丢弃法)
  * [无偏差的加入噪音](#无偏差的加入噪音)
  * [使用丢弃法](#使用丢弃法)
    * [训练中的丢弃法](#训练中的丢弃法)
    * [推理中的丢弃法](#推理中的丢弃法)
* [总结](#总结)
* [丢弃法从零实现和简洁实现](#丢弃法从零实现和简洁实现)
* [12 丢弃法 Q&A](#12-丢弃法-qa)


# 前言

>   在上一节 权重衰退 中， 介绍了通过惩罚权重的 𝐿2 范数来正则化统计模型的经典方法。
>   在概率角度看，我们可以通过以下论证来证明这一技术的合理性： 我们已经假设了一个先验，即权重的值取自均值为0的高斯分布。 更直观的是，我们希望模型深度挖掘特征，即将其权重分散到许多特征中， 而不是过于依赖少数潜在的虚假关联


# 动机

## 重新审视过拟合

-  当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 
-  不幸的是，线性模型泛化的可靠性是有代价的，这是说，线性模型没有考虑到特征之间的交互作用。
-  对于每个特征，线性模型必须指定 _正的_ 或 _负的_ 权重，而忽略其他特征。

-  泛化性和灵活性之间的这种基本权衡被描述为 _偏差-方差权衡 bias-variance tradeoff_ 。 
	- 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果
	- 深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。

-  即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。 
-  深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题

## 扰动的稳健性

### 什么是一个“好”的预测模型

-  期待“好”的预测模型能在未知的数据上有很好的表现： 经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标。
-  简单性
	-  一种角度是以较小维度的形式展现， 比如线性模型的单项式函数。 
		-  权重衰减中， 参数的范数也代表了一种有用的简单性度量。
	- 另一个角度是平滑性，即函数不应该对其输入的微小变化敏感

-  总结下来，一个好的模型需要对输入数据的扰动 *鲁棒*
	-  使用有噪音的数据等价于 Tikhonov 正则
	-  *丢弃法*：在层之间加入噪音
		- 因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性

- 吉洪诺夫正则化 Tikhonov Regularization，又被叫作 岭回归 Ridge Regression

# 丢弃法

-  **丢弃法 Dropout**，在训练过程中，在计算后续层之前向网络的每一层注入噪声
-  丢弃法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术。 
-  之所以被称为 _丢弃法_，因为从表面上看是在训练过程中丢弃（drop out）一些神经元。 
	-  在整个训练过程的每一次迭代中，*标准暂退法* 包括在计算下一层之前将当前层中的一些节点置零。

- 那么关键的挑战就是如何注入这种噪声。 一种想法是以一种 _无偏向 unbiased_ 的方式注入噪声。 
- 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。

## 无偏差的加入噪音

-  对 $\textbf x$ 加入噪音得到 $\textbf x'$， 我们希望  $\rm E[\textbf x'] = \textbf x$
-  *标准丢弃法正则化* 对每个元素进行如下扰动
	-  $x_i' = \begin{cases}  0 & \text{ with probablity } p \\   \frac{x_i}{1-p} & \text{otherwise} \end{cases}$

-  根据此设计，其期望值保持不变，$\rm E[\textbf x'] = p * 0 + (1 - p) * \frac{x_i}{1 - p}$

## 使用丢弃法

### 训练中的丢弃法

-   通常将丢弃法作用在隐藏全连接层的输出上

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230325173858.png)

- 被 dropout的隐藏层节点，输出的计算不再依赖它们，并且它们各自的梯度在执行反向传播时也会消失


### 推理中的丢弃法

-  正则项仅在训练中使用：他们影响模型参数的更新

-  在推理过程中，丢弃法直接返回输入
	-  给定一个训练好的模型和一个新的样本，我们不会丢弃任何节点，因此不需要标准化
	-  $\textbf h = \text {dropout}(\textbf h)$
	-  这样也能保证确定性的输出

- 然而也有一些例外：一些研究人员在测试时使用暂退法， 用于估计神经网络预测的“不确定性” 
- 如果通过许多不同的暂退法遮盖后得到的预测结果都是一致的，那么我们可以说网络发挥更稳定。

# 总结
-  本节介绍了丢弃法，首先通过如何定义一个好模型引出，丢弃法的由来
-  介绍了标准丢弃法正则化的公示，就是将一些输出项随机置0来控制模型复杂度，丢弃概率是控制模型复杂度的超参数（常取 0.9，0.5，0.1）
-  介绍了丢弃法的使用，常作用在多层感知机的隐藏层输出上，而且只用在训练时

------

# 丢弃法从零实现和简洁实现

[13 丢弃法的代码实现.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13%20%E4%B8%A2%E5%BC%83%E6%B3%95%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.ipynb)

------

# 12 丢弃法 Q&A

- `dropout随机置零对求梯度和反向传播的影响是什么？`
>  随机置零那么梯度也会变成0，没有置零的梯度也会相应乘以一个数

- `丢弃法的丢弃依据是什么？如果丢弃不合理对输出的结果影响会很大`
>可以简单认为就是一个正则项。丢弃不合理可以认为就是丢弃概率设定有误，太小了，正则效果不够；太大了，就会欠拟合。

- `dropout随机丢弃，如何保证结果的正确性和可重复性？`
>  这是两个问题，实际上，正确性在机器学习中难以保证，因为即使代码中有bug，从结果上是很难看出来的。
>  神经网络的可重复性是很难保证的，而对于丢弃法，如果固定随机种子，重复十次，结果也应该是差不多的。
>  还有一点，CUDA GPU在矩阵计算时，差距会比较大，每次都不一样。
>  最后，可重复性也没那么重要。随机性也某种程序反映模型的稳定性

- `丢弃法是每次迭代一次，随机丢弃一次吗`
>是的，每个层在调用前一项计算时会随机丢弃

- `请问老师，在使用BN的时候，还有必要使用dropout吗`
>  BN是给卷积层使用的，dropout是在全连接层使用。

- `dropout会不会让训练的loss曲线方差变大，不够平滑？`
>有可能是。但不用担心，曲线最终都会平滑，达到收敛。

- `老师，请问可以在解释一下为什么“推理中的dropout是直接返回输入”吗？`
>  如果模型权重不更新，就可以不使用dropout。dropout是一个正则项，作用是让你在更新权重的时候，让模型复杂度变低。而推理中，不需要改变模型复杂度。

- `老师，dropout每次随机选几个子网络，最后做平均的做法是不是类似于随机森林多决策树做投票的这种思想？`
> 是的

- `dropout和权重衰减都属于正则，为何dropout效果更好现在更常用呢？`
> 权重衰减更常用，对卷积等都能用。dropout主要用于全连接层。
> dropout可以认为比权重衰减更直观，调参更方便

- `Transformer可以看作是一种特殊的MLP吗？`
>现在主流是认为transformer是一种kernel machine
