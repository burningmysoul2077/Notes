# 1 深度学习在NLP

-  自然语言处理领域一直在发展变化，能够在各种任务上达到最优效果的模型、算法也层出不穷。
-  最早的范式是 文本表示+ 机器学习，如基础 Baseline 所演示的方法，通过将自然语言文本表示为数值向量，再建立统计机器学习模型实习下游任务。
-  但随着深度学习的发展，自 2013 年，神经网络词向量登上时代舞台，神经网络逐渐成为了  NLP 的核心方法，NLP 的核心研究范式逐渐向深度学习演化。

![[Pasted image 20230822142546.png]]

## 1.1 文本表示 + 统计学习

-  见任务一 和 baseline

## 1.2 神经网络词向量

-  _深度学习的研究方法，主要是通过多层的神经网络来端到端处理下游任务_，将文本表示、特征工程、建模预测都融合在深度神经网络中，减少了人工特征构建的过程，显著提升了自然语言处理能力。神经网络词向量是其中的核心部分，即文本通过神经网络后的向量表示，这些向量表示能够蕴含深层语义且维度合适，后续研究往往可以直接使用以替代传统的文本表示方法，典型的应用如 Word2Vec 。

-  但是，Word2Vec 是静态词向量，即对于每一个词有一个固定的向量表示，无法解决一词多义、复杂特征等问题。

## 1.3 预训练词向量

-  2018年，ELMo 模型的提出拉开了动态词向量、预训练模型的时代大幕。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，_标志着 预训练+微调 范式的诞生_。

---

# 2 BERT 介绍

## 2.1 BERT 简介

-  BERT，是一个经典的深度学习、预训练模型。2018年，由 Google 团队发布的论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》提出了预训练模型 BERT（Bidirectional Encoder Representations from Transformers），在自然语言处理领域掀起了巨大浪潮。该模型实现了包括 GLUE、MultiNLI 等七个自然语言处理评测任务的 the-state-of-art（最优表现），堪称里程碑式的成果。_自 BERT 推出以来，预训练+微调的模式开始成为自然语言处理任务的主流_，标志着各种自然语言处理任务的重大进展以及预训练模型的统治地位建立，_一直到去年 ChatGPT 的发布将研究范式带向大语言模型+提示工程_，但时至今天，BERT 仍然是自然语言处理领域最常用、最重要的预训练模型之一。

![](https://oss.linklearner.com/AI%E5%A4%8F%E4%BB%A4%E8%90%A5/%E7%AC%AC%E4%B8%89%E6%9C%9FNLP/%E4%BB%BB%E5%8A%A1%E4%BA%8C2.png)


## 2.2 预训练 + 微调范式

-  _所谓预训练+微调范式，指先在海量文本数据上进行预训练，再针对特定的下游任务进行微调_。预训练一般基于语言模型，即给定上一个词，预测下一个词。语言模型可以在所有文本数据上建模，无需人工标注，因此很容易在海量数据上进行训练。通过在海量数据上进行预训练，模型可以学习到深层的自然语言逻辑。再通过在指定的下游任务上进行微调，即针对部分人工标注的任务数据进行特定训练，如文本分类、文本生成等，来训练模型执行下游任务的能力。

![[Pasted image 20230822143051.png]]

-  预训练+微调范式一定程度上缓解了标注数据昂贵的问题，显著提升了模型性能，但是，ELMo 使用的双向 LSTM 架构存在难以解决长期依赖、并行效果差的天生缺陷，ELMo 本身也保留了词向量作为特征输入的应用，并没能一锤定音地敲定预训练+微调范式的主流地位。2017年，Transformer 模型的提出，为自然语言处理领域带来了一个新的重要成员——Attention 架构。基于 Attention 架构，同样在2018年，OpenAI 提出的 GPT 模型基于 Transformer 模型，结合 ELMo 模型提出的预训练+微调范式，进一步刷新了众多自然语言处理任务的上限。2023年爆火出圈的 ChatGPT 就是以 GPT 模型作为基础架构的。

-  从静态编码到神经网络计算的静态词向量，再到基于双向 LSTM 架构的预训练+微调范式，又诞生了基于 Transformer的预训练+微调模式，预训练模型逐步成为自然语言处理的主流。但，真正奠定预训练+微调范式的重要地位的，还是之后提出的 BERT。BERT 可以说是综合了 ELMo 和 GPT，使用预训练+微调范式，基于 Transformer 架构而抛弃了存在天生缺陷的 LSTM，又针对 GPT 仅能够捕捉单向语句关系的缺陷提出了能够捕捉深层双向语义关系的 MLM 预训练任务，从而将预训练模型推向了一个高潮。

## 2.3 Transformer 与 Attention

-  BERT 乃至目前正火的 LLM 的成功，都离不开 Attention 机制与基于 Attention 机制搭建的 Transformer 架构。此处简单介绍 Transformer 与 Attention 机制。

-  __在 Attention 机制提出之前，深度学习主要有两种基础架构：卷积神经网络（CNN）与循环神经网络（RNN）__。其中，CNN 在 CV 领域表现突出，而 RNN 及其变体 LSTM 在 NLP 方向上一枝独秀。然而，RNN 架构存在两个天然缺陷：
	-  ① 序列依序计算的模式限制了计算机并行计算的能力，导致 RNN 为基础架构的模型虽然参数量不算特别大，但计算时间成本却很高。
	-  ② RNN 难以捕捉长序列的相关关系。在 RNN 架构中，距离越远的输入之间的关系就越难被捕捉，同时 RNN 需要将整个序列读入内存依次计算，也限制了序列的长度。

-  针对上述两个问题， 2017年 Vaswani 等人发表了论文《Attention Is All You Need》，创造性提出了 Attention 机制并完全抛弃了 RNN 架构。Attention 机制最先源于计算机视觉领域，其核心思想为当我们关注一张图片，我们往往无需看清楚全部内容而仅将注意力集中在重点部分即可。而在自然语言处理领域，我们往往也可以通过将重点注意力集中在一个或几个 token，从而取得更高效高质的计算效果。

-  Attention 机制的特点是通过计算 **Query** (查询值) 与 **Key** (键值)的相关性为真值加权求和，从而拟合序列中每个词同其他词的相关关系。其大致计算过程如图：

![[Pasted image 20230822144813.png]]

-   具体而言，可以简单理解为一个输入序列通过不同的参数矩阵映射为 Q、K、V 三个矩阵，其中，Q 是计算注意力的另一个句子（或词组），V 为待计算句子，K 为待计算句子中每个词的对应键。通过对 Q 和 K 做点积，可以得到待计算句子（V）的注意力分布（即哪些部分更重要，哪些部分没有这么重要），基于注意力分布对 V 做加权求和即可得到输入序列经过注意力计算后的输出，其中与 Q （即计算注意力的另一方）越重要的部分得到的权重就越高。

-  而 Transformer 正是基于 Attention 机制搭建了 Encoder-Decoder（编码器-解码器）结构，主要适用于 Seq2Seq（序列到序列）任务，即输入是一个自然语言序列，输出也是一个自然语言序列。其整体架构如下：

![[Pasted image 20230822145538.png]]

-  Transformer 由一个 Encoder，一个 Decoder 外加一个 Softmax 分类器与两层编码层构成。上图中左侧方框为 Encoder，右侧方框为 Decoder。

-  由于是一个 Seq2Seq 任务，在训练时，Transformer 的训练语料为若干个句对，具体子任务可以是机器翻译、阅读理解、机器对话等。在原论文中是训练了一个英语与德语的机器翻译任务。在训练时，句对会被划分为输入语料和输出语料，输入语料将从左侧通过编码层进入 Encoder，输出语料将从右侧通过编码层进入 Decoder。Encoder 的主要任务是对输入语料进行编码再输出给 Decoder，Decoder 再根据输出语料的历史信息与 Encoder 的输出进行计算，输出结果再经过一个线性层和 Softmax 分类器即可输出预测的结果概率，整体逻辑如下图：

![[Pasted image 20230822154527.png]]

-  Transformer 整体是一个很值得探究的话题，此处不再赘述，如有感兴趣的同学欢迎阅读原论文 [《Attention Is All You Need》]([https://​arxiv​.org​/pdf​/1706​.03762​.pdf](https://arxiv.org/pdf/1706.03762.pdf)) 与 [基于 Pytorch 的 Transformer 源码解读](https://github.com/datawhalechina/thorough-pytorch/blob/main/source/%E7%AC%AC%E5%8D%81%E7%AB%A0/Transformer%20%E8%A7%A3%E8%AF%BB.md%EF%BC%89%E3%80%82)

## 2.4 预训练任务

-  BERT 的模型架构直接使用了 Transformer 的 Encoder 作为整体架构，__其最核心的思想在于提出了两个新的预训练任务——MLM（Masked Language Model，掩码模型）和 NSP（Next Sentence Prediction，下个句子预测）__，而不是沿用传统的 LM（语言模型）。

![[Pasted image 20230822155743.png]]

1.  MLM 任务，是 BERT 能够深层拟合双向语义特征的基础。简单来讲，MLM 任务即以一定比例对输入语料的部分 token 进行遮蔽，替换为 （MASK）标签，再让模型基于其上下文预测还原被遮蔽的单词，即做一个完形填空任务。由于在该任务中，模型需要针对 (MASK) 标签左右的上下文信息来预测标签本身，从而会充分拟合双向语义信息。
	-  例如，原始输入为 I like you。以 30% 的比例进行遮蔽，那么遮蔽之后的输入可能为：I (MASK) you。而模型的任务即为基于该输入，预测出 (MASK) 标签对应的单词为 like。

2. NSP 任务，是 BERT 用于解决句级 sentence-level 自然语言处理任务的预训练任务。BERT 完全采用了预训练+微调的范式，因此着重通过预训练生成的模型可以解决各种多样化的下游任务。MLM 对 token 级自然语言处理任务（如命名实体识别、关系抽取等）效果极佳，但对于句级自然语言处理任务（如句对分类、阅读理解等），由于预训练与下游任务的模式差距较大，因此无法取得非常好的效果。NSP 任务，是将输入语料都整合成句对类型，句对中有一半是连贯的上下句，标记为 IsNext，一半则是随机抽取的句对，标记为 NotNext。模型则需要根据输入的句对预测是否是连贯上下句，即预测句对的标签。
	-  例如，原始输入句对可能是 (I like you ; Because you are so good) 以及 (I like you; Today is a nice day)。而模型的任务即为对前一个句对预测 IsNext 标签，对后一个句对预测 NotNext 标签。

-  基于上述两个预训练任务，BERT 可以在预训练阶段利用大量无标注文本数据实现深层语义拟合，从而取得良好的预测效果。同时，BERT 追求预训练与微调的深层同步，由于 Transformer 的架构可以很好地支持各类型的自然语言处理任务，从而在 BERT 中，**微调仅需要在预训练模型的最顶层增加一个 SoftMax 分类层即可**。同样值得一提的是，由于在实际下游任务中并不存在 MLM 任务的遮蔽，因此在策略上进行了一点调整，即对于选定的遮蔽词，仅 80% 的遮蔽被直接遮蔽，其余将有 10% 被随机替换，10% 被还原为原单词。

---

# 3 解题思路

-  使用 __预训练的 BERT 模型__ 进行建模的思路步骤如下：

1. **数据预处理**：首先，对文本数据进行预处理，包括文本清洗（如去除特殊字符、标点符号）、分词等操作。可以使用常见的NLP工具包（如NLTK或spaCy）来辅助进行预处理。
2. **构建训练所需的 dataloader 与 dataset**，构建 Dataset 类时，需要定义三个方法 `__init__`，`__getitem__`， `__len__`，其中 `__init__` 方法完成类初始化，`__getitem__` 要求返回返回内容和 label，`__len__` 方法返回数据长度
3. **构造 Dataloader**，在其中完成对句子进行编码、填充、组装 batch 等动作：
4. **定义预测模型利用预训练的BERT模型来解决文本二分类任务**，我们将使用BERT模型编码中的 \[CLS]向量来完成二分类任务。

-  __[CLS]就是classification的意思，可以理解为用于下游的分类任务。__ 主要用于以下两种任务：

-  单文本分类任务：对于文本分类任务，BERT 模型在文本前插入一个 \[CLS] 符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更 “公平” 地融合文本中各个字/词的语义信息。

-  在模型设计中思路就体现为我们取出文本数据经过向量化后的 \[CLS] 向量，然后经过二分类预测层得到最终的结果

```python
outputs = self.bert(**src).last_hidden_state[:, 0, :]
self.predictor(outputs)
self.predictor = nn.Sequential(
            nn.Linear(768, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
```

1.  **模型训练和评估**：使用训练集对选定的机器学习模型进行训练，然后使用测试集进行评估。评估指标可以选择准确率、精确率、召回率、F-1 值等。
2.  **调参优化**：如果模型效果不理想，可以尝试调整特征提取的参数（如词频阈值、词袋大小等）或机器学习模型的参数，以获得更好的性能。

-  在这个进阶实践中，我们使用深度学习方法，一般会遵循以下流程：

![](https://oss.linklearner.com/AI%E5%A4%8F%E4%BB%A4%E8%90%A5/%E7%AC%AC%E4%B8%89%E6%9C%9FNLP/%E4%BB%BB%E5%8A%A1%E4%BA%8C1.png)

-  在进阶Baseline中，我们会使用 Bert 模型

---

# 4 进阶实践 - 深度方法

-  
