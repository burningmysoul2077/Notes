# InternLM 模型全链条开源

# InternLM 模型

>   `InternLM` 是一个开源的轻量级训练框架。
>   旨在 _支持大模型训练而无需大量的依赖_。

-  基于 InternLM 训练框架，_上海人工智能实验室_ 已经发布了两个开源的预训练模型：
	-  InternLM-7B
	-  InternLM-20B

## Lagent

>  `Lagent` 是一个轻量级、开源的基于大语言模型的 __智能体 agent__ 框架。
>   支持用户快速将一个大预言模型转变为多种类型的智能体，并提供了一些典型工具。

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106155421.png)

## 浦语灵笔

>  `浦语·灵笔` 是基于书生·浦语大语言模型研发的视觉-语言大模型。
>  提供出色的图文理解和创作能力，结合了视觉和语言的先进技术，能够实现图像到文本、文本到图像的双向转换。

---

# 创建 InternLM-Chat-7B Demo

## 环境准备

-  InternStudio 创建开发机

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106163112.png)

-  选择镜像 ： Cuda11.7-conda 
-  选择 GPU 资源: A100 (1/4) 

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106163423.png)

-  稍等一会，开发机就会准备好

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106163605.png)

-  熟悉一下开发机的工作台

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106163737.png)

-  左上角是可以切换 `JupyterLab`、`终端`和 `VScode`

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106164006.png)

-  新建一个 `terminal` 窗口，输入 `bash` 进入 conda 环境

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106164238.png)

-  使用命令

```shell
# 使用命令从本地 clone 一个已有的 `pytorch` 环境
conda create --name internlm-demo --clone=/root/share/conda_envs/internlm-base

# 然后使用以下命令激活环境
conda activate internlm-demo

# 安装依赖
# 升级pip
python -m pip install --upgrade pip

pip install modelscope==1.9.5
pip install transformers==4.35.2
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
```


## 模型下载

-  [InternStudio](https://studio.intern-ai.org.cn/) 平台的 `share` 目录下已经准备了全系列的 `InternLM` 模型

```shell
#复制模型
mkdir -p /root/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory
# -r 选项表示递归地复制目录及其内容
```

>  也可以使用 `modelscope` 中的 `snapshot_download` 函数下载模型，第一个参数为模型名称，参数 `cache_dir` 为模型的下载路径。

-  下载模型。在 `/root` 路径下新建目录 `model`，在目录下新建 `download.py` 文件，输入代码

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-chat-7b', cache_dir='/root/model', revision='v1.0.3')
```

-  执行命令

```shell
python /root/model/download.py
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106210959.png)

## 代码准备


```shell
# 首先 `clone` 代码，在 `/root` 路径下新建 `code` 目录，然后切换路径, clone 代码
cd /root/code
git clone https://gitee.com/internlm/InternLM.git
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106210858.png)

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106210939.png)

```python
# 切换 commit 版本，与教程 commit 版本保持一致，可以让大家更好的复现。
cd InternLM
git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106211038.png)

-  将 `/root/code/InternLM/web_demo.py` 中 29 行和 33 行的模型更换为本地的 `/root/model/Shanghai_AI_Laboratory/internlm-chat-7b`。

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106211134.png)

## 终端运行

-  在 `/root/code/InternLM` 目录下新建一个 `cli_demo.py` 文件，输入代码：
- 然后在终端运行以下命令，即可体验 `InternLM-Chat-7B` 模型的对话能力。对话效果如下所示：

```shell
python /root/code/InternLM/cli_demo.py
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106211246.png)

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240106211549.png)

## 配置本地端口

-  通过使用SSH隧道的方法，将服务器上的这些特定端口映射到本地计算机的端口
-  首先配置本地的 `SSH Key`，打开 `Power Shell`，输入

```shell
ssh-keygen -t rsa
```

-  选择密钥文件的保存位置，默认情况下是在 `~/.ssh/` 目录中。
-  通过系统自带的 `cat` 工具查看文件内容:

```shell
cat ~\.ssh\id_rsa.pub
```

-  在 `InternStudio` 控制台，点击配置 SSH Key

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240108204923.png)

- 在本地终端输入以下指令：  `6006` 是在服务器中打开的端口，而 `dev_port` 是根据开发机的端口进行更改:

```shell
ssh -CNg -L 6006:127.0.0.1:6006 root@ssh.intern-ai.org.cn -p {dev_port}
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020240108205133.png)

## web demo 运行

-  切换到 `VScode` 中，运行 `/root/code/InternLM` 目录下的 `web_demo.py` 文件，输入以下命令：

```shell
bash
conda activate lmdemo  # 首次进入 vscode 会默认是 base 环境，所以首先切换环境
cd /root/code/InternLM
streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715162754.png)

-  在浏览器打开 `http://127.0.0.1:6006` 页面后，模型才会加载

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715182363.png)

-  在加载完模型之后，尝试让 InternLM-Chat-7B 写一篇不超过500字的悬疑小说，但是很明显不够生动。

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715443948.png)

---

# Lagent 智能体工具

## 环境准备

-  选择和第一个 `InternLM` 一样的镜像环境

## 模型下载

-  选择和第一个 `InternLM` 一样的模型

## 安装 Lagent

```shell
cd /root/code
git clone https://gitee.com/internlm/lagent.git
cd /root/code/lagent
git checkout 511b03889010c4811b1701abb153e02b8e94fb5e # 尽量保证和教程commit版本一致
pip install -e . # 源码安装
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715582270.png)

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715652844.png)

## 修改代码

-  整体修改  `/root/code/lagent/examples/react_web_demo.py`

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704715700137.png)

## Demo 运行

```shell
streamlit run /root/code/lagent/examples/react_web_demo.py --server.address 127.0.0.1 --server.port 6006
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704716205053.png)

-  切换到 `VScode` 页面，运行成功后，将端口映射到本地
-  在本地浏览器输入 `http://127.0.0.1:6006`
-  在 `Web` 页面选择 `InternLM` 模型，等待模型加载完毕后，输入数学问题

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704716370157.png)

---

# 浦语灵笔图文理解创作

## 环境准备

-  在 [InternStudio](https://studio.intern-ai.org.cn/) 上选择 A100(1/4)*2 的配置

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704716788612.png)

## 安装以来

-  接下来打开刚刚租用服务器的 `进入开发机`，并在终端输入 `bash` 命令，进入 `conda` 环境
-  使用以下命令从本地克隆一个已有的`pytorch 2.0.1` 的环境

```shell
/root/share/install_conda_env_internlm_base.sh xcomposer-demo
```

-  激活环境

```shell
conda activate xcomposer-demo
```

-  运行以下命令，安装 `transformers`、`gradio` 等依赖包。请严格安装以下版本安装！

```shell
pip install transformers==4.33.1 timm==0.4.12 sentencepiece==0.1.99 gradio==3.44.4 markdown2==2.4.10 xlsxwriter==3.1.2 einops accelerate
```

## 模型下载

```shell
mkdir -p /root/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-xcomposer-7b /root/model/Shanghai_AI_Laboratory
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704717404572.png)

## 代码准备

```shell
cd /root/code
git clone https://gitee.com/internlm/InternLM-XComposer.git
cd /root/code/InternLM-XComposer
git checkout 3e8c79051a1356b9c388a6447867355c0634932d  # 最好保证和教程的 commit 版本一致
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704717605180.png)

## Demo 运行

-  在终端运行以下代码：

```shell
cd /root/code/InternLM-XComposer
python examples/web_demo.py  \
    --folder /root/model/Shanghai_AI_Laboratory/internlm-xcomposer-7b \
    --num_gpus 1 \
    --port 6006
```

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704717775564.png)

-  将端口映射到本地。在本地浏览器输入 `http://127.0.0.1:6006` 即可。以`似此星辰非昨夜，为谁风露立中宵`为提示词，体验图文创作的功能，如下图所示：

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704717925068.png)

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/17047179250686006.png)

-  体验一下图片理解能力

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/1704718080621.png)
