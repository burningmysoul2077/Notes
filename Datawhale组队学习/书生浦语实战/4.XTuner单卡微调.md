# 概述

## XTuner

>  一个大语言模型微调工具箱。_由_ _MMRazor_ _和_ _MMDeploy_ _联合开发。_

## 支持的开源 LLM

-  **[InternLM](https://huggingface.co/internlm/internlm-7b)** ✅
- [Llama，Llama2](https://huggingface.co/meta-llama)
- [ChatGLM2](https://huggingface.co/THUDM/chatglm2-6b)，[ChatGLM3](https://huggingface.co/THUDM/chatglm3-6b-base)
- [Qwen](https://huggingface.co/Qwen/Qwen-7B)
- [Baichuan](https://huggingface.co/baichuan-inc/Baichuan-7B)，[Baichuan2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base)
- ......
- [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)

## 特色

-  傻瓜化：以配置文件的形式封装了大部分微调场景，0基础的非专业人员也能一键开始微调




---

# 2 上手

## 2.1 准备环境

![[Pasted image 20240122203332.png]]

-  clone 一个已有的 pytorch 2.0.1 环境

```shell
# 如果在 InternStudio 平台，则从本地 clone 一个已有 pytorch 2.0.1 的环境：
/root/share/install_conda_env_internlm_base.sh xtuner0.1.9
# 如果在其他平台：
conda create --name xtuner0.1.9 python=3.10 -y

# 激活环境
conda activate xtuner0.1.9
# 进入家目录 （~的意思是 “当前用户的home路径”）
cd ~
# 创建版本文件夹并进入，以跟随本教程
mkdir xtuner019 && cd xtuner019

# 拉取 0.1.9 的版本源码
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# 无法访问github的用户请从 gitee 拉取:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# 进入源码目录
cd xtuner

# 从源码安装 XTuner
pip install -e '.[all]'
```

![[Pasted image 20240122205627.png]]

-  安装完后，就开始准备在 oasst1 数据集上微调 internlm-7b-chat

```python
# 创建一个微调 oasst1 数据集的工作路径，进入
mkdir ~/ft-oasst1 && cd ~/ft-oasst1
```

## 2.2 微调

### 2.2.1 XTuner 配置

- XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：

```shell
# 列出所有内置配置
xtuner list-cfg
```

-  `假如显示bash: xtuner: command not found的话可以考虑在终端输入 export PATH=$PATH:'/root/.local/bin'`

![[Pasted image 20240122210425.png]]

- 拷贝一个配置文件到当前目录： `# xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}`

在本案例中即：（注意最后有个英文句号，代表复制到当前路径）

```shell
cd ~/ft-oasst1
# 模型名  internlm_chat_7b 
# 使用算法  qlora 
# 数据集  oasst1 
# 把数据集跑几次  跑3次：e3 (epoch 3 ) 
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

![[Pasted image 20240122211150.png]]

*无 chat比如 `internlm-7b` 代表是基座(base)模型

### 2.2.2 模型下载

#### 软连接

`由于下载模型很慢，用教学平台的同学可以直接复制模型。`

```shell
ln -s /share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/
```

- 以上是通过软链的方式，将模型文件挂载到家目录下，优势是：
1. 节省拷贝时间，无需等待
2. 节省用户开发机存储空间

![[Pasted image 20240219104319.png]]

#### 数据拷贝

-  当然，也可以用 `cp -r /share/temp/model_repos/internlm-chat-7b ~/ft-oasst1/` 进行数据拷贝。

#### 自己下载模型

-  以下是自己下载模型的步骤。

- 不用 xtuner 默认的`从 huggingface 拉取模型`，而是提前从 ~~OpenXLab~~ ModelScope 下载模型到本地

```shell
# 创建一个目录，放模型文件，防止散落一地
mkdir ~/ft-oasst1/internlm-chat-7b

# 装一下拉取模型文件要用的库
pip install modelscope

# 从 modelscope 下载下载模型文件
cd ~/ft-oasst1
apt install git git-lfs -y
git lfs install
git lfs clone https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3
```

### 2.2.3 数据集下载

-  `[https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main]`

-  由于 huggingface 网络问题，已经提前下载好了，复制到正确位置即可：

```shell
cd ~/ft-oasst1
# ...-guanaco 后面有个空格和英文句号啊
cp -r /root/share/temp/datasets/openassistant-guanaco .
```

![[Pasted image 20240219104520.png]]

### 2.2.4 修改配置文件

-  修改其中的模型和数据集为 本地路径

```shell
cd ~/ft-oasst1
vim internlm_chat_7b_qlora_oasst1_e3_copy.py
```


![[Pasted image 20240219104707.png]]

-  **常用超参**

|参数名|解释|
|---|---|
|**data_path**|数据路径或 HuggingFace 仓库名|
|max_length|单条数据最大 Token 数，超过则截断|
|pack_to_max_length|是否将多条短数据拼接到 max_length，提高 GPU 利用率|
|accumulative_counts|梯度累积，每多少次 backward 更新一次参数|
|evaluation_inputs|训练过程中，会根据给定的问题进行推理，便于观测训练状态|
|evaluation_freq|Evaluation 的评测间隔 iter 数|
|......|......|

> 如果想把显卡的现存吃满，充分利用显卡资源，可以将 `max_length` 和 `batch_size` 这两个参数调大。


## 2.3 开始微调

-  训练

> xtuner train ${CONFIG_NAME_OR_PATH}

-  **也可以增加 deepspeed 进行训练加速：**

>  xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2

- 例如，我们可以利用 QLoRA 算法在 oasst1 数据集上微调 InternLM-7B：

```shell
# 单卡
## 用刚才改好的config文件训练
xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# 多卡
NPROC_PER_NODE=${GPU_NUM} xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

# 若要开启 deepspeed 加速，增加 --deepspeed deepspeed_zero2 即可
```

-  开始微调

![[Pasted image 20240219105225.png]]

![[Pasted image 20240219144206.png]]

-  经历了三次失败，终于...

![[Pasted image 20240219210515.png]]



> 微调得到的 PTH 模型文件和其他杂七杂八的文件都默认在当前的 `./work_dirs` 中。

-  将得到的 PTH 模型转换为 HuggingFace 模型，**即：生成 Adapter 文件夹**

>  xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH_file_dir} ${SAVE_PATH}

-  在本示例中：

```shell
mkdir hf
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER=GNU
xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf
```

- **此时，hf 文件夹即为我们平时所理解的所谓 “LoRA 模型文件”**

> 可以简单理解：LoRA 模型文件 = Adapter

## 2.4 部署与测试

### 2.4.1 将 HuggingFace adapter 合并到大语言模型：

```shell
xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
# xtuner convert merge \
#     ${NAME_OR_PATH_TO_LLM} \
#     ${NAME_OR_PATH_TO_ADAPTER} \
#     ${SAVE_PATH} \
#     --max-shard-size 2GB
```

### 2.4.2 与合并后的模型对话：

```shell
# 加载 Adapter 模型对话（Float 16）
xtuner chat ./merged --prompt-template internlm_chat

# 4 bit 量化加载
# xtuner chat ./merged --bits 4 --prompt-template internlm_chat
```

### 2.4.3 Demo

- 修改 `cli_demo.py` 中的模型路径

```diff
- model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"
+ model_name_or_path = "merged"
```

- 运行 `cli_demo.py` 以目测微调效果

```shell
python ./cli_demo.py
```

---

# XTuner InternLM-Chat 个人小助手认知微调实践

## 微调环境准备

```shell
# InternStudio 平台中，从本地 clone 一个已有 pytorch 2.0.1 的环境（后续均在该环境执行，若为其他环境可作为参考）
# 进入环境后首先 bash
# 进入环境后首先 bash
# 进入环境后首先 bash
bash
conda create --name personal_assistant --clone=/root/share/conda_envs/internlm-base
# 如果在其他平台：
# conda create --name personal_assistant python=3.10 -y
# copy一份环境作为虚拟环境

# 激活环境
conda activate personal_assistant
# 进入家目录 （~的意思是 “当前用户的home路径”）
cd ~
# 创建版本文件夹并进入，以跟随本教程
# personal_assistant用于存放本教程所使用的东西
mkdir /root/pa && cd /root/pa
mkdir /root/pa/xtuner019 && cd /root/pa/xtuner019

# 拉取 0.1.9 的版本源码
git clone -b v0.1.9  https://github.com/InternLM/xtuner
# 无法访问github的用户请从 gitee 拉取:
# git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

# 进入源码目录
cd xtuner

# 从源码安装 XTuner
pip install -e '.[all]'
```

![[Pasted image 20240220092103.png]]

## 数据准备

-  创建 `data` 文件夹用于存放用于训练的数据集

```shell
mkdir -p /root/pa/data && cd /root/pa/data
```

-  在 `data` 目录下创建一个 json 文件 `pa.json` 作为本次微调所使用的数据集。json 中内容可参考下方 (_复制粘贴 n 次做数据增广，数据量小无法有效微调_，下面仅用于展示格式，下面也有生成脚本)

其中 `conversation` 表示一次对话的内容，`input` 为输入，即用户会问的问题，`output` 为输出，即想要模型回答的答案。

```json
[
    {
        "conversation": [
            {
                "input": "请介绍一下你自己",
                "output": "我是毕竟我辣么萌的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"
            }
        ]
    },
    {
        "conversation": [
            {
                "input": "请做一下自我介绍",
                "output": "我是毕竟我辣么萌的小助手，内在是上海AI实验室书生·浦语的7B大模型哦"
            }
        ]
    }
]
```

- 以下是一个python脚本，用于生成数据集。在 `data` 目录下新建一个 `generate_data.py` 文件，将以下代码复制进去，然后运行该脚本即可生成数据集。

```python
import json

# 输入你的名字
name = 'xxxxxx'
# 重复次数
n = 10000

data = [
    {
        "conversation": [
            {
                "input": "请做一下自我介绍",
                "output": "我是{}的小助手，内在是上海AI实验室书生·浦语的7B大模型哦".format(name)
            }
        ]
    }
]

for i in range(n):
    data.append(data[0])

with open('personal_assistant.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False, indent=4)
```

## 配置准备

### 下载模型`InternLM-chat-7B`

-  [InternStudio](https://studio.intern-ai.org.cn/) 平台的 `share` 目录下已经为我们准备了全系列的 `InternLM` 模型，可以使用如下命令复制`internlm-chat-7b`：

```shell
mkdir -p /root/pa/model/Shanghai_AI_Laboratory
cp -r /root/share/temp/model_repos/internlm-chat-7b /root/pa/model/Shanghai_AI_Laboratory
```

### 配置

-  XTuner 提供多个开箱即用的配置文件，用户可以通过下列命令查看：

```shell
# 列出所有内置配置
xtuner list-cfg
```

```shell
#创建用于存放配置的文件夹config并进入
mkdir /root/pa/config && cd /root/pa/config
```

- 拷贝一个配置文件到当前目录：`xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}` 在本例中：（注意最后有个英文句号，代表复制到当前路径）

```shell
xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .
```

- 修改拷贝后的文件 `internlm_chat_7b_qlora_oasst1_e3_copy.py`，修改下述位置： (这是一份修改好的文件[internlm_chat_7b_qlora_oasst1_e3_copy.py](https://github.com/InternLM/tutorial/blob/main/xtuner/internlm_chat_7b_qlora_oasst1_e3_copy.py)) 

![[Pasted image 20240220093459.png]]

> 红框为配置文件中PART 1需要修改的内容

![[Pasted image 20240220093531.png]]

> 红框为配置文件中PART 3需要修改的内容

```shell
# PART 1 中
# 预训练模型存放的位置
pretrained_model_name_or_path = '/root/pa/model/Shanghai_AI_Laboratory/internlm-chat-7b'

# 微调数据存放的位置
data_path = '/root/pa/data/personal_assistant.json'

# 训练中最大的文本长度
max_length = 512

# 每一批训练样本的大小
batch_size = 2

# 最大训练轮数
max_epochs = 3

# 验证的频率
evaluation_freq = 90

# 用于评估输出内容的问题（用于评估的问题尽量与数据集的question保持一致）
evaluation_inputs = [ '请介绍一下你自己', '请做一下自我介绍' ]


# PART 3 中
dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path))
dataset_map_fn=None
```

## 启动微调

- 用 `xtuner train` 命令启动训练、

```shell
xtuner train /root/pa/config/internlm_chat_7b_qlora_oasst1_e3_copy.py
```

![[Pasted image 20240220101323.png]]

## 微调后参数转换/合并

- 训练后的 pth 格式参数转 Hugging Face 格式

```shell
# 创建用于存放Hugging Face格式参数的hf文件夹
mkdir /root/pa/config/work_dirs/hf

export MKL_SERVICE_FORCE_INTEL=1

# 配置文件存放的位置
export CONFIG_NAME_OR_PATH=/root/pa/config/internlm_chat_7b_qlora_oasst1_e3_copy.py

# 模型训练后得到的pth格式参数存放的位置
export PTH=/root/pa/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth

# pth文件转换为Hugging Face格式后参数存放的位置
export SAVE_PATH=/root/pa/config/work_dirs/hf

# 执行参数转换
xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH
```

![[Pasted image 20240220102256.png]]

- Merge模型参数

```shell
export MKL_SERVICE_FORCE_INTEL=1
export MKL_THREADING_LAYER='GNU'

# 原始模型参数存放的位置
export NAME_OR_PATH_TO_LLM=/root/pa/model/Shanghai_AI_Laboratory/internlm-chat-7b

# Hugging Face格式参数存放的位置
export NAME_OR_PATH_TO_ADAPTER=/root/pa/config/work_dirs/hf

# 最终Merge后的参数存放的位置
mkdir /root/pa/config/work_dirs/hf_merge
export SAVE_PATH=/root/pa/config/work_dirs/hf_merge

# 执行参数Merge
xtuner convert merge \
    $NAME_OR_PATH_TO_LLM \
    $NAME_OR_PATH_TO_ADAPTER \
    $SAVE_PATH \
    --max-shard-size 2GB
```

![[Pasted image 20240220102154.png]]

## 网页DEMO

- 安装网页Demo所需依赖

```shell
pip install streamlit==1.24.0
```

- 下载[InternLM](https://github.com/InternLM/InternLM)项目代码（欢迎Star）

```shell
# 创建code文件夹用于存放InternLM项目代码
mkdir /root/pa/code && cd /root/pa/code
git clone https://github.com/InternLM/InternLM.git
```

![[Pasted image 20240220102315.png]]

- 将 `/root/code/InternLM/web_demo.py` 中 29 行和 33 行的模型路径更换为Merge后存放参数的路径 `/root/pa/config/work_dirs/hf_merge`

- 运行 `/root/pa/code/InternLM` 目录下的 `web_demo.py` 文件，输入以下命令后，[**查看本教程5.2配置本地端口后**](https://github.com/InternLM/tutorial/blob/main/helloworld/hello_world.md#52-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%AB%AF%E5%8F%A3)，将端口映射到本地。在本地浏览器输入 `http://127.0.0.1:6006` 即可。

```
streamlit run /root/pa/code/InternLM/chat/web_demo.py --server.address 127.0.0.1 --server.port 6006
```

![[Pasted image 20240220105043.png]]

- 注意：要在浏览器打开 `http://127.0.0.1:6006` 页面后，模型才会加载。 在加载完模型之后，就可以与微调后的 InternLM-Chat-7B 进行对话了

![[Pasted image 20240220105059.png]]

![[Pasted image 20240220105254.png]]

![[Pasted image 20240220103230.png]]

-  微调后

![[Pasted image 20240220105242.png]]