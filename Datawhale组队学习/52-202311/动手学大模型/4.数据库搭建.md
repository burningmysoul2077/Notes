# 1 知识库文档处理

## 1.1 知识库设计

-  选用一些开源文档或视频
-  此处 pdf 文档 选用了 邱锡鹏老师的 [《神经网络》](https://github.com/nndl/nndl.github.io) 
-  此处 Markdown 文档选用了自己翻译的 《A Survey of LLM》

## 1.2 文档加载

### 1.2.1 PDF 

```python
## 安装必要的库
# !pip install rapidocr_onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple
# !pip install "unstructured[all-docs]" -i https://pypi.tuna.tsinghua.edu.cn/simple
# !pip install pyMuPDF -i https://pypi.tuna.tsinghua.edu.cn/simple
```

-  使用 `PyMuPDFLoader` 来读取知识库的 PDF 文件

![[Pasted image 20231122081244.png]]

![[Pasted image 20231122081338.png]]

### 1.2.2 Markdown

-  与 PDF 类似

![[Pasted image 20231122081308.png]]

![[Pasted image 20231122081353.png]]

### 1.2.3 MP4

-  使用 Whisper 实现视频的转写  [知乎|开源免费离线语音识别神器whisper如何安装](https://zhuanlan.zhihu.com/p/595691785)

## 1.3 文档分割

-  Langchain 中文本分割器都根据 `chunk_size` (块大小)和 `chunk_overlap` (块与块之间的重叠大小)进行分割。
-  `chunk_size` 指每个块包含的字符或 Token （如单词、句子等）的数量 
-  `chunk_overlap` 指两个块之间共享的字符数量，用于保持上下文的连贯性，避免分割丢失上下文信息

![[Pasted image 20231121084944.png]]

-  Langchain 提供多种文档分割方式，_区别在怎么确定块与块之间的边界、块由哪些字符/token组成、以及如何测量块大小_

-  `RecursiveCharacterTextSplitter()`: 按字符串分割文本，递归地尝试按不同的分隔符进行分割文本。
-  `CharacterTextSplitter()`: 按字符来分割文本。
-  `MarkdownHeaderTextSplitter()`: 基于指定的标题来分割 markdown 文件。
-  `TokenTextSplitter()`: 按 token 来分割文本。
-  `SentenceTransformersTokenTextSplitter()`: 按token来分割文本
-  `Language()`: 用于 CPP、Python、Ruby、Markdown 等。
-  `NLTKTextSplitter()`: 使用 NLTK（自然语言工具包）按句子分割文本。
-  `SpacyTextSplitter()`: 使用 Spacy 按句子的切割文本。

## 1.4 文档词向量化

-  Embeddings, 是一种将类别数据，如单词、句子或整个文档，转化为实数向量的技术。

-  这里提供三种方式进行，一种是直接使用 openai 的模型去生成 embedding，另一种是使用 HuggingFace 上的模型去生成 embedding。
1. openAI 的模型需要消耗 api，对于大量的token 来说成本会比较高，但是非常方便。
2. HuggingFace 的模型可以本地部署，可自定义合适的模型，可玩性较高，但对本地的资源有部分要求。
3. 采用其他平台的 api。对于获取 openAI key 不方便的同学可以采用这种方法。

### 相关性

- 我们已经生成了对应的向量，我们如何度量文档和问题的相关性呢？

- 这里提供两种常用的方法：
1. 计算两个向量之间的点积。
2. 计算两个向量之间的余弦相似度

-  点积是将两个向量对应位置的元素相乘后求和得到的标量值。点积相似度越大，表示两个向量越相似。

---

# 2 向量数据库

## 2.1 向量数据库简介

>  向量数据库是用于高效计算和管理大量向量数据的解决方案。向量数据库是一种专门用于存储和检索向量数据（embedding）的数据库系统。

-  _它与传统的基于关系模型的数据库不同，它主要关注的是向量数据的特性和相似性。_

## 2.2 Chroma

### 2.2.1 Chroma 介绍

> the AI-native open-source embedding database
-  官网 https://www.trychroma.com/
-  https://github.com/chroma-core/chroma

![[Pasted image 20231122083615.png]]

![[Pasted image 20231122083630.png]]

![[Pasted image 20231122083656.png]]

### 2.2.2 LangChain-Chroma

-  https://python.langchain.com/docs/integrations/vectorstores/chroma
-  Chroma runs in various modes.
	- `in-memory` - in a python script or jupyter notebook
	- `in-memory with persistance` - in a script or notebook and save/load to disk
	- `in a docker container` - as a server running your local machine or in the cloud

### 2.2.3 构建 Chroma 向量库

-  将 embeddings 持久化到磁盘上

![[Pasted image 20231121151244.png]]

---

# 3 通过向量数据库检索

## 3.1 相似度检索

-  通过持久化后 Chroma 进行搜索

![[Pasted image 20231121152005.png]]

## 3.2 MMR 检索

-  如果只考虑检索出内容的相关性会导致内容过于单一，可能丢失重要信息。
-  __最大边际相关性 Maximum marginal relevance MMR__ 可以在保持相关性的同时，增加内容的丰富度。
-  _核心思想是在已经选择了一个相关性高的文档之后，再选择一个与已选文档相关性较低但是信息丰富的文档_。这样可以在保持相关性的同时，增加内容的多样性，避免过于单一的结果。

![[Pasted image 20231121154021.png]]

![[Pasted image 20231122081428.png]]

---

# 4 构造检索式问答链

## 4.1 直接询问 LLM

-  此处 LLM 采用了智谱的 CHATGLM

![[Pasted image 20231122082401.png]]

-  LLM 的回答：

![[Pasted image 20231122082330.png]]


## 4.2 结合 Prompt 提问

-  _对于 LLM ， Prompt 可以更好地发挥大模型的能力_

### 4.2.1 定义提示模板

-  _要包含一些如何使用下面的上下文片段的说明，然后有一个上下文变量的占位符_

![[Pasted image 20231122083403.png]]

![[Pasted image 20231122083415.png]]


-  这种方法的好处是，只涉及对语言模型的一次调用
-  局限性是如果文档太多，可能无法将它们全部适配到上下文窗口中。


## 4.3 LangChain 配置

-  LangChain 提供了集中不同的处理文档的方法

![[Pasted image 20231122081559.png]]

-  通过配置 `chain_type` 的参数，选择对应的处理方式

```
RetrievalQA.from_chain_type(
	llm,
	retriever=vectordb.as_retriever(),
	chain_type="map_reduce"
)
```
