# 1 Prompt 设计的原则和技巧

>  Prompt: 用户与大模型交互输入的代称。
>  Completion：大模型返回的输出。

-  设计高效 Prompt 的两个关键原则： __编写清晰、具体的指令__ 和 __给予模型充足思考时间__。

## 1.1 原则一：编写清晰、具体的指令

-  Prompt 需要清晰明确的表达需求，提供充足上下文，就像在给外星人讲解人类世界一样。

### 1.1.1 使用分隔符清晰地表示输入的不同部分

-  _分隔符用来将不同的指令、上下文、输入隔开，避免意外混淆_。
-  可以采用各种标点符号，如 \`\`\` 、“”“、<>、`，`、： 等。

![[Pasted image 20231123101515.png]]

![[Pasted image 20231123101529.png]]

### 1.1.2 不使用分隔符

-  使用分隔符尤其重要的是要防止 __提示词注入 Prompt Injection__

>  提示词注入，就是 _用户输入的文本可能包含与你的预设 Prompt 相冲突的内容_。

-  如果不使用分隔符，这些输入可能注入并操纵语言模型。

![[Pasted image 20231123101656.png]]

![[Pasted image 20231123101723.png]]

### 1.1.3 寻求结构化的输出

>  结构化输出，就是按照某种格式组织的内容，例如 JSON、HTML 等。这种输出非常适合在代码中进一步解析和处理。

![[Pasted image 20231123102526.png]]

![[Pasted image 20231123104149.png]]


### 1.1.4 要求模型检查是否满足条件

-  如果任务包含不一定能满足的假设或条件，我们可以告诉模型先检查这些假设；如果不满足，则会指出并停止执行后续的完整流程。
-  还可以考虑可能出现的边缘情况和模型的应对，以避免意外的结果或错误发生。

![[Pasted image 20231123104319.png]]

![[Pasted image 20231123104341.png]]

-  如果提供给模型 _没有预期指令的输入_，模型讲判断未提供步骤

### 1.1.5 提供少量示例

>  Few-shot prompting 少样本提示，即在要求模型执行实际任务之前，给模型一两个已完成的样例，让模型了解我们的要求和期望的输出样式。

![[Pasted image 20231123110426.png]]

![[Pasted image 20231123110445.png]]

## 1.2 原则二：给模型时间去思考

>  给予语言模型充足的推理时间，是 Prompt Engineering 中一个非常重要的设计原则

-  LM 与人类一样，需要时间来思考并解决复杂问题。
-  我们应通过 Prompt 引导 LM 进行深入思考，可以要求其先列出对问题的各种看法，说明推理依据，然后再得出最终结论。
-  在 Prompt 中添加逐步推理的要求，能让 LM 投入更多时间逻辑思维，输出结果也将更可靠准确。

### 1.2.1 指定完成任务所需的步骤

![[Pasted image 20231123135805.png]]

![[Pasted image 20231123140135.png]]

### 1.2.2 指导模型在下结论之前找出一个自己的解法

-  在设计 Prompt 时，我们还可以通过明确指导语言模型进行自主思考，来获得更好地效果。

-  举个例子，假设我们要语言模型判断一个数学问题的解答是否正确。仅仅提供问题和解答是不够的，语言模型可能会匆忙做出错误判断。
-  相反，我们可以在 Prompt 中先要求语言模型自己尝试解决这个问题，思考出自己的解法，然后再与提供的解答进行对比，判断正确性。这种先让语言模型自主思考的方式，能帮助它更深入理解问题，做出更准确的判断。

-  提供给 LLM 一个看似正确实际错误的 Prompt

![[Pasted image 20231123140724.png]]

![[Pasted image 20231123140803.png]]

-  智谱 AI 看似认出了 Prompt 的不正确，但是后续它的回答暴露了其实没有完全理解，正确答案应该是 `360x + 100,000`

-  在接下来这个 Prompt 中，我们要求模型先自行解决这个问题，再根据自己的解法与学生的解法进行对比，从而判断学生的解法是否正确。同时，我们给定了输出的格式要求。通过拆分任务、明确步骤，让 模型有更多时间思考，有时可以获得更准确的结果。

-  采用 `model=chatglm_turbo`，就能得到正确的答案

![[Pasted image 20231123142748.png]]


### 1.2.3 大模型的幻觉

>  在开发与应用语言模型时，需要注意它们可能生成虚假信息的风险。尽管模型经过大规模预训练，掌握 了丰富知识，但它实际上并没有完全记住所见的信息，难以准确判断自己的知识边界，可能做出错误推断。若让语言模型描述一个不存在的产品,它可能会自行构造出似是而非的细节。这被称为`“幻觉” (Hallucination)`，是语言模型的一大缺陷。

![[Pasted image 20231123143610.png]]

![[Pasted image 20231123143624.png]]

-  语言模型的幻觉问题 _事关应用的可靠性与安全性_。
-  开发者有必要认识到这一缺陷，并采取 Prompt 优化、外部知识等措施予以缓解。
-  这也是未来语言模型进化的重要方向之一。

---



