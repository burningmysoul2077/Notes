# 1 验证迭代的一般思路

-  如果使用传统方法开发 LLM 应用，很可能出现的场景：
1. 会在 1 到 3 个的小样本中调整 Prompt，尝试使其在这些样本上起效。
2.  随后，对系统进一步测试时，很可能会遇到一些棘手的例子，这些例子无法通过 Prompt 或者算法解决。
-  这就是使用 LLM 构建应用程序的开发者所面临的挑战。

-  在这种情况下，你可以将这些额外的几个例子添加到你正在测试的集合中，有机地添加其他难以处理的例子。最终，你会将足够多的这些例子添加到你逐步扩大的开发集中，以至于手动运行每一个例子以测试 Prompt 变得有些不便。然后，你开始开发一些用于衡量这些小样本集性能的指标，例如平均准确度。这个过程的有趣之处在于，如果你觉得你的系统已经足够好了，你可以随时停止，不再进行改进。实际上，很多已经部署的应用程序就在第一步或第二步就停下来了，而且它们运行得非常好。

## 1.1 一般流程

>  验证迭代是构建以 LLM 为中心的应用程序所必不能少的重要步骤，通过不断寻找 Bad Case，针对性调整 Prompt 或优化应用框架，来推动应用达到我们目标中的性能与精度。

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231123144930.png)


### Prompt Engineering 

-  在上一章 Prompt Engineering 部分，我们已经讲解了如何在数个简单样例的基础上设计 Prompt Engineering，初步实现核心任务的启动。

### Bad Case

-  我们将首先介绍如何找出 Bad Case 的一些思路提示，以及针对 Bad Case 针对性做出 Prompt 优化的一般思路。注意，在这一过程中，你仍然应该谨记我们在上一节中所讲述的 Prompt 设计原则与技巧，并时刻保证优化后的 Prompt 不会在原先表现良好的样例上出现失误。

### 自动评估

-  接着介绍大模型开发评估的几种方法。对于有简单标准答案的任务来说，评估很容易得到实现；但大模型开发一般是需要实现复杂的生成任务，如何在没有简单答案甚至没有标准答案的情况下实现评估，能够准确地反映应用的效果。

-  最后，随着我们不断寻找到 Bad Case 并做出针对性优化，我们可以将这些 Bad Case 逐步加入到验证集，从而形成一个有一定样例数的验证集。针对这种验证集，一个一个进行评估就是不切实际的了。我们需要一种自动评估方法，实现对该验证集上性能的整体评估。

---

# 2 解决 Bad Case

## 2.1 构造向量数据库

-  使用之前构造的本地持久化的向量数据库
-  再引入智谱 AI Embedding
-  最后使用 LangChain PromptTemplate

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231123154346.png)

-  可以查看检索到的相关文本

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231123154755.png)

## 2.2 提升直观回答质量


>  寻找 Bad Case 的思路有很多，最直观也最简单的就是评估直观回答的质量，结合原有资料内容，判断在什么方面有所不足。

-  尝试把上一小节测试改造成一个 Bad Case。

```
问题：什么是神经网络
初始回答：" 神经网络是一种由人工神经元和神经元之间的连接构成的信息处理系统，其中有两类特殊的神经元：一类用来接收外部的信息，另一类用来输出信息。神经网络可以看作是从输入到输出的信息处理系统。"
存在不足：回答太笼统，需要回答更具体；谢谢你的提问感觉比较死板，可以去掉
```

-  第二个模板果然回答丰富了很多。

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_17007261113037.png)

- 更进一步思考，要求模型给出具体、详细的回答，是否会导致针对一些有要点的回答没有重点、模糊不清？我们测试以下问题：

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231123161805.png)

-  针对 Bad Case，改进 Prompt，要求其对有几点的答案进行分点标号，让答案清晰具体

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231124084614.png)

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231124084637.png)

## 2.3 标明知识来源，提高可信度

-  由于大模型存在幻觉问题，有时我们会怀疑模型回答并非源于已有知识库内容，这对一些需要保证真实性的场景来说尤为重要

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231124085054.png)

![imagetext](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/Datawhale%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/Pasted%20image%2020231124085107.png)

-  改进 Prompt，要求在回答时附上原文

![[Pasted image 20231124085600.png]]

-  可惜智谱的回答并没有附上原文

![[Pasted image 20231124085512.png]]

## 2.4 构造思维链

-  LLM 现有的一些缺陷：幻觉、无法理解复杂指令、无法执行复杂步骤等。
-  这些暂时都可以通过构造思维连，将 Prompt 构造成一系列步骤来尽量减少其能力限制。

-  构建一个 Bad Case

```
问题：我们应该如何去构造一个 LLM 项目 
初始回答：略 
存在不足：事实上，知识库中并没有关于如何构造LLM项目的内容，模型的回答看似有道理，实则是大模型的幻觉，将部分相关的文本拼接得到，存在问题
```

![[Pasted image 20231124140030.png]]

![[Pasted image 20231124140139.png]]

-  如果优化 Prompt，将之前的 Prompt 变成两个步骤，要求模型在第二个步骤中做出反思：
-  那就很可惜，chatglm_std 还是有幻觉的

![[Pasted image 20231124141950.png]]

-  chatglm_turbo 同样如此

![[Pasted image 20231124142936.png]]

## 2.5 增加一个指令解析

-  我们往往会面临一个需求，即我们需要模型以我们指定的格式进行输出。但是，由于我们使用了 Prompt Template 来填充用户问题，用户问题中存在的格式要求往往会被忽略，例如：

---

# 3 大模型评估方法

-  当验证集较小时，我们可以采用人工评估的方法，即对验证集中的每一个验证案例，人工评估系统输出的优劣；但当验证集随着系统的优化而不断扩张，其体量会不断增大，以至于人工评估的时间和人力成本扩大到我们无法接受的程度。

-  因此，我们需要采用自动评估的方法，自动评估系统对每一个验证案例的输出质量，从而评估系统的整体性能。

## 3.1 人工评估的一般思路

### 准则一 量化评估

-  我们应该对每一个验证案例的回答都给出打分，最后计算所有验证案例的平均分得到版本系统的得分。量化的量纲根据个人风格和业务实际情况而定。
-  量化后的评估指标应当有一定的评估规范，保证不同评估员之间评估的相对一致。

### 准则二 多维评估

-  大模型是典型的生成模型，即其回答为一个由模型生成的语句。一般而言，大模型的回答需要在多个维度上进行评估。设计每个维度的评估指标，在每个维度上都进行打分，从而综合评估系统性能。同时需要注意，多维评估应当和量化评估有效结合，对每一个维度，可以设置相同的量纲也可以设置不同的量纲，应充分结合业务实际。

## 3.2 简单自动评估

-  大模型评估之所以复杂：_一个重要原因在于生成模型的答案很难判别，即客观题评估判别很简单，主观题评估判别则很困难_。尤其是对于一些没有标准答案的问题，实现自动评估就显得难度尤大。
-  但是，在牺牲一定评估准确性的情况下，我们可以将没有标准答案的主观题进行转化，从而变成有标准答案的问题，进而通过简单的自动评估来实现。
-  下文介绍两种方法： __构造客观题__ 与 __计算标准答案相似度__

### 方法一 构造客观题

-  _主观题的评估是非常困难的_，但是客观题可以直接对比系统答案与标准答案是否一致，从而实现简单评估。_通过将部分主观题构造为多项或单项选择的客观题，进而实现简单评估_。

-  但不是所有的案例都可以构造为客观题，针对一些不能构造为客观题或构造为客观题会导致题目难度骤降的情况，这是需要用到第二种情况。

### 方法二 计算答案相似度

-  _生成问题的答案评估在 NLP 中也是一个大问题_。NLP 一般对生成问题采用人工构造标准答案并计算回答与标准答案相似度的方法来实现自动评估。

-  计算相似度的方法有很多，一般采用 __BLEU__ 来计算相似度。
