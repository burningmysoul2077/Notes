# 前言

>  **图的表示学习和嵌入指的是同一个事情，从不同角度的称呼而已**。*都是在嵌入空间中表示图的方法*。

-  “嵌入”是指将网络中的每个节点映射到低维空间，这将使我们深入了解节点的相似性和网络结构。
-  现代机器学习算法是为简单的序列或网格（例如，固定大小的图像/网格或文本/序列）而设计的，网络通常具有复杂的拓扑结构和多模型特征。 我们将探索嵌入方法来克服困难。

# 1. 节点表示学习

-  **节点嵌入的目标是对节点进行编码，使得嵌入空间中的相似性（例如点积）近似于原始网络中的相似性**，我们将探索的节点嵌入算法通常由三个基本阶段组成：
1.  定义一个编码器，即*从节点到嵌入的映射*。下面一张图来说明这个过程，编码器 𝐸𝑁𝐶 将节点 𝑢 和 𝑣 映射到低维向量 $𝑧_𝑢​$ 和 $𝑧_𝑣$​。
2. 定义节点相似度函数（即原始网络中相似性的度量），它指定向量空间中的关系如何映射到原始网络中的关系。
3.  优化编码器的参数，使得 𝑢 和 𝑣 在嵌入空间的相似度 $𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑢,𝑣)= 𝑧_𝑢^𝑇𝑧𝑣​$ 更高。




## 1.1 节点嵌入的方法

### a. 一般的随机游走: 深度游走

#### 随机游走

1.  给定一个图和一个起点，我们随机选择它的一个邻居，并移动到这个邻居；
2.  然后我们随机选择该点的邻居，并移动到它，以此类推。

-  以这种方式随机选择的点的序列就是图上的随机游走。

#### 深度游走

-  随机游走是一个比较宽泛的概念，只要它满足随机游走的想法。但是**深度游走**算法特指运行固定长度、无偏的随机游走。
-  可以按照以下步骤进行深度游走：

1.  从节点 𝑢 开始采用随机游走策略 𝑅 进行随机游走，得到附近的节点为 $𝑁_𝑅(𝑢)$。最简单的想法是从每个节点开始运行固定长度、无偏的随机游走，这就是深度游走。
2.  由于我们希望在嵌入空间中使得附近的节点嵌入相似度高，因此我们需要进行嵌入的优化，以使附近的节点在网络中靠近在一起。 我们可以优化嵌入以最大化随机游走共现的可能性，其损失函数为：

### b. 有偏的随机游走： Node2Vec

-  最简单的随机游走策略是深度游走，即从每个节点开始运行固定长度、无偏的随机游走。但是，这种游走策略太死板，会限制表征的学习。Node2Vec 提出了一种更高效的、灵活的、有偏的随机游走策略，以得到一个更好的 𝑁𝑅(𝑢)NR​(u)。Node2Vec 通过图上的广度优先遍历（Breath First Search, BFS）和深度优先遍历（Depth First Search, DFS）在网络的局部视图和全局视图之间进行权衡。

---

# 2. 图表示学习

-  如果需要在某些应用中*嵌入整个图 𝐺*（例如，对有毒分子与无毒分子进行分类、识别异常图）。

-  完成图嵌入有几种想法：
1.  简单的想法是在图 𝐺G 上运行图的节点嵌入技术，然后对图 𝐺G 中的节点嵌入求和（或平均）。
2. 引入“虚拟节点”来表示图并运行标准图节点嵌入技术。
3. 我们还可以使用匿名游走嵌入。 为了学习图嵌入，我们可以枚举所有可能的匿名游走，并记录它们的计数，然后将图表示为这些游走的概率分布。