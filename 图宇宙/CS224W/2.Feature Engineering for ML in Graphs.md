## 2.图的基本表示
- 同济子豪兄-中文精讲视频：[https://www.bilibili.com/video/BV1n84y1e7SF](https://www.bilibili.com/video/BV1n84y1e7SF)
- 扩展阅读：[https://github.com/TommyZihao/zihao_course/blob/main/CS224W/1-Intro.md](https://github.com/TommyZihao/zihao_course/blob/main/CS224W/1-Intro.md)

## Components of a Network
- Objects: nodes, vertices    $N$
- Interactions: links, edges  $E$
- System: network, graph    $G(N, E)$

## 本体图 Ontology
- 如何设计图的本体，取决于将来想解决什么问题
- Choice of the proper network representation of a given domain/problem determines our ability to use networks successfully
	- In some cases, there is a unique, unambiguous representation
	- In others, the representation is by no means unique
	- The way u assign links will determine the nature of the question u can study

## 图的种类
- **Undirected**, links: undirected, symmetrical, reciprocal相互、双向的
- **Directed**, links: directed, arcs
- **Heterogeneous Graphs异质图**: 多种节点和多种边， is defined as $G = (V, E, R, T)$
	- Nodes with node types $v_i \in V$
	- Edges with relation types $(v_i, r, v_j) \in E$
	- Node Type $T(v_i)$
	- Rletion type $r \in R$
- **Bipartite Graph**二分图: 如果一个异质图里面节点的种类是2，即两种节点
	- is a graph whose nodes can be divided into 2 disjoint sets $U$ and $V$ such that every link connects a node in $U$ to one in $V$; $U$ and $V$ are independent sets
	- Folded/Projected Bipartite Graphs 展开二分图

## 节点连接数
- Undirected
	- Node degree $k_i$: the # of edges adjacent to node $i$
	- Avg. degress: $\overline{k} = \left \langle k \right \rangle = \frac{1}{N} \sum\limits_{i=1}^N k_i = \frac{2E}{N}$
- Directed
	- in-degree out-degree, total = sum of in- and out-degree
	- $\overline{k} = \frac{E}{N} \quad \overline{k^{in}} = \overline{K^{out}}$
	- Source: Node with $k^{in} = 0$
	- Sink: Node with $k^{out} = 0$

## 邻接矩阵 Adjacency Matrix
- 无向图
	- 对称阵 $A_{ij} = A_{ji}$
	- 主对角线为0: $A_{ii} = 0$
	- 节点的Node degree，沿行或列求和均可 $k_i = \sum\limits_{j=1}^N A_{ij} \quad k_j = \sum\limits_{i=1}^N A_{ij}$
	- 连接总数 $L = \frac{1}{2} \sum\limits_{i=1}^N k_i = \frac{1}{2} \sum\limits_{ij}^N A_{ij}$
- 有向图
	- 非对称阵 $A_{ij} \neq A_{ji}$
	- 主对角线为0: $A_{ii} = 0$
	- 节点的 in-degree，沿列求和 $k_i^{out} = \sum\limits_{j=1}^N A_{ij}$；out-degree，沿行求和 $k_j^{in} = \sum\limits_{i=1}^N A_{ij}$
	- 连接总数 $L =  \sum\limits_{i=1}^N k_i^{in} = \sum\limits_{j=1}^N k_j^{out} = \sum\limits_{i, j}^N A_{ij}$
- Adjacency Matrices are Sparse稀疏矩阵, are filled with zeros

## 连接列表、邻接列表
- Represent graph as a **list of edges**，只记录存在指向连接的节点对
- Adjacency list: 每个节点占用一行，列出节点所有指向的all neighbors

## 其他种类的图
- Undirected
	- Unweighted，连接不带权重
		- $A_{ii} = 0$
		- $A_{ij} = A_{ji}$
		- $E = \frac{1}{2} \sum\limits_{i,j=1}^N A_{ij}$
		- $\overline{k} = \frac{2E}{N}$
	- Weighted
		- $A_{ii} = 0$
		- $A_{ij} = A_{ji}$
		- $E = \frac{1}{2} \sum\limits_{i,j=1}^N nonzero(A_{ij})$
		- $\overline{k} = \frac{2E}{N}$
	- Self-edges(self-loops)
		- $A_{ii} \neq 0$
		- $A_{ij} = A_{ji}$
		- $E = \frac{1}{2} \sum\limits_{i,j=1,i\neq j}^N A_{ij} + \sum\limits_{i=1}^N A_{ii}$
	- Multigraph
		- $A_{ii} = 0$
		- $A_{ij} = A_{ji}$
		- $E = \frac{1}{2} \sum\limits_{i,j=1}^N nonzero(A_{ij})$
		- $\overline{k} = \frac{2E}{N}$

## 图的连通性

#### Connectivity of Undirected Graphs
- **Connected graph**
	- Any 2 vertices can be joined by a path
	- The adjacency matrix can be written in a **block-diagonal分块对角** form
- A **disconnected graph** is made up by $\geq 2$ **connected components连通域**
	- Largest Component: **Giant Component**
	- **Isolated node**: 孤立的、离群的节点

#### Connectivity of Directed Graphs
- **Strongly connected directed graph**
	- 有向图中，任意两个节点可以相互触达，has a path from each node to every other node and vice versa
- **Weakly connected directed graph**
	- is connected if we disregard the edge directions
- Strongly connected components, SCCs 强连通域
	- In-component: nodes that can reach the SCC
	- Out-component: nodes that can reached from the SCC
- 六度空间理论

## 本章总结
- 本讲介绍了图的组成: 节点、顶点、连接、边、网络、图和本体图设计
- 介绍了图的种类，有向图和无向图，异质图和特殊的异质图-二分图和其他种类的图，带权重和不带权重的
- 介绍了有向图和无向图的节点连接数计算，邻接矩阵、连接列表和邻接列表
- 介绍了图的连通性，无向图、连通域，有向图、强连通图、弱连通图和强连通域，六度空间理论
***

## 3.传统图机器学习的特征工程

### Node and Edge Attributes
- 节点、连接、子图、全图都可以有特征
- Possible opts:
	- Weight
	- Ranking
	- Type
	- Sign
	- Properties depending on the structre of the rest of the graph
		- 多模态特征: 图像、视频、文本、音频

### Traditional ML Pipeline
- 特有的连接特征: 某节点在整个图中与其它节点的关系
- 传统图机器学习，特征工程: 本讲注重讲结构信息(连接特征)，而不讲自带属性(属性特征)
- Design features for nodes/links/graphs，抽取 D 个特征，编码为 D-维 向量
- Train an ML model: Random Forest，SVM，NN etc.
- Apply the model: Given a new node/link/graph，obstain its features and make a prediction
- 专注于构造 D-维 向量

### This Lecture: Feature Design
- **Using effective features over graphs is the key to achieving good model performance**
- Traditional ML pipeine uses *hand-designed features*
- This lecture focus on ***undirected graphs***

## 节点层面的特征工程
- Given: $G = (V, E)$
- Learn a function: $f: V \rightarrow \mathbb{R}$，输入某个节点的D-维向量，输出该节点是某类的概率
- Nodel-Level Tasks: 节点分类，半监督 semi-supervised 
- 为每个节点构造特征Characterize the structure and position of a node in the network:
	- **Node degree 节点的度**，即该节点的所连边/直接邻居的数目；有向图还分in-和out-
		- The degree $k_v$ of node $v$ = # edges(neighboring nodes) that $v$ has
		- Treats all neighboring nodes equally
		- 不看直接邻居的质量importance，只看数量
		- **Degree counts #(edges) that a node touches**
	- **Node centrality**
		- $C_v$ takes the node importance in a graph into acct
			- Eigenvector centrality 特征向量节点重要度
				- A node $v$  is important if *surrounded by important neighboring nodes $u \in N(v)$*
				- $C_v = \frac{1}{\lambda} \sum\limits_{u \in N(v)} c_u$
					- $\lambda$ is normalization constant( the largest eigenvalue of A)，归一化
				- The equation models centrality is in a recursive manner
					- Rewrite the equation in the matrix form $\lambda c = Ac$
						- $A$, Adjacency matrix $A_{uv} = 1 \enspace if \enspace u \in N(v)$
						- $c$: Centrality vector
						- $\lambda$: Eigenvalue
					- s.t. centraility $c$ is the *eigenvector of* $A$
					- Perron-Fronbenius Theorem: the largest eigenvalue $\lambda_{max}$ is alwz positive and unique
					- The eigenvector $c_{max}$ corresponding to $\lambda_{max}$ is used for centrality，节点 $v$ 的特征向量节点重要度就是 $c_{max}$ 的第 $v$ 个分量
			- Betweenness centrality，节点作为枢纽的重要性
				- A node is important if it lies on many shortest paths between other nodes $c_v = \sum\limits_{s \neq v \neq t} \frac{\#(shortest \enspace paths\enspace between\enspace s\enspace and\enspace t\enspace that\enspace contain\enspace v)}{\#(shortest \enspace paths \enspace between \enspace s \enspace and \enspace t)}$, $i.e. 分子分母 =  \frac{其中有多少对节点的最短路径通过节点 v }{除了 v 节点之外，两两节点对数 (对于连通域，分母都相同)}$
			- Closeness centrality，节点 $v$ 与图中其它节点的接近程度
				- A node is important if it has small shortest path lengths to all other nodes  $c_v = \frac{1}{\sum_{u \neq v} shortest \enspace path \enspace length \enspace between \enspace u \enspace and \enspace v}$ ， $i.e. 节点 v 到其它节点 u 最短路径长度求和$
			- and goes....
	- **Clustering coefficient 集群系数**，描述的是节点的邻居的集聚程度
		- Measures how connected $v's$ neighboring nodes are:  $e_v = \frac{\#(edges \enspace among \enspace \ neighboring \enspace nodes)}{(\mathop{k_v}\limits_2)  } \in [0, 1]$ ， $i.e. 分子分母 = \frac{v节点相邻节点两两也相连个数(三角形个数)}{v节点相邻节点两两对数，\#(node \enspace pairs \enspace among \enspace k_v \enspace neighboring \enspace nodes)}$ ，分母用来归一化
		- ego-betwork 自我中心网络
		- Clustering coefficient counts the #(triangles) in the ego-network
		- **Clustering coefficient counts #(triangles) that a node touches**
	- **Graphlets 非同构子图**: rooted connected induced non-isomorphic subgraphs
		- *Graphlet Degree Vector(GDV)*: A count vector of graphlets rooted at a given node，描述了节点 $u$ 的局部邻域拓补结构信息
			- 基于 graphlets 的特征，它计算以目标节点 $v$ 为root，能形成的不同graphlets的数目向量 = 描述了 $v$ 周围不同子结构的子图个数
			- **GDV counts #(graphlets) that a node touches**
		- **Graphlets r small subgraghs that descibe the structure of node $u$'s network neighborhood**
		- Considering graphlets of size 2-5 nodes we get:
			- Vector of 73 coordinates is a signature of a node that descrbes the topology of node's neighborhood
		- GDV provides a measure of a *node's $\underline{local}$ network topology*
			- 比较两个节点的GDV向量可以计算距离和相似度(比node degree 或 clustering coefficient更more detailed measure)

### Summary
- Categorize the ways to obtain node features
	- Importance-based features
		- Node degree
		- Different node centrality measures
		- PageRank/Katz Centrality/HITS hubs and Authorities
	- Structure-based features:
		- Node degree
		- Clustering coefficient
		- Graphlet count vector
***

## 连接层面的特征工程

### Link-Level Prediction Task
- The task is to predict new links based on the existing links. 通过已知连接补全未知连接
- The key is to design features for a pair of nodes. 两种思路:
	- 可取: 直接提取 link 的特征，把 link 变成 D-维 向量
	- 不可取: 把 link 两端节点的的 D-维 向量拼在一起。 因为丢失了 link 本身的连接结构信息

### Link Prediction as  a Task
- 2 formulations of the link prediction task:
	- Links missing at random:
		- Remove a random set of links and then aim to predict them
		- 用于 客观静态图 e.g. 蛋白质、分子
	- Links over time:
		- Given $G[t_0, t_0']$ a graph defined by edges up to time $t_o'$, *output a ranked list L* of edges(not in $G[t_0, t_0']$) that are predicted to appear in time $G[t_1, t_1']$
		- Evaluation:
			- $n = |E_{new}|$:  # new edges that appear during the test period $[t_1, t_1']$
			- 比较预测出的 top n 个连接和真实的 n 个连接
			- 用于 社交网络

### Link Prediction via Proximity邻近
- Methodology:
	- For each pair of nodes $(x, y)$ compute score $c(x, y)$
		- e.g. $c(x, y)$ could be the # of common neighbors of $x$ and $y$
	- Sort pairs $(x, y)$ by the decreasing score $c(x, y)$
	- *Predict top n* pairs as new links
	- C which of these links actually appear in $G[t_1, t_1']$

### Link-Level Features

#### Distance-based feature
- Shortest-path distance between 2 nodes
	- Dijkstra算法和Floyd算法
	- This neglects the degree of neighborhood overlap

#### Local neighborhood overlap 局部邻居重叠比例/两节点局部连接信息
- Captures # neighboring nodes shared between 2 nodes $v_1$ and $v_2$
	-  **Common neighbors**: $|N(v_1) \cap N(v_2)|$
	-  **Jaccard's coefficient**: $\frac{|N(v_1)\cap N(v_2)|}{|N(v_1) \cup  N(v_2)|}$
	-  **Adamic-Adar index**: 共同好友是不是社牛(共同好友的连接数求和) $\sum_{u \in N(v_1)\cap N(v_2)}  \frac{1}{log(k_u)}$
		- 如果共同好友的度越小，则说明两个节点的关系越紧密
		- 但是只考虑了一跳直接相连的邻居，没有考虑间接相连的邻居
	- Metric = 0 if 2 nodes don't have any neighbors in common

#### Global neighborhood overlap 全局邻居重叠比例/两节点在全图的连接信息
-  **Katz index**: count the # of walks of all lengths between a given pair of ndoes. 节点 $u$ 和 $v$ 之间长度为 $k$ 的路径个数
	- Computing through *powers of the graph adjancy matrix*
	- ![[Pasted image 20230216085808.png]]
	-  $A = \begin{pmatrix}  0 & 1 & 0 & 1 \\  1 & 0 & 0 & 1 \\  0 & 0 & 0 & 1 \\  1 & 1 & 1 & 0 \end{pmatrix}$
	- Recall: $A_{uv} = 1 \enspace if \enspace u \in N(V)$，说明 $u$ 是 $v$ 的邻居节点
	- Let $P_{uv}^{(k)}$ = \#walks of length $K$ between $u$ and $v$
	- $P_{uv}^{(1)}$ = \#walks of length 1 (direct neighborhood) between $u$ and $v$ = $A_{uv}$
	- $P_{uv}^{(2)} = \sum_i A_{ui} \ast P_{iv}^{(1)} = \sum_i A_{ui} \ast A_{iv} = A_{uv}^{(2)}$
		- 是将邻接矩阵乘以自己，$A_{ui}$ 表示：与 $u$ 隔一步的邻居 $i$ ，$P_{iv}^{(1)}$ 表示：  $i$ 是否与 $v$ 隔一步 
	- Sum up, 节点 $u$ 和 $v$ 之间长度为 $k$ 的路径个数 = $A_{uv}^{(k)}$ 矩阵的第 $u$ 行 第 $v$ 列元素
		- $A_{uv}$ specifies \#walks of *length 1 (direct neighborhood)* between $u$ and $v$
		- $A_{uv}^2$ specifies \#walks of *length 2 (neighbor of neighbor)* between $u$ and $v$
		- i.e. $A_{uv}^l$ specifies \#walks of length l between $u$ and $v$， 数学归纳法
	- Katz index = sum over all walk lengths
		- $S_{v_1v_2} = \sum\limits_{l=1}^{\infty} \beta^l A_{v_1v_2}^l$, $0 < \beta < 1$: discount factor
		- in closed-form: $S = \sum\limits_{i=1}^{\infty} \beta^i A^i = (I -\beta A)^{-1} - I$
		- 类比等比数列求和，用矩阵的几何级数推导

***

## 全图层面的特征工程
- 提取出的特征反映全图的结构特点

### Graph Kernel
- Goal: Design graph feature vector $\phi(G)$ D-维向量
- Key idea: 
	- Bag-of-Words(BoW) for a graph - Bag of Nodes
		- 把图看作文章，把节点看作单词
		- 只看节点有没有存在，并没看到节点之间的连接结构
	- Bag of Node Degrees
		- 只看 Node Degree 个数，不看节点，不看连接结构
	- **Graphlet Kernel** and **Weisfeiler-Lehman(WL) Kernel** use $Bag-of-\ast$ representation of graph

### Graphlet
#### Graphlet Feature
- Let $g_k = (g_1, g_2, \dots, g_{n_k})$ be a list of graphlets of size $k$
- Count the # of different graphlets in a graph
- 这里的graphlet 的定义域node-level features 里的 graphlet 有点不同
	- 这里可以存在孤立节点 isolated nodes
	- 计数全图 graphlet 个数，而非特定节点邻域的 graphlet 个数
- Define the graphlet count vector $f_G \in \mathbb{R}^{n_k}$ as ${(f_G)}_i = \#(g_i \subseteq G) \enspace for \enspace i=1,2,\dots,n_k$ , 第 $i$ 个 graphlet 在全图中的个数

#### Graphlet Kernel
- Given 2 graphs $G \enspace and \enspace G'$, graphlet kernel is computed as $K(G, G') = f_G^Tf_{G'}$,两个图 graphlet count vector 数量积
- Normalization: $h_G = \frac{f_G}{Sum(f_G)}$, s.t. $K(G, G') = h_G^Th_{G'}$, cuz if $G$ and $G'$ have different sizes, that will greatly skew the value
- Limitations: Counting graphlets is expensive
	- Counting size-$k$ graphlets for a graph with size n by enumeration takes $n^k$,多项式复杂度
	- The worst-case since *subgraph isomorphism test* is **NP-hard**
	- 就算 node degree is bounded by $d$, 复杂度降为 $O(nd^{k-1})$

### Weisfeiler-Lehman Kernel
- Idea: Use neighborhood structure to iteratively enrich node vocabulary
	- Generalized version of *Bag of node degrees*
- Algorithm: **color refinement**

#### Color Refinement
1. Assign initial colors
2. Aggregate neighboring colors
3. Hash aggregated colors
4. Repeat aggregate neighboring colors, then hash aggregated colors
- Given: A graph $G$ with a set of nodes $V$
	- Assign an initial color $c^{(0)}(v) to each node v$
	- Iteratively refine node colors by  $c^{(k+1)}(v) = HASH(\{ c^{(k)}(v), \{ c^{(k)}(u)_{u\in N(v)} \} \})$, where *HASH* maps different inputs to different colors
	- After $K$ steps of color refinement, $c^{{K}}(v)$ summarizes the structure of *K-hop* neighborhood

#### Weisfeiler-Lehman Graph Features
- After color refinement, WL kernel counts # of nodes with a given color.

#### Weisfeiler-Lehman Kernel
- The WL kernel value = the inner product of the color count vectors
- WL kernel is *computationally efficient*
	- Time complexity for color refinement at each step is linear in \#(edges)
- When computing a kernel value, only colors appeared in the two graphs need to be tracked
	- i.e. \#(colors)  at most = the ttl# of nodes
- Counting colors takes linear-time w.r.t. #(nodes)
- Ttlly, time complexity if *linear in #(edges)*

## Summary
- Graphlet Kernel
	- Represented as *Bag-of-graphlets*
	- Computationally expensive
- Weisfeiler-Lehman Kernel
	- Represented as *Bag-of-colors*
	- Coumputationally efficient
	- Closely related to GNN
- Other kernels

## BG: Kernel Methods
- Kernel methods are widely-used for traditional ML for graph-level prediction
- Idea: Design kernels instead of feature vectors
- A quick into to Kernels:
	- Kernel $K(G, G') \in \mathbb{R}实数标量$ measures similarity b/w data
	- Kernel matrix $K = (K(G, G'))_{G, G'}$ must always be positivie semidefinite半正定 (i.e. has positive eigenvalues)
	- There exists a feature representation $\phi (\cdot)$ s.t. $K(G, G') = \phi (G)^T\phi(G')$ 两个向量的数量积
	- Once the kernel is defined, off-the-shelf ML model, such as *kernel SVM*, can be used to make predictions

## 本章总结
- 本讲介绍了传统图机器学习的特征工程，包括属性特征和连接特征，本讲着重于后者
- 介绍了特征工程的目的是创造D-维向量，包括节点层面特征、连接层面特征、全图层面特征
- 介绍了节点层面特征中基于节点度的，节点重要性的，聚集系数的和graphlets的特征
- 介绍了连接层面特征中的基于距离的和基于局部/全图邻居覆盖的特征
- 介绍了全图层面特征中的基于graphlet kernel的和wl kernel的特征
- 介绍了核方法的各方面信息
