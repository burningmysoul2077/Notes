* [前言](#前言)
* [数值稳定性](#数值稳定性)
  * [神经网络的梯度](#神经网络的梯度)
* [数值稳定性的常见两个问题](#数值稳定性的常见两个问题)
  * [梯度爆炸 gradient exploding](#梯度爆炸-gradient-exploding)
  * [梯度消失 gradient vanishing](#梯度消失-gradient-vanishing)
  * [举例：MLP](#举例mlp)
    * [梯度爆炸](#梯度爆炸)
      * [使用ReLU作为激活函数](#使用relu作为激活函数)
      * [梯度爆炸问题](#梯度爆炸问题)
    * [梯度消失](#梯度消失)
      * [使用Sigmoid作为激活函数](#使用sigmoid作为激活函数)
        * [梯度消失的问题](#梯度消失的问题)
  * [小结](#小结)
* [模型初始化和激活函数](#模型初始化和激活函数)
  * [让训练更加稳定](#让训练更加稳定)
  * [假设：让每层的均值/方差是一个常数](#假设让每层的均值方差是一个常数)
    * [权重初始化](#权重初始化)
  * [举例：MLP](#举例mlp-1)
    * [假设（没有激活函数）](#假设没有激活函数)
      * [正向方差](#正向方差)
      * [反向均值和方差](#反向均值和方差)
      * [Xavier 初始](#xavier-初始)
    * [假设线性的激活函数](#假设线性的激活函数)
      * [正向](#正向)
      * [反向](#反向)
    * [检查常用激活函数](#检查常用激活函数)
* [额外阅读](#额外阅读)
* [总结](#总结)
* [14 数值稳定性+模型初始化和激活函数Q&A](#14-数值稳定性模型初始化和激活函数qa)


# 前言

>  到目前为止，我们实现的每个模型都是根据某个预先指定的分布来初始化模型的参数。
>  有人会认为初始化方案是理所当然的，忽略了如何做出这些选择的细节。甚至有人可能会觉得，初始化方案的选择并不是特别重要。 
>  相反，初始化方案的选择在神经网络学习中起着举足轻重的作用， 它对保持数值稳定性至关重要。 此外，这些初始化方案的选择可以与非线性激活函数的选择有趣的结合在一起。 我们选择哪个函数以及如何初始化参数可以决定优化算法收敛的速度有多快。 糟糕选择可能会导致我们在训练时遇到梯度爆炸或梯度消失。 


# 数值稳定性

-  数值稳定性是深度学习中比较重要的点，特别是当神经网络变得很深的时候，数值通常很容易变得不稳定。

## 神经网络的梯度

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230326154529.png)


-  如图所示考虑一个具有 d 层、输入 𝐱 和输出 $y$ 的深层网络。 
	-  每一层由变换 $𝑓_d$ 定义， 该变换的参数为权重 $𝐖_d$， 
	-  其隐藏变量是 $𝐡^t$ ，表示第 $t-1$ 层的输出，经过一个 $f_{t}$ 函数后，得到第 $t$ 层的输出
	-  最终输出 $\text y$ 的表示：输入 $\textbf x$ 经过 d 层的函数作用，最后被损失函数作用得到输出 $\text y$

-  网络可以表示为： $\textbf h^t = f_t(\textbf h^{t - 1})$  and  $\text{y} = \mathscr{l} \circ f_d \circ \cdots \circ f_1(\textbf {x})$

-  计算损失函数 $\ell$ 关于第 $t$ 层参数 $W_{t}$ 的梯度
	-  由链式法则得到图中公式

-  需要进行 d-t 次 *矩阵乘法*
	-  所有的 $h$ 都是一些 **向量**，向量关于向量的导数是一个矩阵，维数为 分子维度 x 分母维度 ，
	-  这也是导致数值稳定性问题的主要因素，因为做了太多次的矩阵乘法。

- 不稳定梯度带来的风险不止在于数值表示，不稳定梯度也威胁到我们优化算法的稳定性

# 数值稳定性的常见两个问题

## 梯度爆炸 gradient exploding

-  参数更新过大，破坏了模型的稳定收敛
-  假设梯度都是一些比 1 大的数
	-  比如 1.5，做 100 次乘积之后 $\approx 4\times 10^{17}$ 
	-  这个数字很容易带来浮点数上限的问题

## 梯度消失 gradient vanishing

-  假设梯度都是一些比1小的数
	-  比如 0.8，做 100次 乘积之后得到 $\approx 2\times10^{-10}$ 
	-  也会带来浮点数下溢的问题

## 举例：MLP

- 加入如下 MLP (为了简单省略了偏移)

- $f_t(\textbf {h}^{t-1}) = \sigma(\textbf {W}^t\textbf h^{t-1})$
	- $\sigma$ 是激活函数
	- 这个公式里面，$\textbf h^t$ 和 $\textbf h^{t-1}$ 都是向量，$f_t$ 是他们之间的函数关系
		- 第 $t$ 层的权重矩阵 $\textbf {W}^t$ 作用于 $t-1$ 层的输出 $\textbf h^{t-1}$ 后经过激活函数 $\sigma$ 得到 $\textbf h^t$
		- 注意激活函数是逐元素计算


- $\frac{\partial \textbf h^t}{\partial \textbf h^{t-1}} = \text {diag} (\sigma'(\textbf W^t \textbf h^{t-1}))(W^t)$
	- 这里用到链导法则，激活函数 $\sigma$  先对内部向量逐元素求导，然后把求导后这个向量变成对角矩阵（可以理解为链导法则中内部向量 $W^{t}h^{t-1}$ 对自身进行求导，变成一个 nxn 的对角矩阵，更多请参考[邱锡鹏 《神经网络与深度学习》](https://nndl.github.io/nndl-book.pdf)）

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230327180808.png)


- 视频中**勘误说明**：链导法则中  $\frac{\partial W^{t}h^{t-1}}{\partial h^{t-1}}= W^{t}$  而不是 $\left (W^{t} \right )^{T}$ （这点由分子分母维度也容易推出），故最终求导结果包含 $W^{t}$，而不是其转置。
- 以上公式说明引用自 [动手学深度学习笔记](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#245-%E5%81%87%E8%AE%BE%E7%BA%BF%E6%80%A7%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0)

### 梯度爆炸

#### 使用ReLU作为激活函数

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230327175512.png)

-  由于激活函数 Relu 求导后只能是 1/0，变为对角矩阵的斜对角线元素后，与 $\textbf W^{i}$ 做乘积，斜对角线为 1 的部分会使得 $\textbf W$ 中元素保留，最终该连乘式中有一些元素来自 $\prod\left ( W^{i} \right )$，另外一些元素是 0 ，如果大部分 $W^{i}$ 中 值都大于1，且层数比较大，那么连乘之后可能导致梯度爆炸的问题。

#### 梯度爆炸问题

-  值太大了，超出值域 infinity
	-  对于 16 位浮点数尤为严重，而现在 Nvidia GPU用 16 位浮点数更快
	-  16位浮点数的数值区间 [6e-5 , 6e4]，这个区间其实很小

-  对学习率敏感
	-  如果学习率太大 → 权重大，即大参数值 → 梯度是权重的乘法，这样带来更大的梯度，如此循环几次，容易导致梯度爆炸
	-  如果学习率太小 → 训练无进展
	-  我们可能需要在训练过程中不断调整学习率，但是调整范围比较小

### 梯度消失

#### 使用Sigmoid作为激活函数

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328084103.png)



-  蓝色曲线  -  值的函数

-  黄色曲线  -  梯度，注意到：当输入 x 值取 ±6 时，此时梯度已经变得很小，由图也可以看出，当输入值稍大或稍小都很容易引起小梯度。

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328084251.png)

-  所以最终连乘式中 $\prod \text{diag}\left ( \sigma ^{'}\left ( W^{i}h^{i-1} \right ) \right )$ 项乘出来会很小，导致整个梯度很小，产生梯度消失问题。

##### 梯度消失的问题

-  最严重的时候，梯度值变为 0
	- 对 16 位浮点数尤为严重，因为 < 6e-5时可以被当作是 0

-  训练没有进展
	-  不管如何选择学习率，由于梯度已经为 0 了，学习率 x 梯度 = 0

-  对于底部层尤为严重
	-  仅仅顶部层训练得较好。第 $t$ 层导数包含 d-t 个矩阵乘积，越往底层走，$t$ 越小，乘得越多，梯度消失越严重，所以底部层效果更差。
	-  无法让神经网络更深。只能把顶部层训练得比较好，底部层跑不动,只把顶层训练好，这和给一个浅的神经网络没有什么区别。

## 小结

-  当数值过大或者过小时，会导致数值问题
-  常发生在深度模型中，因为其会对 n 个数累乘


# 模型初始化和激活函数

## 让训练更加稳定

-  我们的一个核心问题是如何让训练更稳定，梯度值不要太大也不要太小

-  目标：让梯度值在合理的范围内
	-  例如 [1e-6, 1e3]

-  常见方法：
	-  将乘法变加法：
		- CNN: ResNet（跳跃连接，如果很多层，加入加法进去）
		- RNN: LSTM（带时序的，引入记忆细胞，更新门，遗忘门，通过门权重求和，控制下一步是否更新）
	-  归一化：
		-  梯度归一化（比如归一化均值为 0，方差为 1）
		-  梯度裁剪(clipping)：比如大于/小于一个固定的阈值，就让梯度等于这个阈值，将梯度限制在一个范围中。
			- 可以缓解梯度爆炸

-  如果做合理的权重初始和合理的激活函数：本节讲述重点

## 假设：让每层的均值/方差是一个常数

- **将每层的输出和梯度都看做随机变量**
	- 比如第 i 层有 100 维，就将输出和梯度分别看成 100 个随机变量

- **让它们的均值和方差都保持一致**
	- 我们的目标，这样不管神经网络多深，最后一层与第一层差不多，均值为 0 、方差为某个固定值，希望我的输出和梯度都在这个值区间内，从而不会梯度爆炸和消失

-  根据假设，数学上来讲：

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328085316.png)

-  正向
	-  输出是标量，均值为 0 
	-  方差为 a ， 常数
-  反向
	-  梯度 均值为 0 ，
	-  方差为 b，常数
-  这些都是我们的假设，我们希望设计成这样，使得满足我们的需求 

### 权重初始化

-  在合理值区间里随机初始参数
-  训练开始的时候更容易有数值不稳定
	-  远离最优解的地方，损失函数表面可能很复杂
	-  最优解附近表面会比较平

-  使用 $\mathscr N$(0, 0.01)分布来初始可能对小网络没问题，但不能保证深度神经网络

## 举例：MLP

-  还是回到之前的 MLP 例子

### 假设（没有激活函数）

-  $w_{i, j}^t$ 是 i.i.d, 那么 $\mathbb{E}[w_{i, j}^t] = 0 \enspace,\enspace Var[w_{i, j}^t] = \gamma_t$
	-  即每一层 *权重* 中的变量均为 *独立同分布* ，那么均值为 0 、方差为 $\gamma_t$

-  $h_i^{t - 1}$ 独立于 $w_{i, j}^t$
	-  即每一层 *输入* 的变量 *独立于* 该层 *权重* 变量。同时 *输入变量* 之间 *独立同分布*。

-  假设没有激活函数 $\textbf h^t = \textbf W^t \textbf h^{t - 1}$，这里 $\textbf W^t \in \mathbb{R}^{n_t \times n_{t - 1}}$
	-  得出  $\mathbb{E}[h_{i}^t] = \mathbb{E} [\sum\limits_j w_{i, j}^th_j^{t - 1}] = \sum\limits_j\mathbb{E}[w_{i, j}^t]\mathbb{E}[h_j^{t - 1}] = 0$
		-  可以求得该层输出的期望为 0。
	-  没有激活函数是先简化分析，之后会考虑有激活函数的情况

-  此处用到了一个重要性质：一般来说，乘积的期望不等于期望的成绩，除非变量相互独立。

-  [引用](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7.md#245-%E5%81%87%E8%AE%BE%E7%BA%BF%E6%80%A7%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0) 更多均值、方差运算可以参考[期望、方差、协方差及相关系数的基本运算](https://blog.csdn.net/MissXy_/article/details/80705828)

#### 正向方差

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328092510.png)

-  通过上图的推导，我们发现需要  $n_{t-1}\gamma _{t}=1$

#### 反向均值和方差

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328101557.png)

-  反向的情况和正向的类似，不过此时我们需要满足的式子变为 $n_{t}\gamma _{t}=1$
-  因为我们的假设，期望都是为 0 

#### Xavier 初始

-  上述推导带来的问题：难以同时满足 $n_{t-1}\gamma _{t}=1$  和  $n_{t}\gamma _{t}=1$ "
	-  这是因为 $n_{t-1}$ 是 $t$ 层输入的维度，$n_t$ 是输出的维度，这就需要每层输入输出的维度都相同

-   **Xavier** 权衡

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328102957.png)

-  这意味着如果能确定每层输入、输出维度大小，则能确定该层权重需要满足的方差大小

-  对当前层权重初始化方式：
	-  正态分布
	-  均匀分布，均值/方差满足 Xavier 的假设

-  适配权重形状变换，特别是 $n_t$
-  Xavier , 说明权重初始化时的方差是根据 输入/输出维度决定的
	- 尤其当输入输出维度相差比较大，每个网络变化比较大时，可以根据输入输出维度来适配 

### 假设线性的激活函数

-  真实情况下，我们并不会用线性的激活函数
	-  相当于没有进行激活）
	-  这里为了简化问题，假设激活函数是线性的

#### 正向

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328104146.png)

-  上述推导表明，为了使得前向传播的均值为 0，方差固定的话，激活函数只能是 $f(x)=x$，这种恒等映射

#### 反向

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328104358.png)

-  正向和反向结果一样

### 检查常用激活函数

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230328104617.png)

-  对于常用激活函数：tanh，relu满足在零点附近，确实近似 $f(x)=x$

-  sigmoid函数在零点附近不满足要求，可以对sigmoid函数进行调整
	-  根据Taylor展开式，调整其过原点

# 额外阅读

-  上面的推理仅仅触及了现代参数初始化方法的皮毛。 
-   深度学习框架通常实现十几种不同的启发式方法。 
-   此外，参数初始化一直是深度学习基础研究的热点领域。 
-   其中包括专门用于参数绑定（共享）、超分辨率、序列模型和其他情况的启发式算法

# 总结

- 本节介绍了数值稳定性，当深度神经网络过深时，因为其会对n个数累乘，带来的不稳定的梯度，会导致数值问题。
- 介绍了两种常见的数值稳定性问题，梯度消失和梯度爆炸
- 介绍了让训练稳定的一种方法： 稳定合理的权重初始值(如Xavier)和激活函数的选取(如relu, tanh, 调整后的sigmoid)可以提升数值稳定性

------

# 14 数值稳定性+模型初始化和激活函数Q&A

- `nan, inf是怎么产生的以及怎么解决的？`
>   inf一般是初始值太大了，nan一般是梯度很小了，再除个0
>   NaN 和 Inf 具体怎么产生的：参考 [出现nan、inf原因](https://blog.csdn.net/qq_16334327/article/details/86526854)
>   如何解决 参考本节：把学习率调小，权重初始方差的区间小一些
>   具体如何解决：参考 [深度学习中nan和inf的解决](https://blog.csdn.net/u011119817/article/details/103908065) 以及 [训练网络loss出现Nan解决办法 ](https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况： 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。)

- `老师，训练过程中，如果网络层的输出的中间层特征元素的值突然变成nan了，是发生梯度爆炸了吗？还有什么可能的原因`
>  一般来说是的
> 具体请参考[训练网络loss出现Nan解决办法 ](https://zhuanlan.zhihu.com/p/89588946#:~:text=一般来说，出现NaN有以下几种情况： 1.,如果在迭代的100轮以内，出现NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现NaN为止，一般来说低于现有学习率1-10倍即可。)

- `LSTM这里乘法变加法，这里乘法说的是梯度的更新的时候用的梯度的乘法是吗`
>  LSTM采用指数、对数操作单元，改变累乘

- `老师，让每层方差是一个常数的方法，您指的是batch normalization吗？想问一下bn层为什么要有伽马和贝塔？去掉可以吗`
>  让每层方差是一个常数，和 batch norm 没有太多关系
>  本节介绍的方法是合理地初始化权重和设置激活函数
>  这些以后会讲，batch norm可以让你的输出变成一个均值为0，方差差不多是一个固定值的东西，但它不一定能保证你的梯度。

- `随机初始化，有没有一种最好的/最推荐的概率分布来找到初始随机值吗？`
>没有更好想法的时候，就用 xavier

- `强制使得每一层的输出特征均值为 0， 方差为 1，是不是损失了网络的表达能力，改变了数据的特征？会降低学到的模型的准确率？`
>并不是这样。只是限制了数值的区间

- `一般权重实在每个epoch结束以后更新的把？`
>权重是在每次迭代，iterate之后更新
