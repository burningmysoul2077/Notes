

- 前几节课着重介绍了机器能够学习的条件并做了详细的推导和解释。机器能够学习必须满足两个条件：  
	1. 假设空间 $H$ 的Size $M$是有限的，即当 $N$ 足够大的时候，那么对于假设空间中任意一个假设 $g$，$E_{out} \approx E_{in}$
	2. 利用算法 $A$ 从假设空间 $H$ 中，挑选一个 $g$，使 $E_{in} \approx 0$，则 $E_{out} \approx 0$

- 这两个条件，正好对应着 test 和 trian 两个过程
	- Train 的目的是使 损失期望 $E_{in}(g) \approx 0$
	- Test 的目的是使将 算法 用到 新的样本 时的 损失期望 也尽可能小，即$E_{out} \approx 0$

- 正因为如此，上节引入了 break point，并推导出只要 break point 存在，则 $M$ 有上界，一定存在 $E_{out} \approx E_{in}$

### Definition of VC Dimension

- 首先，如果一个假设空间 $H$ 有break point $k$，那么它的成长函数是有界的，它的上界称为 Bound function
- 根据数学归纳法，Bound function也是有界的，且上界为 $N^{k-1}$。从下面的表格可以看出，$N^{k-1}$ 比 $B(N,k)$ 松弛很多。

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230319153112.png)

- 根据上一节课的推导，VC bound 可以转换为：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230319153141.png)

1. 若假设空间 $H$ 有break point $k$，且 $N$ 足够大，则根据 VC bound 理论，算法有良好的泛化能力
2. 在假设空间中选择一个 $g$，使 $E_{in} \approx 0$，则其在全集数据中的错误率会较低

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230319153322.png)

#### VC Dimension

- VC Dimension  -  the formal name of maximum non-break point
- 就是某假设集 $H$ 能够 shatter 的最多 inputs 的个数 $N$，即最大完全正确的分类能力
	- $d_{vc}(\cal H)$  is largest $N$ for which $m_{\mathcal{H}}(N) = 2^N$
	- $N \leq d_{vc}$  =>  $\cal H$ can shatter some $N$ inputs
	- $k > d_{vc}$  =>  $k$ is a break point for $\cal H$
	- $d_{vc}(\cal H) =$  minimnm $k$ - 1
- 注意，只要存在一种分布的 inputs 能够正确分类也满足

- shatter 的英文意思是“粉碎”，也就是说对于 inputs 的所有情况都能列举出来
- 例如对 $N$ 个输入，如果能够将 $2^N$种 情况都列出来，则称该 $N$ 个输入能够被hypothesis $H$ shatter。

- 小结，$if \enspace N \geq 2 \enspace , \enspace d_{vc} \geq 2 \enspace m_{\mathcal{H}}(N) \leq N^{d_{vc}}$

#### The Four VC Dimensions

- 回顾之前介绍的四种例子，它们对应的 VC Dimension：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320085450.png)

#### VC Dimension and Learning

- 用 $d_{vc}$ 代替 $k$，那么VC bound的问题也就转换为与 $d_{vc}$ 和 $N$ 相关了
- 同时，如果一个假设集 $\cal H$ 的 $d_{vc}$ 确定了，则就能满足机器能够学习的第一个条件$E_{out} \approx E_{in}$
	- regardless of learning algorithm $\cal A$
	- regardless of input distribution $P$
	- regardless of target function $f$

### VC Dimension of Perceptrons

#### 2D PLA Revisited

- Linearly seperable $\cal D$  =>  PLA can converge  => if $T$ large enough,  $E_{in}(g) = 0$
- 已知 Perceptrons 的 $k=4$，即 $d_{vc} = 3$
- 根据 VC Bound 理论，当 $N$ 足够大的时候，$E_{out}(g) \approx E_{in}(g)$
- 如果找到一个 $g$，使 $E_{in} \approx 0$，那么就能证明 PLA 是可以学习的

- 下一个问题就是，这个 PLA 是2D的，需要 general PLA with more than 2 features

#### VC Dimension of Perceptrons

- 1D Perceptron, aka pos/neg rays， $d_{vc} = 2$ 
- 2D Perceptrons， $d_{vc} = 3$ 
- 那假设：d-D perceptrons:  $d_{vc} = d + 1$ 

- 要证明的话，只需分两步证明：
	1. $d_{vc} \geq d + 1$ 
	2. $d_{vc} \leq d + 1$ 

#### $d_{vc} \geq d + 1$ 

- 在 d-D percepton，只要找到某一类的 d+1 个 inputs 可以被 shatter 的话，那么必然得到 $d_{vc} \geq d + 1$ 。
- 所以，构造一个 d维 的矩阵 $X$ 能够被 shatter 就行。
- $X$ 是 d维 的，有 $d+1$ 个inputs，每个 inputs 加上第零个维度的常数项 1，得到 $X$ 的矩阵
- 矩阵中，每一行代表一个inputs，每个 inputs 是 $d+1$ 维的，共有 $d+1$ 个inputs
	- $X$ 明显是可逆的

- Shatter 的本质是假设空间 $\cal H$ 对 $X$ 的所有情况的判断都是对的，即总能找到权重 $W$，满足 $X∗W=y$，$W=X−1∗y$
- 矩阵 $X$ 的逆矩阵存在，那么d维 的所有 inputs 都能被 shatter，也就证明了第一个不等式

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320085429.png)

- 在 d维 里，如果对于任何的 $d+2$ 个inputs，一定不能被 shatter，则不等式成立。

- 我们构造一个任意的矩阵 $X$，其包含 $d+2$ 个inputs，该矩阵有 $d+1$ 列，$d+2$ 行。
- Linear dependence  -  这$d+2$ 个向量的某一列一定可以被另外 $d+1$ 个向量线性表示，可表示为：
	- $X_{d+2} = a_11X_1 + a_2X_2 + \cdots + a_{d+1}X_{d+1}$
- 其中，假设 $a_1>0，a_2,⋯, a_{d+1} < 0$
- 那么如果 $X_11$ 是正类，$X_2,⋯,X_d$ 均为负类，则存在 $W$，得到如下表达式：
	- $w^TX_{d+2} = a_1w^TX_1 + a_2w^TX_2 + \cdots + a_{d+1}w^TX_{d+1} > 0$
- 因为其中蓝色项 > 0，代表正类；红色项 < 0，代表负类
- 所有对于这种情况，$X_{d+2}$ 一定是正类，无法得到负类的情况
	- 即 $d+2$ 个 inputs 无法被 shatter

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320090456.png)

- 综上可证，$d_{vc} = d + 1$

### Physical Intuition VC Dimension

#### Degrees of Freedom

- Hypothesis parameters $w = (w_0, w_1, \cdots, w_d)$ : creates degrees of freedom
	- 即自由度。自由度是可以任意调节的，如同旋钮一样。
- VC Dimension 代表了假设空间的分类能力，即反映了 $\cal H$ 的自由度，产生 dichotomy 的数量 = features 的个数，但不绝对

- 例如，对 2D Perceptrons，线性分类，$d_{vc} = 3$，则 $W=\{w_0, w_1, w_2 \}$
	- 只要 3 个 features 就可以进行学习，自由度 = 3

#### M and $d_{vc}$

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092012.png)

### Interpreting VC Dimension

- VC Bound

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092134.png)

- 根据之前的泛化不等式，如果 $|E_{in} − E_{out}|>ϵ$，即出现 bad 的概率 $\leq δ$。
- 那么反过来，对于 good 发生的概率最小为 $1−δ$，则对上述不等式进行重新推导：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092246.png)

- $ϵ$ 表现了假设空间 $\cal H$ 的泛化能力，$ϵ$ 越小，泛化能力越大

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092416.png)

#### THE VC Message

- 至此，已经推导出泛化误差 $E_{out}$ 的边界，因为我们更关心其上界（$E_{out}$可能的最大值），即：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092510.png)

- 上述不等式的右边第二项称为 模型复杂度，其模型复杂度与样本数量 $N$、假设空间 $\mathcal{H}(d_{vc})$、$ϵ$ 有关。

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320092652.png)

- 通过该图得出如下结论：
	1. $d_{vc}$ 增大，$E_{in}$ 降低，但是 Ω 越大（模型复杂）
	2.  $d_{vc}$ 减小，Ω 越小（模型简单) , 但是 $E_{in}$ 变大
	3. 随着 $d_{vc}$ 增大，$E_{out}$  会先减小再增大
	4. best $d^*_{VC}$ in the middle 

#### VC Bound Rephrase: Sample Complexity

- 样本复杂度 Sample Complexity 

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320093338.png)

- 通过计算得到 $N = 29300$，刚好满足 $δ = 0.1$
- $N$ 大约是 $d_{vc}$ 的10000倍
	- 这个数值太大了，实际中往往不需要这么多的样本数量，大概只需要 $d_{vc}$ 的10倍就足以
- $N$ 的理论值之所以这么大是因为 VC Bound 过于宽松，得到的是一个比实际大得多的上界。

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230320093527.png)

- VC Bound是非常宽松的，而如何收紧它却不是那么容易，这也是机器学习的一大难题。
- 但是，VC Bound 基本上对所有模型的宽松程度是基本一致的
	- 所以，不同模型之间还是可以横向比较。
- 综合下来，VC Bound 宽松对机器学习的可行性没有太大影响。

## 总结

- 本节介绍了 VC Dimension 的本质就是最大的 non-break point， 即minimum $k - 1$
- 介绍了通过两步如何证明 d-D Perceptrons 的 VC Dimension 是 $d+1$
- 介绍了degree of freedom自由度概念，$d_{vc}$ 就可以看作是自由度
- 介绍了反向推导出 $E_{out}(g)$ 的上届。 模型复杂度与样本数量 $N$、假设空间 $\mathcal{H}(d_{vc})$、$ϵ$ 有关。
- 介绍了最终结论：$d_{vc}$ 不能过大也不能过小。选取合适的值，才能让 $E_{out}$ 足够小，使假设空间 $\cal H$ 具有良好的泛化能力。
- 介绍了 VC Bound 是非常宽松的，往往极大于实际应用中的 $N$，但 VC Bound对所有模型的宽松程度基本一致
