
- 机器学习的核心问题，严格证明为什么机器可以学习  

### Recap and Preview
- 基于统计学的机器学习流程图：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316133820.png)

- 该流程图中
	- 机器学习的前提，训练样本 $\cal D$ 和最终测试 $\cal h$ 的样本都是来自同一个数据分布
	- 训练样本 $\cal D$ 应该足够大，且 hypothesis set 的个数有限
	- 根据 Hoeffding's inequality，才不会出现 Bad Data，保证 $E_{in}(h) \approx E_{out}(h)$，即有很好的泛化能力
	- 同时，通过训练，得到使 $E_{in}(h)$ 最小的 $h$，作为模型最终的 $g$，$g$ 接近于目标函数 $f$。

- 总结一下前四节的主要内容：
	- 第一节，介绍了机器学习的定义，目标是找出最好的 $g$，使 $g≈f$，保证$E_{out}(g) \approx 0$
	- 第二节，介绍了如何让 $E_{in} \approx 0$，可以使用 PLA、Pocket 等算法来实现
	- 第三节，介绍了机器学习的分类
	- 第四节，介绍了机器学习的可行性，通过统计学知识，把 $E_{in}(g) \enspace   E_{out}(g)$ 联系起来，证明了在一些条件假设下，$E_{in}(g) \approx E_{out}(g) \approx 0$成立

#### Two Central Questions

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316134623.png)

- 这样把机器学习的主要目标分成两个核心的问题：
	1. $E_{in}(g) \approx E_{out}(g)$
	2. $E_{in}(g)$ 足够小

#### Trade-off on M
- 之前介绍的机器学习可行的一个条件是 hypothesis set 的个数 $M$ ($|\cal H|$)是有限的，那 $M$ 跟上面这两个核心问题有什么联系
- small $M$ ，
	- 第一个核心问题成立  -  由 Hoeffding's inequality，$E_{in}(g) \approx E_{out}(g)$，即保证
	- 不能保证第二个核心问题成立  -  但 $M$ 很小时，算法 $\cal A$ 可以选择的 hypothesis 有限，不一定能找到使$E_{in}(g)$ 足够小的 hypothesis
- large $M$ ，
	- 第一个核心问题可能不成立  -  同样由 Hoeffding's inequality，$E_{in}(g) \enspace E_{out}(g)$的差距可能比较大，。
	- 第二个核心问题可能成立 - 而 $M$ 很大，使的算法 $\cal A$ 的可以选择的 hypothesis 就很多，很有可能找到一个 hypothesis，使 $E_{in}(g)$ 足够小
- Above all, $M$ 的选择直接影响机器学习两个核心问题是否满足，不能太大也不能太小
- 那么如果M无限大的时候，是否机器就不可以学习了呢？例如 PLA 算法中直线是无数条的，但是PLA能够很好地进行机器学习，这又是为什么呢？
- 如果能将无限大的 $M$ 限定在一个有限的 $m_{\cal H}$ 内，问题似乎就解决了

### Effective Number of Line

#### Where Did $M$ Come From
- 先看一下之前推导的 Hoeffding's inequality：
	- $P[|E_{in}(h) - E_{out}(h)| > \epsilon] \leq 2\cdot M \cdot\exp(-2\epsilon^2N)$
		- $M$ -  hypothesis的个数
 
- 每个hypothesis下的 **BAD events** $B_m: \enspace |E_{in}(h_m) - E_{out}(h_m)| > \epsilon$
- To give $\cal A$ freedom of choice: bound $\mathbb{P}[\mathcal{B}_1 \enspace or \enspace \mathcal{B}_2 \enspace or \cdots \mathcal{B}_M]$
- Worst case  -  所有的 $B_m$ 级联的形式满足下列不等式：
	- $\mathbb{P}[\mathcal{B}_1 \enspace or \enspace \mathcal{B}_2 \enspace or \cdots \mathcal{B}_M] \leq(union \enspace bound) \enspace \mathbb{P}[\mathcal{B}_1]+ \mathbb{p}[\mathcal{B}_2]+  \cdots + \mathbb{P}[\mathcal{B}_M]$

#### Where Did Uniform Bound Fail
- 当 $M=∞$ 时，上面不等式右边值将会很大，似乎说明 BAD events 很大，$E_{in}(g) \enspace E_{out}(g)$ 也并不接近
- 但是BAD events $B_m$ 级联的形式实际上是扩大了上界，union bound 过大
- 这种做法假设各个 hypothesis 之间没有交集，这是最坏的情况，可是实际上往往不是如此，很多情况下，都是有交集的，也就是说 $M$ 实际上没那么大，如下图所示：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316141005.png)


- Union bound over-estimating 
- 所以，需要找出不同 BAD events 之间的重叠部分
	- 也就是将无数个 hypothesis 分成有限个类别。

#### How Many Lines Are There
- 先看这样一个例子  $\mathcal{H} = \{ all \enspace lines \enspace in \enspace \mathbb{R}^2\}$
- 假如平面上用直线将点分开，跟 PLA 一样
	- 如果平面上只有一个点 $x_1$，那么直线的种类有 2 种
		1. 将 $x_1$ 划为 +1
		2. 将 $x_1$ 划为 -1
	- 如果平面上有两个点 $x_1 \enspace x_2$，那么直线的种类共 4 种
		1. $x_1 \enspace x_2$都为 +1
		2. $x_1 \enspace x_2$都为 -1
		3. $x_1$ 为 +1 且 $ x_2$ 为 -1
		4. $x_1$ 为 -1 且  x_2$ 为 +1：
	- 如果平面上有三个点 $x_1 \enspace x_2 \enspace x_3$，那么直线的种类共 8 种
	- 但是，在三个点的情况下，也会出现不能用一条直线划分的情况：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316141829.png)

- 也就是说，对于平面上三个点，不能保证所有的8个类别都能被一条直线划分
	- 四个点 $x_1 \enspace x_2 \enspace x_3 \enspace x_4$，平面上找不到一条直线能将四个点组成的 16 个类别完全分开，最多只能分开其中的 14 类，即直线最多只有 14 种：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316142043.png)

#### Effective Number of Lines
- 经过分析，得到平面上线的种类是有限的
	- 1 个点最多有 2 种
	- 2 个点最多有 4 种
	- 3 个点最多有 8 种
	- 4 个点最多有 14（<24）种...
	- 有效直线的数量  ≤ $2^N$
		- $N$ 是点的个数
- 以上说明 effective 是有限的
- 所以，用 effective(N) 代替 M ， Hoeffding's inequality可以写成：
	- $P[|E_{in}(h) - E_{out}(h)| > \epsilon] \leq 2\cdot effective(N) \cdot\exp(-2\epsilon^2N)$
	- 如果能够保证 $effective(N) \enspace can \enspace replace \enspace M \enspace and \enspace \ll 2^N$，那么即使 $M$ 无限大，直线的种类也很有限，机器学习也是可能的。

### Effective Number of Hypotheses

#### Dichotomies: Mini-hypotheses
- 二分类 dichotomy
	- dichotomy  -  就是将空间中的点用一条直线分成正类（o）和负类（x）
- 令 $\cal H$ 是将平面上的点用直线分开的所有 hypothesis $h$ 的集合
- dichotomy  $\cal H(x_1, x_2, \cdots, x_N)$  与 hypotheses  $\cal H$ 的关系是：
	- hypotheses  $\cal H$ 是平面上所有直线的集合，个数可能是无限个
	- 而 dichotomy  $\cal H$ 是平面上能将点完全用直线分开的直线种类，它的上界是$2^N$
- 接下来，尝试用 dichotomy 代替 $M$。

#### Growth Function
- 成长函数 growth function ，记为 $m_\mathcal{H}(N)$
	- 定义是：对于由 $N$ 个点组成的不同集合中，某集合对应的 dichotomy 最大，那么这个dichotomy值就是 $m_\mathcal{H}(N)$，它的上界是 $2^N$：
	- $m_\mathcal{H}(N) = \mathop{max}\limits_{x_1,x_2,\cdots,x_N \in \mathcal{X}}  |\mathcal{H}(x_1, x_2, \cdots, x_N)|$
- 成长函数其实就是之前讲的 effective lines 的数量最大值
- 根据成长函数的定义，二维平面上，$m_\mathcal{H}(N)$ 随 $N$ 的变化关系是：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316144050.png)

#### Growth Function for Positive Rays
- 先看一个简单情况，一维的 Positive Rays：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316150920.png)

- $\mathcal{X} = \mathbb{R}$ (1d)
- $\cal H$ contains $h$，where each $h(x) = sign(x - a)$ for threshold $a$

- 若有 $N$ 个点，则整个区域可分为 $N+1$ 段
- 成长函数 $m_\mathcal{H}(N) = N+1$
- 注意, 当 N 很大时，$(N+1) \ll 2^N$

- 另一种情况是一维的 Positive Intervals：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316151519.png)

- 它的成长函数可以由下面推导得出：
![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316151533.png)

- 这种情况下，$m_h(N) \ll 2^N$，在 $N$ 很大的时候，仍然是满足的。

#### Growth Function for Convex Sets

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316151930.png)

- $\mathcal{X} = \mathbb{R}^2$(2d)
- $\cal H$ contains $h$，where each $h(x) = +1$ iff x in  $a$ convex region, $-1$ otherwise

- 假设在二维空间里，如果 hypothesis 是凸多边形或类圆构成的封闭曲线，如图所示，左边是convex的，右边不是convex的

- 当数据集 $\cal D$ 按照如下的凸分布时，很容易计算得到它的成长函数 $m_h(N) = 2^N$
- 这种情况下，$N$ 个点所有可能的分类情况都能够被 hypotheses set 覆盖，这种情形称为 _shattered_
	- 也就是说，如果能够找到一个数据分布集，hypotheses set 对 $N$ 个输入所有的分类情况都做得到，那么它的成长函数就是 $2^N$。

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316152215.png)


### Break Point

#### The Four Growth Functions
- 目前介绍了四种不同的成长函数，分别是：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316152525.png)

- positive rays 和 positive intervals 的成长函数都是 polynomial
	- 如果用 $m_H(N)$ 代替 $M$ 的话，good
- convex sets 的成长函数是 exponential 的，即 = M，bad
- 2D perceptrons

#### Break Point of $\cal H$
- 对于2D perceptrons
	- 之前分析了3个点，可以做出 8 种所有的 dichotomy
	- 而4个点，无法做出所有16 个点的 dichotomy
- 所以，就把 4 称为 2D perceptrons 的 **break point**（5、6、7等都是break point）
- if no $k$ inputs can be shattered by $\cal H$, call $k$ a break point for $\cal H$
	- $m_{\mathcal{H}}(k) < 2^k$
	- $k+1$, $k+2$, $k+3$, ... also break points
	- will study minimus break point $k$

#### The Four Break Points
- 根据 break point  定义，满足 $m_{\mathcal H}(k)≠2^k$ 的 $k$ 的最小值
- 对于我们之前介绍的四种成长函数，break point分别是：
- 
![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316153617.png)

- Conjecture
	- no break point :  $m_{\mathcal{H}}(N) = 2^N$
	- break point $k$ :   $m_{\mathcal{H}}(N) = O(N^{k-1})$


## 总结
- 本章深入探讨了机器学习的可行性
- 介绍了把机器学习拆分为两个核心问题：Ein(g)≈Eout(g)和Ein(g)≈0，关键就是对于M的取舍
- 介绍了 effective number 及其上限，如果能保证 effective number$(N)$ ，机器学习是可行的
- 介绍了 $M$ 个 hypothesis 到底可以划分为多少种，也就是成长函数 $m_{\mathcal{H}}(N)$
- 介绍了break point 的概念，break point 的计算方法，以及如果没有 break point 的复杂度
