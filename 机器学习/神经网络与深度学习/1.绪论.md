## 第1章 绪论

- 深度学习 是机器学习的一个分支，是指一类问题以及解决这类问题的方法
- 深度学习的模型很复杂，样本的原始输入到输出目标之间的数据流经过多个线性或非线性的 *组件component*。在得到输出结果时，并不清楚其中每个组件的贡献是多少，这个问题叫作 **贡献度分配问题Credit Assignment Problem CAP**。
	- 人工神经网络 Artifical Neural Network ANN，比较好解决CAP问题
- 神经网络 $\neq$ 深度学习。深度学习可以采用神经网络模型，也可以采用其它模型。

### 1.1 人工智能 Artificial Intelligence
- 1950年，Alan Turing 发表了 《Computing Machinery and Intellifence》
- 1956年，Dartmouth会议，John McCarthy提出 “人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样”
- 人工智能的主要领域
	- 感知，语音信息处理和计算机视觉
	- 学习，监督学习、无监督学习和强化学习
	- 认知，知识表示、自然语言理解、推理、规划、决策

#### 人工智能的发展历史
- 推理期
	- 图灵机
	- 达特茅斯会议
	- Rosenblatt 感知器
- 知识期
	- 专家系统
	- Prolog
- 学习期
	- Machine Learning，统计机器学习
	- 神经网络
	- 深度学习

#### 人工智能的流派
- 符号主义 Symbolism，又称逻辑主义、心理学派、计算机学派
	- 指通过分析人类智能的功能，然后用计算机来实现这些功能的一类方法
	- 两个基本假设
		- 信息可以用符号来表示
		- 符号可以通过显示的规则来操作
- 连接主义 Connectionism，又称仿生学派、生理学派
	- 是认知科学领域中的一类信息处理的方法和理论
	- 认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程

### 1.2 机器学习 Machine Learning
- 浅层学习Shallow Learning  -  传统机器学习主要关注如何学习一个预测模型，首先将数据表示为一组 **特征Feature**，然后将特征输入到预测模型，输出预测结果。
	- 不涉及特征学习，特征主要靠人工经验或特征转换方法
- ![[Pasted image 20230307094745.png]]

### 1.3 表示学习
- 为了提高机器学习系统的准确率，将输入信息转换为有效的特征，或者更一般性地称为 **表示representation**
- 表示学习representation learning  -  一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能
- 语义鸿沟semantic gap 问题  -  是指输入数据的底层特征和高层语义信息之间的不一致性和差异性

#### 局部表示和分布式表示
- *好的表示* 应该具备:
	- 很强的表示能力，即同样大小的向量可以表示更多信息
	- 使后续的学习任务变得简单，即需要包含更高层的语义信息
	- 具有一般性，是任务或领域独立的

| 局部表示 Local Representation                                                                                              | 分布式表示 Distributed Representation    |
| -------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------- |
| 也称离散表示、符号表示                                                                                                     | 也称分散式表示                           |
| one-hot向量                                                                                                                | 表示为低维、稠密的向量                   |
| 具有很好的解释性，有利于高效的特征工程。通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高 | 表示能力更强，分布式表示的向量维度比较低 |
| one-hot向量维数很高。不同向量之间相似度都为0                                                                               | 嵌入复杂                                 |
| 比如 颜色: 红色 $[0, 0, 1, 0]^T$                                                                                           | 红色表示成RGB空间中的一个点， $[0.67, 0.22, 0.12]^T$                                         |

- 嵌入 Embedding  -  一个过程，就是使用神经网络将高位的局部表示空间 $\mathbb{R}^{|V|}$ 映射到一个非常地位的分布式表示空间 $\mathbb{R}^D, D \ll |V|$。在这个低维空间中，每个特征不再是坐标轴上的店，而是分散在整个低维空间中。
	- NLP中，叫作 词嵌入

- 要学习到一种好的高层语义表示，通常需要从底层特征开始，经过多步非线性转换才能得到。
- 表示学习的关键是构建具有一定深度的多层次特征表示

### 1.4 深度学习
- 深度 是指原始数据进行非线性特征转换的次数
- ![[Pasted image 20230307105956.png]]
- 深度学习某种意义上可以看作一种 强化学习reinforcement learning
- 深度学习主要是神经网络模型，主要原因是可以使用误差反向传播算法，从而解决贡献度分配问题

#### 端到端学习
- 传统机器学习方法，对于复杂任务，需要将一个任务的输入和输出之间人为地切割成很多子模块(或多个阶段)
	- 有两个问题: 每一个模块都需要单独优化，并且其优化目标和任务总体目标并不一致
	- 错误传播，前一步的错误会对后续的模型造成很大的影响
- 端到端学习 End-to-End Learning，也称端到端训练，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标
- 也需要解决贡献度分配问题

### 1.5 神经网络
- 在机器学习领域，神经网络是指由很多人工神经元构成的网络结构模型，这些人工神经元之间的连接强度是可学习的参数

#### 人脑神经网络
- 大脑  -  人体中最复杂的器官，由神经元、神经胶质细胞、神经干细胞和血管组成
- 神经元neuron，也叫 神经细胞nerve cell，是携带和传输信息的细胞，是人脑神经系统中最基本的单元。
- 人脑神经系统是一个非常复杂的组织，包含近 860亿个神经元，每个神经元有上千个突触和其他神经元相连接。
- 典型的神经元结构大致分为 细胞体 和 细胞突起
	- 细胞体soma 中的神经细胞膜上有各种受体和离子通道，胞膜的受体可与相应的化学物质神经递质结合，引起离子通透性及膜内外电位差发生改变，产生相应的生理活动: 兴奋或抑制
	- 细胞突起，是由细胞体延伸出来的细长部分，分为:
		- 树突dendrite，可以接收刺激并将兴奋传入细胞体。每个神经元可以有一或多个树突
		- 轴突axon，可以把自身的兴奋状态从胞体传送到另一个神经元或其他组织。每个神经元只有一个轴突
- 神经元可以接收/发送信息给其他神经元
- 神经元之间没有物理连接，两个“连接”的神经元之间有 20纳米左右 的缝隙，并靠 突触synapse 进行互联来传递信息，形成一个神经网络，即神经系统
- 突触，可以理解为连接“接口”
- 一个神经元可以被视为一种只有两种状态的细胞：兴奋和抑制。神经元的状态取决于从其他的神经细胞收到的输入信号量，以及突触的强度。
- 当信号量总和 > 某阈值，细胞体就会兴奋，产生电脉冲。电脉冲沿着轴突并通过突触传递到其他神经元
- 赫布理论 Hebbian Theory，又称 赫布规则 Hebbian Rule，Hebb's Rule  -  1949年，加拿大心理学家 Donald Hebb 《行为的组织 The Organization of Behavior》提出 突触可塑性 的基本原理
	- 当神经元A的一个轴突和神经元B很近，足以对它产生影响，并且持续地、重复地参与了对神经元B的兴奋，那么在这两个神经元或其中之一会发生某种生长过程或新陈代谢变化，以致神经元A作为能使神经元B兴奋的细胞之一，它的效能加强了。
	- Hebb 认为人脑有两种记忆，长期记忆和短期记忆

#### 人工神经网络
- 人工神经元网络是由大量神经元通过极其丰富和完善的连接而构成的自适应非线性动态系统
- 网络容量netowrk capacity  -  一个人工神经网络塑造复杂函数的能力

#### 神经网络发展史
- 第一阶段 模型提出 1943~1969
	- 1943  Warren McCulloch 和 Walter Pitts  MP模型
	- 1948  Alan Turing  B型图灵机
	- 1951  McCulloch 和 Marvin Minsky  第一台神经网络机 SNARC
	- 1958  Rosenblatt  提出 感知器 Perceptron
- 第二阶段 冰河期 1969~1983
	- 1974  Paul Werbos  反向传播算法 BackPropagation BP
- 第三阶段 反向传播算法引起的复兴 1983~1995
	- 1983  John Hopfield  提出 Hopfield网络
	- 1984  Geoffrey Hinton  提出 Boltzmann Machine
- 第四阶段 流行度降低 1995~2006
- 第五阶段 深度学习的崛起 2006~至今

### 1.6 本书知识体系
![[Pasted image 20230307153840.png]]

### 总结
- 本章介绍了各种概念，以及机器学习和深度学习
- 介绍了贡献度分配问题
- 介绍了深度学习主要以神经网络模型为基础

### 扩展阅读
- Deep Learning  -  Goodfellow 2016, Bengio 2009
- Stanford CS231N CS224N
- 国际表示学习会议 ICLR
- 神经信息处理系统年会 NeurIPS
- 国际机器学习会议 ICML
- 国际人工智能联合会议 IJCAI
- 美国人工智能协会年会 AAAI
