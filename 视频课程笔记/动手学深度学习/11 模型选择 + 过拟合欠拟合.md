* [前言](#前言)
* [模型选择](#模型选择)
  * [泛化的模型](#泛化的模型)
  * [训练误差和泛化误差](#训练误差和泛化误差)
  * [验证数据集和测试数据集](#验证数据集和测试数据集)
  * [K-则交叉验证](#k-则交叉验证)
  * [小结](#小结)
* [过拟合和欠拟合](#过拟合和欠拟合)
  * [什么是过拟合和欠拟合？](#什么是过拟合和欠拟合)
  * [模型容量](#模型容量)
  * [模型容量的影响](#模型容量的影响)
    * [估计模型容量](#估计模型容量)
  * [VC 维](#vc-维)
    * [线性分类器的 VC 维](#线性分类器的-vc-维)
    * [VC维的用处](#vc维的用处)
  * [数据复杂度](#数据复杂度)
* [多项式回归](#多项式回归)
* [总结](#总结)
* [模型选择 + 过拟合欠拟合 Q&A](#模型选择--过拟合欠拟合-qa)


------

# 前言

>  作为机器学习科学家，我们的目标是发现 _模式 pattern_
>  但是，我们如何才能确定模型是真正发现了一种泛化的模式， 而不是简单地记住了数据呢？

------

#  模型选择

- 在机器学习中，我们通常在评估几个候选模型后选择最终的模型，这个过程叫做 _模型选择_。 
	-  有时，需要进行比较的模型在本质上是完全不同的（比如，决策树与线性模型）
	-  有时，需要比较不同的超参数设置下的同一类模型

-  例如，训练多层感知机模型时，我们可能希望比较具有 不同数量的隐藏层、不同数量的隐藏单元以及不同的激活函数组合的模型。 为了确定候选模型中的最佳模型，我们通常会使用验证集。

-  举例：预测谁会偿还贷款

-  银行雇你来调查谁会偿还贷款
	-  你得到了100个申请人的信息，其中五个人在3年内违约了
	-  然后你看这100人的申请信息，所有的五个人在面试时都穿了蓝色衬衫，在美国，暗示蓝领
	-  你的模型也发现了这个强信号
	-  这会有什么问题？
		-  是不是穿红衣服，模型就不认识了

## 泛化的模型

- 所以更正式地说，机器学习的目标是发现某些模式，
	- 这些模式捕捉到了我们的训练集 *潜在总体的规律*。 
	- 如果成功做到了这点，即使是对以前从未遇到过的个体， 模型也可以成功地评估风险

- **如何发现可以泛化的模式是机器学习的根本问题**

## 训练误差和泛化误差

- **训练误差 training error**：模型在训练数据集上计算得到的误差

- **泛化误差 generalization error**：模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望
	- 我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象
	- 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成

- 举例：根据模考成绩来预测未来考试分数
	- 在过去的考试中表现很好（*训练误差*）不代表未来会好（*泛化误差*）
	- 学生A通过背书在模考中拿到很好成绩
	- 学生B知道答案后面的原因

- **其中，泛化误差是我们所最关心的**

## 验证数据集和测试数据集

- **验证数据集 validation dataset**， 也叫 **验证集 validation set**：一个用来评估模型好坏的数据集，即评估模型的超参数
	- 例如拿出50%的训练数据
	- 一定不要跟训练数据混在一起（常犯错误）
	- 验证数据集的精度也有可能是偏高的

- **测试数据集 test dataset**：只用一次的数据集。例如：
	- 未来的考试
	- 我出价的房子的实际成交价
	- 用在kaggle私有排行榜中的数据集

- 二者最大的区别就是，**验证数据集可以拿来用很多次，相当于高三的模考，而测试数据集则只能用一次来评估模型的性能，相当于高考**

- 原则上，在我们确定所有的超参数之前，我们不希望用到测试集。
- 如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险，那就麻烦大了。
- 如果我们过拟合了训练数据，还可以在测试数据上的评估来判断过拟合。 但是如果我们过拟合了测试数据，我们又该怎么知道呢？

- 因此，我们决不能依靠测试数据进行模型选择。 然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。

- 虽然理想情况下我们只会使用测试数据一次， 以评估最好的模型或比较一些模型效果，但现实是测试数据很少在使用一次后被丢弃。
- 我们很少能有充足的数据来对每一轮实验采用全新测试集。

## K-则交叉验证

- 在没有足够多数据时使用（这是常态）

- 算法：
	- 将训练数据分割 k 块 （K 个不重叠的子集）
	- For i = 1，……，k （执行 K 次模型训练和验证）
		- 使用第 i 块作为验证数据集，其余的作为训练数据集
	- 报告 k 个验证集误差的平均

- 常用：k = 5 或 10

- K-则交叉验证的目的是在没有足够多数据使用时评估模型和超参数的性能，也就是说，**K次训练和验证使用的是相同的超参数和模型**

## 小结

- 训练数据集：训练模型参数
- 验证数据集：选择模型超参数
- 非大数据集上通常使用 k-折 交叉验证

# 过拟合和欠拟合

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324101740.png)

## 什么是过拟合和欠拟合？

- 当我们比较训练和验证误差时，我们要注意两种常见的情况
	- 训练误差和验证误差都很严重， 但它们之间仅有一点差距。 如果模型不能降低训练误差，这可能意味着模型过于简单（即表达能力不足）， 无法捕获试图学习的模式，由于我们的训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为 *欠拟合*
	- 当我们的训练误差明显低于验证误差时要小心， 这表明严重的 *过拟合*
| 模型容量 \ 数据 | 简单 | 复杂 |
| :-------------: | :------: | :----: |
| 低 | 正常 | 欠拟合 underfitting |
| 高 | 过拟合 overfitting | 正常 |

- **模型容量**  即模型的复杂度，也代表了模型拟合各种函数的能力

-  注意，_过拟合_ 并不总是一件坏事。 特别是在深度学习领域
-  众所周知， 最好的预测模型在训练数据上的表现往往比在保留（验证）数据上好得多。
-  最终，我们通常更关心验证误差，而不是训练误差和验证误差之间的差距。

## 模型容量

-  拟合各种函数的能力
-  低容量的模型难以拟合训练数据
-  高容量的模型可以记住所有的训练数据

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324105048.png)

- 显然，模型容量太低或太高都不好。
	- 第一种太低，一次线性模型过于简单，表达能力不足，模型分类效果差
	- 第二种则过于复杂，把噪声全部都拟合住了，这是我们所不希望的。

## 模型容量的影响

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324105323.png)

- 模型容量比较低的时候，训练误差比较高，这是因为模型过于简单。
- 随着模型容量的升高，训练误差是会慢慢下降的，甚至可以到 0，但是数据里有着大量噪音
- 而泛化误差会慢慢下降到一个低点，再回升，这是因为模型太过于关注细枝末节
- 我们的核心任务就是把泛化误差往下降，在最优点，尽量把泛化误差和训练误差的gap变得小

###  估计模型容量

- 难以在不同的种类算法之间比较
	- 例如 树模型和神经网络

- 给定一个模型种类，将有两个主要因素
	- 参数的个数
	- 参数的选择范围

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324110134.png)

- 线性模型 有 $d+1$ 个参数
- 单隐藏层 有 $(d+1)m + (m+1)k$ 个参数


## VC 维

- VC 维是统计学习理论的一个核心思想
- 这里大致了解就行，因为很难计算之后学习的模型（如CNN,RNN)的 VC 维，故并不经常用

- 定义：对于一个分类模型，VC 维等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型对它进行完美分类。
	- 即存在 $\cal H$ 个样本，模型能把 $\cal H$ 个样本的 $2^H$ 种标号方式打散的 $\cal H$ 的最大值。

### 线性分类器的 VC 维

- 2 维输入的感知机，VC 维 = 3
	- 对于 3 个点的任意标号都能分类，而对于任意 4 个点的样本都存在不能被打散的标号，就像之前讲过的 异或XOR

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324110640.png)

- 支持 N 维输入的感知机的 VC 维是 $N+1$
- 一些多层感知机的VC维是 $O(N \log_2N)$

### VC维的用处

- 提供为什么一个模型好的理论依据
	- 它可以衡量训练误差和泛化误差之间的间隔

- 但深度学习中很少使用，即统计学习理论在深度学习中就很少使用
	- 还不能把 VC维理论应用在深度学习上，衡量不是很准确
	- 计算深度学习模型的 VC 维很困难
		- 感知机还可以，但是对于一些常用模型是算不出来的

## 数据复杂度

-  多个重要因素
	-  样本个数
	-  每个样本的元素个数
	-  是不是有复杂的时间、空间结构
	-  数据的多样性

-  训练数据集中的样本越少，我们就越有可能（且更严重地）过拟合。随着训练数据量的增加，泛化误差通常会减小
-  对于固定的任务和数据分布，模型复杂性和数据集大小之间通常存在关系
-  对于许多任务，深度学习只有在有数千个训练样本时才优于线性模型
-  从一定程度上来说，深度学习目前的生机要归功于 廉价存储、互联设备以及数字化经济带来的海量数据集

------

# 多项式回归 

[11 模型选择、欠拟合和过拟合 - 多项式回归 .ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11%20%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E3%80%81%E6%AC%A0%E6%8B%9F%E5%90%88%E5%92%8C%E8%BF%87%E6%8B%9F%E5%90%88%20-%20%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92%20.ipynb)

------

#  总结

- 本节介绍了模型选择、训练误差和泛化误差、验证数据集和过拟合、欠拟合。
- 介绍了模型选择的概念，训练误差和泛化误差，验证数据集和测试数据集。在实际中我们更看重验证误差。对于小样本数据，采用k折交叉验证
- 介绍了过拟合和欠拟合，通常出现于我们比较训练和验证误差时，这两种情况都告诉我们有些参数是好的，有些是不好的。在深度学习领域，过拟合并不是件坏事
- 介绍了机器学习的理论根据 VC 维，它提供了一个模型好的理论依据
- 介绍了模型容量和数据复杂度的概念，这两个是影响我们训练的重要因素

 ------

# 模型选择 + 过拟合欠拟合 Q&A

-  `我感觉svm从理论上讲应该对于分类效果不错，和神经网络相比，缺点在哪里？`
>  svm是通过kernel来匹配模型复杂度，算起来不简单。当数据不大时，可以，一旦数据很大，svm很难计算。svm能够调的超参数不仅不多，效果也一般，可调性一般。
>  神经网络主要优点是它是一门语言

- `老师，除了权重衰减，dropout，还可以介绍一些其他技巧吗？比如BN、模型剪枝、蒸馏等等`
>  模型剪枝、蒸馏其实是让你得到一个适用于部署的小模型

- `老师，如果是时间序列上的数据，训练集和验证集可能会有自相关，这时候应该怎么处理呢？`
>  时序数据的话，要保证验证集在训练集之后

- `验证数据集和训练数据集的数据清洗（如异常值处理）和特征构建（如标准化）需不需要放在一起处理？`
>  一种是放在一起处理，在工业上没什么问题；另一种是分开做，这种比较保险一点

- `老师，深度学习一般训练集合比较大，所以K折交叉验证在深度学习中是不是没什么应用？训练成本太高了吧`
>  是这样的。k折主要用来解决数据不够多的情况

- `为什么cross validation就好呢，他其实也并没有解决数据来源的问题`
>  cross validation 只是用来解决超参数

- `k折交叉验证中的 k 怎么确定？有什么方法吗`
>  k越大效果越好，但是最优的是控制在训练成本下

- `cross validation每块训练时获得的最终模型参数可能是不同的，这时候要怎么说明道理？应该选哪个模型`
>  最后报告的是平均参数，遵循大数定理

- `所以是出现了overfitting后者underfitting才需要hyperparameter training吗`
>  调参指的是调整参数使得泛化误差比较好。overfitting后者underfitting会告诉你什么是好的、什么是不好的

- `老师，如何有效设计超参数，是不是智能搜索？最好用的搜索时贝叶斯方法还是网格、随机？老师有推荐吗？`
>  AutoML中hyperparameter tuning HPO。两件事情，一是怎么设计超参数，二是给你上千组合，会有很多可能，网格就是把这些组合都遍历一次，随机就是随便挑选一个。个人推荐，超参数的设计靠专家经验。最好用的搜索是自己动手调，或是随机，每次随机选取组合看情况，重复一百次以上，选取最好的。

- `k折交叉验证的目的是确定超参数吗？然后还要用这个超参数再训练一遍全数据吗？`
>   三种做法，一是就是确定超参数，再在全数据上重新训练；另一种，不训练了，就在k折中找出京都最好的；第三种，把k个模型的超参数都拿出来，然后预测时挨个尝试一遍。

- `老师，所有的验证集上的loss曲线都是这种先下降后上升的吗？为什么网上大部分的图都是一直下降的？`
>  在课程里我们介绍的模型容量的X轴表示的是不同的模型，从简单到复杂，并不是一个模型迭代训练。
