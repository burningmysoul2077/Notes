## 4. 图嵌入表示学习
- 如何把节点映射成D-维向量
	- <font color=Blue>人工特征工程:</font> 节点重要度、集群系数、Graphlet
	- <font color=#008000>图表示学习</font>: 通过*随机游走*构造*自监督学习任务*。DeepWalk、Node2Vec
	- <font color=#008000>矩阵分解</font>
	- <font color=red>深度学习</font>：图神经网络

## 图嵌入-概述

### Graph Representation Learning
- GRL alleviates the need to do feature engineering every single time
- RL could automatically learn the features (multimodal -> vector)
- 将节点映射为D-维向量，也被称为Distributed Representation，D-维向量具有：
	- 低维，向量维度远小于节点数
	- 连续，每个元素都是实数
	- 稠密，每个元素都不为0
- 图的表示学习的目的就是获得独立于不同任务的高效特征，通俗说就是能够针对不同任务学习得到适合任务的嵌入表示

#### Why Embedding
- Map nodes into an embedding space 嵌入D-维空间
- Similarity of embeddings between nodes indicates their similarity in the network.向量相似度反映节点相似度
- Encode network information
- Potentially used for many downstream predictions/tasks

## 图嵌入-基本框架：
## 编码器-解码器
- 如何学习节点的嵌入向量

### Setup
- $G$ : assume a graph
	- $V$ : the vertex set
	- $A$ : the adjacency matrix(assume binary, unweighted)
	- For simplicity: No node features or extra info
- Goal: 能够学习到节点的嵌入向量，这种节点嵌入向量的相似性能够近似节点在图中的相似性

### Embedding Nodes
- **Encodes**: maps form nodes to embeddings, 输入一个节点，输出节点的向量
- **Decoder**: maps from embeddings to the similarity score, *节点的相似度* 由 *向量 $\cdot$ 数值(余弦相似度)* 反映, dot product between node embeddings
	- 原始网络的相似性$similarity(u, v)$ (需人为定义) $\approx$ ${Z_v}^TZ_u$ node embedding的相似性
		- 两个向量完全不相似，正交 = 0
		- 两个向量相似、共现，余弦相似度 = 1
	- 目标：迭代优化每个节点的D-维向量，使得图中**相似节点**向量数量积**大**，**不相似节点**向量数量积**小**
- 两个关键: 编码器和相似性函数

### How to Define Node Similarity
- If two nodes had a similar embedding, so they should be
	- linked / sharing neighbors / having similar structural roles?
- RWs can define and optimize node embedding!

### "Shallow" Encoding
- Simplest encoding approach: Encoder is just an **embedding-lookup**
- $ENC(v) = z_v = Z \cdot v$
	- 构造 d 行 节点个数 列的 $Z$ 矩阵，each column is a node embedding
	- dot product $v$，$v$ is indicator vector 节点个数 行 1 列, all zeroes except a 1 in column indicationg node $v$
- 目标就是优化 $Z$ 矩阵，每一列表示一个节点，行数是向量的维度
	- 直接优化: DeepWalk、Node2Vec

### Summary
- Shallow encoder: embedding lookup
- Deep encoders: GNNs

### Note on Node Embeddings
- This is **unsupervised/self-supervised**
	- We r **not** utilizing node labels
	- We r **not** utilizing node features
	- Directly estimate a set of coordinates(i.e. the embedding) of a node s.t. some aspect of the network structure is preserved
- These embddings r **task independent**


## 图嵌入:
## 基于随机游走的方法

### Random Walk
- The(random) sequence of points visited this way is a **random walk on the graph**

|   图机器学习   |      NLP       |
|:--------------:|:--------------:|
|       图       |      文章      |
|  随机游走序列  |      句子      |
|      节点      |      单词      |
|    DeepWalk    |   Skip-Gram    |
| Node Embedding | Word Embedding |

#### Notation
- Vector $z_u$ : the embedding of node $u$ , aka the GOAL
- Probability $P(v|z_u)$ : the (predicted) probability of visiting node $v$ on RWs starting from node $u$
- Non-linear functions used to produce predicted probabilities
	- Softmax : $\sigma(z)[i] = \frac{e^{z[i]}}{\sum_{j = 1}^K e^{z[j]}}$ , turns vector of $K$ real values into $K$ probabilities that sum to 1，将K个实值组成的向量变成一个和为1的由K个概率组成的概率向量
	- Sigmoid: $S(x) = \frac{1}{1 + e^{-x}}$ , S-shaped function that turns real values into the range of (0, 1)，S形状的函数，能够将实值映射成(0, 1)区间的值

#### Random-Walk Embeddings
- 给定一个图 $G$ 和一个开始节点 $v$ ，我们随机挑选这个节点的邻居节点，然后移到这个邻居节点，以这个邻居节点作为开始点重复这个过程。到达一定次数之后这个过程结束。在整个过程中访问的节点序列就是图上的随机游走
- ${z_u}^Tz_v \approx$ probability that $u$ and $v$ co-occur on a RW over the graph
- 节点相似的定义，共同出现在同一个随机游走序列
- 执行步骤: 
	1. $P_R(v|u)$:  Estimate probability of visiting node $v$ on a RW starting from node $u$ using some RW strategy $R$
	2. 迭代优化每个节点的D-维向量，使得序列中共现节点向量数量积大，不共现节点向量数量积小
		- Similarity in embedding space, i.e. dot product = $cos(\theta)$ encodes RW "similarity"，用embedding space中的相似性来编码节点经过随机游走得出来的“相似性”
		- 在embedding space里，假设向量的模长均为单位模长，只关注$cos(\theta)$
- Why Radnom Walks
	- 表达性强Expressvity: incorporates both local and higher-order neighborhood info(high-order multi-hop info)
	- 效率高Efficiency: consider the node pairs that co-occur on RWs instead of all node pairs

#### 无监督/自监督学习问题
- Given a node $u$, $N_R(u)$...neighborhood of $u$ obtained by some RW strategy $R$
- Optimization
	- Given $G = (V, E)$
	- Goal: $f(u) = z_u$, 期望学到一种映射，使网络中临近的节点靠的最近
	- Log-likelihood objective: $\mathop{\max}\limits_f \sum\limits_{u\in V}logP(N_R(u)|z_u)$, maximum likelihood objective

####  Random Walk Optimization
- Steps:
	- Run short fixed-length RWs starting from each node $u$ in the graph using some RW strategy $R$
	- For each $u$ collect $N_R(u)$， 可以有重复地节点
	- Optimize embeddings according to above log-likelihood
		- 极大似然估计
		- Equivalently, $\mathcal{L}=\sum\limits_{u \in V} \sum\limits_{v \in N_R(u)} - \log(P(v|z_u))$
			- 直观上，遍历所有节点，分别以每一个节点作为随机游走的起点，再遍历从 $u$ 节点出发的随机游走序列所有邻域节点，加上负号变成最小化，整个式子变成了损失函数
			- 为了求解，Parameterize using softmax:  $p(u|z_u)=\frac{\exp({z_u}^Tz_v)}{\sum_{n\in V}\exp({z_u}Tz_n)}$
				- Why softmax? We want node $v$ to be most similar to node $u$, 节点 $u$ 和节点 $v$ 在该随机游走序列中共现
			- Put it all together: $\mathcal{L}=\sum\limits_{u \in V} \sum\limits_{v \in N_R(u)} - \log(\frac{\exp({z_u}^Tz_v)}{\sum_{n\in V}\exp({z_u}Tz_n)})$
			- Optimizing RW embeddings = Finding embeddings $z_u$ that minimize $\mathcal{L}$
			- 但是实践中，式子的第一项需要遍历所有节点，第三项需要遍历所有节点对，需要 $O({|V|}^2)$ 复杂度
			- Solution: Negative sampling
	- Negative Sampling，不归一化所有节点，只针对 $k$ 个
		- $\log(\frac{\exp({z_u}^Tz_v)}{\sum_{n\in V}\exp({z_u}Tz_n)}) \approx \log(\theta({z_u}^Tz_v)) - \sum_{i = 1}^k \log(\theta({z_u}^Tz_{n_i})), n_i \sim P_V$
			- Sigmoid function，k 个负样本，非均均分布随机采样(类似word2vec)
			- Just normalize against $k$ random "negative samples" $n_i$
		- 如何选择负样本，Sample $k$ negative nodes each with prob. probportional to its degree，与它的度成正比
			- Higher $k$ gives more robust estimates
			- Higher $k$ corresponds to higher bias on negative events
			- In practice $k$ = 5-20
			- 理论上，同一个随机游走序列中的节点，不应该被采样为“负样本”

#### 随机梯度下降
- 得到了目标函数，如何对其进行优化(最小化)，Stochastic Gradient Descent, the way of optimizing(minimizing) the objective function
- 全局Gradient Descent: a simple way to minimize $\mathcal{L}$
	- 随机初始化 Initialize $z_u$ at some randomized value for all nodes $u$
	- Iterate until convergence收敛
		- 求所有节点 $u$ 总梯度 $\frac{\partial \mathcal{L}}{\partial z_u}$
		- 迭代更新，For all $u$, make a step in reverse direction of derivative: $z_u \leftarrow z_u - \eta\frac{\partial \mathcal{L}}{\partial z_u}$
			- $\eta$, learning rate
- 随机梯度下降，与全局计算所有样本不同，每个训练样本优化一次
	- 随机初始化 Initialize $z_u$ at some randomized value for all nodes $u$
	- Iterate until convergence: $\mathcal{L}^{(u)} = \sum\limits_{v \in N_R(u)} -\log(P(v|z_u))$
		- 每次随机游走优化一次，sample a node $u$, for all $v$ calculate the derivative $\frac{\partial \mathcal{L}^{(u)}}{\partial z_v}$
		- For all $v$, update $z_v \leftarrow z_v - \eta\frac{\partial \mathcal{L}^{(u)}}{\partial z_v}$

#### Random Walks: Summary
1. Run *short fixed-length RWs* starting from each node on the graph
2. For each node $u$ collect $N_R(u)$, the multiset of nodes visited on RWs starting from $u$
3. Optimize embeddings using Stochastic Gradient Descent: $\mathcal{L}=\sum\limits_{u \in V} \sum\limits_{v \in N_R(u)} - \log(P(v|z_u))$, through negative sampling
- 目前为止，我们已经描述了如何在给定随机游走策略 $R$ 的情况下优化 node embedding。下面的问题是，我们应该怎么做这个随机游走，即应该使用什么策略 $R$
	- 最简单的想法，从每个节点开始运行*固定长度，无偏*的随机漫步，但对于相似性存在局限性
	- 所以，引出 node2vec，一种更加随机的、更加丰富的、可以随机游走更多富有表现力的方法，以便更好地优化embedding

### DeepWalk 讨论
- 首个将深度学习和自然语言处理的思想用于图机器学习
- 在稀疏标注节点分类场景下，嵌入性能卓越
- 均匀随机游走，没有偏向的游走方向(node2vec)
- 需要大量随机游走序列训练
- 基于随机游走，管中窥豹。距离较远的两个节点无法相互影响。看不到全图信息(图神经网络)
- 无监督，仅编码图的连接信息，没有利用节点的属性特征
- 没有真正用到神经网络和深度学习

### Node2Vec

#### Overview of node2vec
- Goal: Embed nodes with similar network neighborhoods close in the feature space
	- Frame this goal as a maximum likelihhod optimization problem, and task independent
- Key observationl 一个灵活的节点 $u$ 的邻域 $N_R(u)$
- Develop **biased $2^{nd}$ order RW $R$ 有偏二阶随机游走**to generate network neighborhood $N_R(u)$ of node $u$
	- DeepWlak和node2vec对于邻域节点集的定义和如何定义随机游走有着明显差异
- Idea: using **biased $2^{nd}$ order RW**  can **trade off** between *local* and *global* views of the network
	- Always remember where the walk came from, 二阶主要在于记着来时的节点 
	- Two classic strategies to define $N_R(u)$
		- BFS, Local microscopic view，得到的邻域节点将会尽可能的集中在某个节点附近
		- DFS, Global macroscopic view，得到的邻域节点将会尽可能的延伸到全图
	- Interpolating BFS and DFS，bias 定步长随机游走策略 $R$ 去生成 $N_R(u)$
		- 2 parameters
			- $p$ - return parameter, probability pf return back to the previous node
			- $q$ - in-out parameter, moving outwards(DFS) vs. inwards(BFS), 直觉上 is the ratio of BFS vs. DFS
		- $p, q$ model transition probabilities，直觉上邻域节点是三种情况，均是非标准化概率
			- $\frac{1}{p}$ 可能性回到原来节点
			- $\frac{1}{q}$ 可能性走向更远的节点
			- $1$ 可能性走向和原节点等距的节点
		- BFS-like walk: Low value of $p$
		- DFS-like walk: Low value of $q$
- Core idea: Embed nodes s.t. distances in embedding space reflect node similarities in the original network

#### Node2Vec Algorithm
1. Compute RW probabilities
2. Simulate $r$ RWs of length $l$ starting from each node $u$
3. Optimize the node2vec objective using Stochastic Gradient Descent
- Linear-time complexity
- All steps r **individually parallelizable**

### Other RW Ideas
- Dofferent kinds of biased RWs:
	- Based on node attributes
	- Based on learned weights
- Alternative optimization schemes:
	- Directly optimize based on 1-hop and 2-hop RW probabilities, LINE
- Network preprocessing techniques:
	- Run RWs on modified versions of the original network

### Different notions of node similarity, so far
- Naive: similar if 2 nodes r connected
- Neighborhood overlap
- RW approaches
- 没有一种方法能在所有情况下获胜，一般情况下，选择与实际应用的问题匹配的节点相似度的定义

### Node2Vec 图嵌入算法
- Node2vec解决图嵌入问题，将图中的每个节点映射为一个向量(嵌入)
- 向量(嵌入)包含了节点的语义信息(相邻社群和功能角色)
- 语义相似的节点，向量(嵌入)的距离也近、
- 向量(嵌入)用于后续的分类、聚类、Link Prediction、推荐等任务
- 在DeepWalk完全随机游走的基础上，Node2Vec增加 $p, q$ 参数，实现有偏随机游走。不同的 $p, q$ 组合，对应了不同的探索范围和节点语义
- DFS深度优先探索，相邻的节点，向量(嵌入)距离相近
- BFS广度优先探索，相同功能角色的节点，向量(嵌入)距离相近
- DeepWalk是Node2Vec在 $p = 1, q = 1$ 的特例

## 矩阵分解角度
## 图嵌入和随机游走

### Embeddings & Matrix Factorization
- Objective: maximize ${z_v}^Tz_u$ for node pairs($u$, $v$) that r similar， ${z_v}^Tz_u = A_(u, v)$ , $(u, v)$ of the graph adjacency matrix $A$
- Therefore, $Z^TZ = A$
- Exact factorization is generally impossible, cuz the embedding dimension $d$ $\ll$ the # of node $n$
- Learn $Z$ approxomately 数值计算估计
	- Objective: $\mathop{\min}\limits_Z \Vert A - Z^TZ \Vert_2$
		- Frobenius norm: L2 norm of $A - Z^TZ$
- Conclusion: Inner product decoder with node similarity defined by edge connectivity $\iff$ **matrix factorization of** $A$

### RW-based similarity
- DeepWalk $\iff$ matrix facorization of 
	- $\log(vol(G)(\frac{1}{T}\sum_{r = 1}^T{(D^{-1}A)}^r)D^{-1}) - \log b$
		- $vol(G) = \sum\limits_i\sum\limits_jA_{i, j}$ : volume of graph, 即 2 x 连接个数
		- $T = |N_R(u)|$ : context window size，上下文滑窗宽度
		- $^r$ : power of normalized adjacency matrix
		- $D^{-1}$ : diagonal matrix $D$, $D_{u, u} = deg(u)$
		- $b$ : # of negative samples
- Node2vec can also be formulated as a matrix factorization，更复杂
#### Limitation
- Limitations of node embeddings via matrix factorization and random walks
	- Can't obtain embeddings for nodes not in the training set，无法立刻*泛化*到新加入的节点，某种程度的过拟合，需要重新采样随机游走序列，优化更新所有节点嵌入向量
	- Can't capture structural similarity 难以采样到功能结构角色相似，但地理上远隔的节点
		- 解决方案：Anonymous Random Walk、GNN
	- Can't utilize node, edge and graph features
	- Solution: Deep Representation Learning and Graph Neural Networks

#### 小节
- PageRank
	- Measures importance of nodes in graph
	- Can be efficiently computed by power iteration of adjacency matrix
- Personalized PageRank PPR
	- Measures importance of nodes w.r.t. a particular node or set of nodes
	- Can be efficiently computed by random walk
- Node embeddings based on random walks can be expressed as matrix factorization
- All above algorithms focus on viewing graphs as matrices


## 嵌入整张图
- Goal: Wanna embed a subgraph or an entire graph $G$. Graph embedding $z_G$
	- Approch 1, simple but effective
		- Run a standard node embedding technique on the (sub) graph $G$
		- Sum(or average) all the node embeddings
		- $z_G = \sum\limits_{v \in G} z_v$
	- Approch2
		- Introduce a "virtual node" to represent the (sub)graph and run a standard graph embedding technique，超级节点代表全图
	- Approach3, Anonymous Walk Embeddings

### Anonymous Walks
- States in anonymous walks correspond to the index of the 1st time we visited the node in a RW
	- 从头开始，每次见到不同节点，就发一个新编号，只认号不认节点
-  # of anonymous walks grows exponentially

#### Bag of Anonymous Walks
- Simulate anonymous walks $w_i$ of $l$ steps and record their counts
- Sampling anonymous walks: Generate independently a set of $m$ random walks
- Represent the graph as a probability distribution over these walks
- 匿名随机游走长度固定时，欲使误差 $> \epsilon$ 的概率 $< \delta$，需要采样 $m$ 次
	- $m = \left\lceil\frac{2}{{\epsilon}^2}(log(2^{\eta} - 2) - log(\delta))\right\rceil$, where $\eta$ is the total # of anon. walks of length $l$

### New idea: Learn Walk Embeddings
- 给每种匿名随机游走序列单独嵌入编码, 再将所有的匿名游走的embedding加上图的embedding， Learn a graph embedding $z_G$ together with all the anonymous walk embeddings $z_i$ $Z = \{ z_i: i = 1 \dots \eta \}$, where $\eta$ is the # of sampled anonymous walks
- 然后再构造上下文自监督问题
- Starting form node 1: Sample anonymous random walks
- Learn to predict walks that co-occur in $\Delta$-size window, e.g. predict $w_3$ given $w_1, w_2 \enspace if \enspace \Delta = 2$
- Objective: $\mathop{\max}\limits_{z_G} \sum\limits_{t = \Delta + 1}^T \log P(w_t|w_{t-\Delta},\dots,w_{t-1},z_G)$, where $T$ is the total # of walks
	- 对数里面，表示给定图的embedding $z_G$ 和一定范围窗口内的采样匿名随机游走情况下，下一个游走出现的概率
- Run $T$ different RWs from $u$ each of length $l$ : $N_R(u) = \{ w_1^u, w_2^u \dots w_T^u \}$
- Estimate embedding $z_i$ of anon. walk $w_i$, $\eta$ is # of all possible walk embeddings
	- Objective: $\mathop{\max}\limits_{z_i, z_G} \sum\limits_{t = \Delta}^T \log P(w_t|w_{t-\Delta},\dots,w_{t-1},z_G)$
		- $P(w_t|\{w_{t-\Delta},\dots,w_{t-1},z_G\}) = \frac{\exp(y(w_t))}{\sum_{i = 1}^{\eta}\exp(y(w_i))}$，表示当前walk 共现的可能性，分母是归一化因子，确保概率和为1
		- $y(w_t) = {b} + {U} \cdot {(cat(\frac{1}{\Delta}\sum_{i = 1}^{\Delta}z_i, z_G))}$
			- ${cat(\frac{1}{\Delta}\sum_{i = 1}^{\Delta}z_i, z_G)}$ 表示将所有匿名游走的embedding加起来，总共由 $\Delta$ 个，就除以它求平均，然后再和图的embedding拼接起来
			- 最后一起输入到一个线性分类层
- $z_G$ 和 其它 $z_i$ 一起被学习优化
	- Use $z_G$ to make predictions
		- Inner product kernel ${z_{G_1}}^TZ_{G_2}$
		- Use a NN that takes $z_G$ as input to classify $G$

## 本章总结
- 本章介绍了图嵌入表示学习 graph representation learning，无需人工特征工程，通过端到端的表示学习自动学习
- 介绍了图嵌入基本框架包含编码器和解码器，编码采用潜编码器-查表，采用独热编码，解码器基于节点相似度，目的迭代优化每个节点的D-维向量，使得图中相似节点向量数量积大，不相似节点数量积小，而直接优化嵌入向量，采用随机游走
- 介绍了通过随机游走构造自监督学习任务，随机游走的方法步骤，得到目标函数，使用极大似然估计优化目标函数，但由于需要遍历所有节点，时间复杂度太高，使用负采样进行计算优化，采用随机梯度下降法
- 介绍了node2vec，一种比定长，无偏的随机漫步更有丰富表现力的方法。node2vec是有偏，二阶随机游走，引入两个超参数，通过设置不同的超参数，平衡深度优先与广度优先策略
- 介绍了图嵌入和随机游走的矩阵分解角度，就是通过邻接矩阵分解，通过数值计算估计来优化目标函数，采用L2范数。DeepWalk和Node2vec的绝阵分解形式的目标函数。
- 介绍了全图嵌入的三种方法，将图或子图中的所有节点embedding相加或求平均得到；第二种是引入虚拟节点，将其与图和子图中所有节点相连，再计算虚拟节点的embedding，最后的图嵌入的扩展：匿名随机游走。给每种匿名随机游走序列单独嵌入编码，在将所有匿名游走的embeddings加上图的embedding，来构造上下文自监督问题
