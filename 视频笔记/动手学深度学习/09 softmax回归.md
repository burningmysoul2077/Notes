  

###  回归 VS 分类

-  回归估计一个连续值
-  分类预测一个离散类别

-  通常，机器学习实践者用_分类_这个词来描述两个有微妙差别的问题：
	1.  我们只对样本的“硬性”类别感兴趣，即属于哪个类别
	2.  我们希望得到“软性”类别，即得到属于每个类别的概率

-  这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

####  从回归到多类分类：

##### 回归：

-  单连续数值输出
-  自然区间R
-  跟真实值的区别作为损失

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230321163851.png)

##### 分类：

-  **网络架构**
	-  通常多个输出
	-  为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出
	-  需要和输出一样多的 _仿射函数 affine function_。 每个输出对应于它自己的仿射函数
	-  输出 $i$ 是预测为第 $i$ 类的置信度

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230321163910.png)

-  上图中有 4 个特征和 3 个可能的输出类别
	- 需要 12 个标量来表示权重-带下标的𝑤
	- 3个标量来表示偏置-带下标的𝑏 

-  下面为每个输入计算三个 _未规范化的预测_（logit）：$𝑜_1$ 、$𝑜_2$ 和 $𝑜_3$
	- $𝑜_1 = 𝑥_1𝑤_{11} + 𝑥_2𝑤_{12} + 𝑥_3𝑤_{13} + 𝑥_4𝑤_{14} + 𝑏_{1}$
	- $𝑜_2 = 𝑥_1𝑤_{21} + 𝑥_2𝑤_{22} + 𝑥_3𝑤_{23} + 𝑥_4𝑤_{24} + 𝑏_2$
	- $𝑜_3 = 𝑥_1𝑤_{31} + 𝑥_2𝑤_{32} + 𝑥_3𝑤_{33} + 𝑥_4𝑤_{34} + 𝑏_3$
-  通过向量形式表达为 $𝐨=𝐖𝐱+𝐛$
	- 由此，我们已经将所有权重放到一个 3×4 矩阵中
	- 对于给定数据样本的特征 𝐱 ， 输出是由权重与输入特征进行矩阵-向量乘法再加上偏置 𝐛 得到的。

-  由于计算每个输出 $𝑜_1$ 、$𝑜_2$ 和 $𝑜_3$ 取决于 所有输入 $x_1$ 、$x_2$  $x_3$ 和 $x_4$， 所以softmax回归的输出层也是全连接层。

-  分类问题从回归的单输出问题变成多输出
	- softmax回归的输出值个数 = 标签中的类别数

### 增: 置信度和置信区间

#### 点估计 Point Estimation

-  用样本统计量来估计总体参数，因为样本统计量为数轴上某一点值，估计的结果也以一个点的数值表示，所以称为 点估计

#### 区间估计 Interval Estimation

-  区间估计是在点估计的基础上，给出总体参数估计的一个区间范围，该区间通常由样本统计量加减估计误差得到。
-  与点估计不同，进行区间估计时，根据样本统计量的抽样分布可以对样本统计量与总体参数的接近程度给出一个概率度量。

#### 置信区间 Confidence Interval

-  置信区间是指由样本统计量所构造的总体参数的估计区间。在统计学中，一个概率样本的置信区间是对这个样本的某个总体参数的区间估计

#### 置信度 Confidence Level

-  区间估计的估算的区间的准确度(可信度)称为置信度
-  通常情况下，95%被作为常用的置信度
-  随着置信度的上升，置信区间的跨度也就越大，对参数估计的精度必定降低。点估计就一个值，精度高，但置信度则低

### Softmax回归

#### 均方损失

- 对类别进行一位有效编码

##### 独热编码

-  一般的分类问题并不与类别之间的自然顺序有关。 
-  统计学家很早以前就发明了一种表示分类数据的简单方法：_独热编码one-hot encoding_
-  独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 
-  举个例子中，标签 𝑦  是一个三维向量， 其中 (1,0,0) 对应于“猫”、(0,1,0) 对应于“鸡”、(0,0,1) 对应于“狗”： 𝑦∈{(1,0,0),(0,1,0),(0,0,1)}

- $\textbf{y} = [y_{1}, y_{2}, ..., y_{n}]^{T}$
- $y_{i}=\begin{cases} 1&if \enspace i=y\\ 2&otherwise \end{cases}$
- 使用均方损失训练
- 最大值为预测
- $\hat{y}=\underset {i}{argmax}\quad o_{i}$

#### 无校验比例

- 对类别进行一位有效编码
- 最大值为预测
- $\hat{y}=\underset {i}{argmax}\quad o_{i}$
- 需要更置信的识别正确类（大余量），因为我们并不关注数值是多少，更关注置信度
- $o_y - o_i \geq \Delta(y, i)$
- 这样可以将正确的类和其他类区分开

#### 校验比例

- 社会科学家邓肯·卢斯于1959年在 选择模型choice mode）的理论基础上 发明的 _softmax函数_ 正是这样做的： 
	- softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 
- 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 
- 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和

- 输出匹配概率（非负，和为1）
- $\hat{\textbf{y}} = softmax(\textbf{o})$
- $\hat{y_i} = \frac{exp(o_i)}{\sum_{k} exp(o_k)}$

- 真实概率 $\textbf{y}$ 和 $\hat{\textbf{y}}$ 的区别作为损失

#### Softmax和交叉熵损失

##### 对数似然

- softmax函数给出了一个向量 𝐲̂  ， 我们可以将其视为“对给定任意输入 𝐱 的每个类的条件概率”
	- 例如，$𝑦̂_1 =𝑃(𝑦=猫∣𝐱)$。 

- 假设整个数据集 {𝐗,𝐘} 具有 𝑛 个样本， 其中索引 𝑖 的样本由特征向量 $𝐱^{(𝑖)}$ 和独热标签向量 $𝐲^{(𝑖)}$ 组成。 我们可以将估计值与实际值进行比较：
	- $𝑃(𝐘∣𝐗) = \prod\limits_{𝑖=1}^𝑛 𝑃(𝐲^{(𝑖)}∣𝐱^{(𝑖)})$

- 根据最大似然估计，最大化 𝑃(𝐘∣𝐗) ，相当于最小化负对数似然：
	- $−log𝑃(𝐘∣𝐗) = \sum\limits_{𝑖=1}^𝑛 −log𝑃(𝐲^{(𝑖)}∣𝐱^{(𝑖)})= \sum\limits_{𝑖=1}^𝑛 \mathcal{l}(𝐲^{(𝑖)}, 𝐲̂ ^{(𝑖)})$,

- 其中，对于任何标签𝐲 和模型预测 𝐲̂ ，损失函数为：
	- $𝑙(𝐲, 𝐲̂ ) = −\sum\limits_{i = 1}^𝑞 𝑦_ilog𝑦̂_i$

- 交叉熵用来衡量两个概率的区别 $H(p,q) = \sum\limits_{i} -p_{i}log(q_i)$
	- 将它作为损失
		- $l(\textbf{y}, \hat{\textbf{y}}) = -\sum\limits_{i}y_{i}log\hat{y_{i}} = -log\hat{y_y}$
- 因为只有一类 y 为1，其他全部为0，本质就是对真实类别的 y 求log，再求负
- 不关于对非正确类的预测值，只关心正确类预测值的置信度

##### softmax及其导数

- 其梯度是真实概率和预测概率的区别
- $\partial_{o_{i}} l(\textbf{y}, \hat{\textbf{y}}) = softmax(\textbf{o})_{i} - y_{i}$

- 尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个 _线性模型linear model_

#### 小结

- Softmax回归是一个多类分类模型
- 使用Softmax操作子得到每个类的预测置信度，和为1的概率
- 使用交叉熵来衡量和预测标号的区别

### 增: 损失函数

#### L2 Loss

- $l(y, y^{'}) = \frac{1}{2}(y - y^{'})^2$

- 蓝色表示当y=0，y'的函数
- 绿色是它的似然函数
- 橙色是损失函数的梯度，穿过原点的一次函数

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230321184131.png)

> 随着预测值与真实值越来越接近，梯度会随着结果逼近而下降

#### L1 Loss

- $l(y, y^{'}) = \lvert y - y^{'}\rvert$

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230321184115.png)

> 梯度保持不变，但在 0 处不可导、梯度随机

> 当预测值跟真实值比较远时，梯度是常数，好处是稳定，坏处是 0 处不可导，似然函数不平滑

#### Huber's Robust Loss

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230321184059.png)

> 结合L1 Loss 和 L2 Loss的优点

> 当预测值与真实值相差很大时，是一个绝对值误差，在这里在减去 $\frac{1}{2}$ 为的是让曲线连接起来

> 当预测值与真实值比较接近，是均方误差

### 增：信息论基础

#### 信息论 information theory

- 涉及编码、解码、发送以及尽可能简洁地处理信息或数据

#### 熵

- 信息论的核心思想是量化数据中的信息内容
- 在信息论中，该数值被称为分布 𝑃 的 **熵 entropy**， 可以通过以下方程得到：
	- $H[P] = \sum\limits_j  -P(j)logP(j)$

- 信息论的基本定理之一指出，为了对从分布 𝑝 中随机抽取的数据进行编码， 我们至少需要 𝐻[𝑃] “纳特 nat” 对其进行编码
- “纳特” 相当于 _比特 bit_，但是对数底为 𝑒 而不是 2
	- log可分别取2、e、10为底，在工程中相应的的单位分别为比特(bit)、奈特(nat)、迪西特(decit)
- 因此，一个纳特是 $\frac{1}{ln(2)} ≈ 1.44$ 比特。

#### 信息量

-  压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 
-  如果我们很容易预测下一个数据，那么这个数据就很容易压缩。 为什么呢？ 
-  举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，我们总是知道下一个数据是什么。 
-  所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。

-  但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到"惊异"。 
-  克劳德·香农决定用信息量 $log\frac{1}{𝑃(𝑗)} = −log𝑃(𝑗)$ 来量化这种惊异程度
	- 在观察一个事件 𝑗 时，并赋予它（主观）概率 𝑃(𝑗) 
- 当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。

- 熵， 是当分配的概率真正匹配数据生成过程时的 _信息量的期望_ 

#### 重新审视交叉熵

- 如果把熵 𝐻(𝑃) 想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？ 

- 交叉熵 从 𝑃 到 𝑄 ，记为 𝐻(𝑃,𝑄) 
- 我们可以把交叉熵想象为  “主观概率为 𝑄 的观察者在看到根据概率 𝑃 生成的数据时的预期惊异”
- 当 𝑃=𝑄 时，交叉熵达到最低 
	- 在这种情况下，从 𝑃 到 𝑄 的交叉熵是 𝐻(𝑃,𝑃)=𝐻(𝑃)  

- 简而言之，我们可以从两方面来考虑交叉熵分类目标： 
	1. 最大化观测数据的似然
	2. 最小化传达标签所需的惊异。

## 总结
- 本节介绍了回归与分类问题，softmax函数及softmax回归
- 介绍了均方损失、无校验比例、校验比例以及softmax和交叉熵损失，最后推导了softmax的梯度导数
- 介绍了损失函数，包括 L1 损失、L2损失和 结合前两者优点的Huber's Robust 损失
- 补充介绍了信息论基础，熵与信息量以及交叉熵的概念理解



### 图片分类数据集

[09 图像分类数据集.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09%20%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86.ipynb)

### 从零实现softmax回归

[09 softmax回归从零实现.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09%20softmax%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0.ipynb)

### softmax的简洁实现

[09 softmax回归简洁实现.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09%20softmax%E5%9B%9E%E5%BD%92%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0.ipynb)

## softmax回归Q&A

- `softlabel训练策略以及为什么有效？`
>  softmax用指数很难逼近1，softlabel将正例和负例分别标记为0.9和0.1使结果逼近变得可能，这是一个常用的小技巧。

- `softmax回归和logistic回归是一样的吗，如果不一样的话，哪些地方不同？`
>  可以认为是一样的，logistic回归为二分类问题，是softmax回归的特例

- `为什么使用交叉熵，而不用相对熵，互信息熵等其他基于信息量的度量？`
>  相对熵是一种对称关系，互信息熵计算比较复杂。实际上使用哪一种熵的效果区别不大，所以哪种简单就用哪种

- $y*log\hat{y}$ `为什么我们只关心正确类，而不关心不正确的类呢，如果关心不正确类效果有没有可能更好呢？` 
>  并不是不关心，而是独热编码将不正确的的类标号为零，所以算式中不体现，如果使用softlabel策略，就会体现出不正确的类。

-  `这样的n分类，对每一个类别来说，是不是可以认为只有1个正类，n-1个负类吗，会不会类别不平衡呢？`
>   会有不平衡。但是其实更关心样本数量

- `似然函数曲线是怎么得出来的？有什么参考意义？`
>   最小化损失函数也意味着最大化似然函数，似然函数表示统计概率和模型的拟合程度。

- `老师，今天讲的不同损失函数梯度下降的速度和昨天讲的学习率的关系是？昨天理解的学习率好像是梯度下降的步长？步长和速度有点搞不清`
>   w作更新有两项，负的梯度方向和学习率，所以被梯度数值和学习率控制。假设学习率固定，不同损失函数梯度不同导致步长不同

- `Dataloader()的num_workers是并行了嘛？`
>   是的

- `w的方差0.01有什么讲究吗？`
>  之后会讲，方差对深度神经网络是个很重要的东西。这里知道是个超参数就行

- `老师好，pytorch训练好模型，测试的时候发现无论batchsize设为1还是更多，测试的总时间都差不多，但正常理解如果设成4不应该是设为1的4倍吗？`
>  batchsize跟计算量无关，是计算并行度能不能增加，CPU上没有太多区别，GPU上会有区别

- `为什么不在accuracy函数中把除以len(y)做完呢？`
>  因为很可能最后一个batchsize是不满的

- `accuracy函数可以再讲一下吗？`
>  之后会讲在GPU上怎么计算

- `在多次迭代之后欧如果测试精度出现上升后再下降是过拟合了吗？可以提前终止吗？`
> 很有可能是过拟合，可以继续训练来观察是否持续下降

- `cnn网络主要学习到的是纹理还是轮廓还是所有内容的综合？`
>  目前认为主要学习到的是纹理信息

- `是否对全连接层的输出特征进行L2归一化，对最后的损失和精度有什么影响？`
>  之后会详细讲

- `softmax究竟学到了什么？softmax可解释吗？有没有什么指标可以衡量神经网络的解释性`
>  单纯softmax是可解释的，可以在统计书籍中找到相关的解释。
