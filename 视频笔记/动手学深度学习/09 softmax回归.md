

### å›å½’VSåˆ†ç±»ï¼š

- å›å½’ä¼°è®¡ä¸€ä¸ªè¿ç»­å€¼
- åˆ†ç±»é¢„æµ‹ä¸€ä¸ªç¦»æ•£ç±»åˆ«


#### ä»å›å½’åˆ°å¤šç±»åˆ†ç±»ï¼š

- å›å½’ï¼š
	- å•è¿ç»­æ•°å€¼è¾“å‡º
	- è‡ªç„¶åŒºé—´R
	- è·ŸçœŸå®å€¼çš„åŒºåˆ«ä½œä¸ºæŸå¤±

![[Pasted image 20230320114513.png]]

- åˆ†ç±»ï¼š
	- é€šå¸¸å¤šä¸ªè¾“å‡º
	- è¾“å‡º $i$ æ˜¯é¢„æµ‹ä¸ºç¬¬ $i$ ç±»çš„ç½®ä¿¡åº¦

![[Pasted image 20230320114532.png]]

- softmaxå›å½’çš„è¾“å‡ºå€¼ä¸ªæ•° = æ ‡ç­¾ä¸­çš„ç±»åˆ«æ•°

##### å‡æ–¹æŸå¤±

- å¯¹ç±»åˆ«è¿›è¡Œä¸€ä½æœ‰æ•ˆç¼–ç 
	- $\textbf{y} = [y_{1}, y_{2}, ..., y_{n}]^{T}$
	- $y_{i}=\begin{cases} 1&if \enspace i=y\\ 2&otherwise \end{cases}$

- ä½¿ç”¨å‡æ–¹æŸå¤±è®­ç»ƒ
- æœ€å¤§å€¼ä¸ºé¢„æµ‹
	- $\hat{y}=\underset {i}{argmax}\quad o_{i}$

##### æ— æ ¡éªŒæ¯”ä¾‹

- å¯¹ç±»åˆ«è¿›è¡Œä¸€ä½æœ‰æ•ˆç¼–ç 
- æœ€å¤§å€¼ä¸ºé¢„æµ‹
	- $\hat{y}=\underset {i}{argmax}\quad o_{i}$
- éœ€è¦æ›´ç½®ä¿¡çš„è¯†åˆ«æ­£ç¡®ç±»ï¼ˆå¤§ä½™é‡ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸å…³æ³¨æ•°å€¼æ˜¯å¤šå°‘ï¼Œæ›´å…³æ³¨ç½®ä¿¡åº¦
	- $o_y - o_i \geq \Delta(y, i)$
	- è¿™æ ·å¯ä»¥å°†æ­£ç¡®çš„ç±»å’Œå…¶ä»–ç±»åŒºåˆ†å¼€

##### æ ¡éªŒæ¯”ä¾‹

- è¾“å‡ºåŒ¹é…æ¦‚ç‡ï¼ˆéè´Ÿï¼Œå’Œä¸º1ï¼‰
	- $\hat{\textbf{y}} = softmax(\textbf{o})$
	- $\hat{y_i} = \frac{exp(o_i)}{\sum_{k} exp(o_k)}$
- çœŸå®æ¦‚ç‡  $\textbf{y}$ å’Œ $\hat{\textbf{y}}$ çš„åŒºåˆ«ä½œä¸ºæŸå¤±

#### Softmaxå’Œäº¤å‰ç†µæŸå¤±

- äº¤å‰ç†µç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡çš„åŒºåˆ« $H(p,q) = \sum\limits_{i} -p_{i}log(q_i)$
	- å°†å®ƒä½œä¸ºæŸå¤±
	- $l(\textbf{y}, \hat{\textbf{y}}) = -\sum\limits_{i}y_{i}log\hat{y_{i}} = -log\hat{y_y}$
		- å› ä¸ºåªæœ‰ä¸€ç±» y ä¸º1ï¼Œå…¶ä»–å…¨éƒ¨ä¸º0ï¼Œæœ¬è´¨å°±æ˜¯å¯¹çœŸå®ç±»åˆ«çš„ y æ±‚logï¼Œå†æ±‚è´Ÿ
		- ä¸å…³äºå¯¹éæ­£ç¡®ç±»çš„é¢„æµ‹å€¼ï¼Œåªå…³å¿ƒæ­£ç¡®ç±»é¢„æµ‹å€¼çš„ç½®ä¿¡åº¦
- å…¶æ¢¯åº¦æ˜¯çœŸå®æ¦‚ç‡å’Œé¢„æµ‹æ¦‚ç‡çš„åŒºåˆ«
	- $\partial_{o_{i}} l(y, \hat{y}) = softmax(\textbf{o})_{i} - y_{i}$

### æ€»ç»“

- Softmaxå›å½’æ˜¯ä¸€ä¸ªå¤šç±»åˆ†ç±»æ¨¡å‹
- ä½¿ç”¨Softmaxæ“ä½œå­å¾—åˆ°æ¯ä¸ªç±»çš„é¢„æµ‹ç½®ä¿¡åº¦ï¼Œå’Œä¸º1çš„æ¦‚ç‡
- ä½¿ç”¨äº¤å‰ç†µæ¥è¡¡é‡å’Œé¢„æµ‹æ ‡å·çš„åŒºåˆ«

### æŸå¤±å‡½æ•°

#### L2 Loss 

- $l(y, y^{'}) = \frac{1}{2}(y - y^{'})^2$

- è“è‰²è¡¨ç¤ºå½“y=0ï¼Œy'çš„å‡½æ•°
- ç»¿è‰²æ˜¯å®ƒçš„ä¼¼ç„¶å‡½æ•°
- æ©™è‰²æ˜¯æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œç©¿è¿‡åŸç‚¹çš„ä¸€æ¬¡å‡½æ•°
![[Pasted image 20230320152401.png]]

> éšç€é¢„æµ‹å€¼ä¸çœŸå®å€¼è¶Šæ¥è¶Šæ¥è¿‘ï¼Œæ¢¯åº¦ä¼šéšç€ç»“æœé€¼è¿‘è€Œä¸‹é™

#### L1 Loss

- $l(y, y^{'}) = \lvert y - y^{'}\rvert$

![[Pasted image 20230320152703.png]]

> æ¢¯åº¦ä¿æŒä¸å˜ï¼Œä½†åœ¨ 0 å¤„ä¸å¯å¯¼ã€æ¢¯åº¦éšæœº
> å½“é¢„æµ‹å€¼è·ŸçœŸå®å€¼æ¯”è¾ƒè¿œæ—¶ï¼Œæ¢¯åº¦æ˜¯å¸¸æ•°ï¼Œå¥½å¤„æ˜¯ç¨³å®šï¼Œåå¤„æ˜¯ 0 å¤„ä¸å¯å¯¼ï¼Œä¼¼ç„¶å‡½æ•°ä¸å¹³æ»‘

#### Huber's Robust Loss

![[Pasted image 20230320152933.png]]

> ç»“åˆL1 Loss å’Œ L2 Lossçš„ä¼˜ç‚¹
> å½“é¢„æµ‹å€¼ä¸çœŸå®å€¼ç›¸å·®å¾ˆå¤§æ—¶ï¼Œæ˜¯ä¸€ä¸ªç»å¯¹å€¼è¯¯å·®ï¼Œåœ¨è¿™é‡Œåœ¨å‡å» $\frac{1}{2}$ ä¸ºçš„æ˜¯è®©æ›²çº¿è¿æ¥èµ·æ¥
> å½“é¢„æµ‹å€¼ä¸çœŸå®å€¼æ¯”è¾ƒæ¥è¿‘ï¼Œæ˜¯å‡æ–¹è¯¯å·®


### å¢ï¼šä¿¡æ¯è®ºåŸºç¡€

- _ä¿¡æ¯è®º information theory_ ï¼šæ¶‰åŠç¼–ç ã€è§£ç ã€å‘é€ä»¥åŠå°½å¯èƒ½ç®€æ´åœ°å¤„ç†ä¿¡æ¯æˆ–æ•°æ®

#### ç†µ

- ä¿¡æ¯è®ºçš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡åŒ–æ•°æ®ä¸­çš„ä¿¡æ¯å†…å®¹
- åœ¨ä¿¡æ¯è®ºä¸­ï¼Œè¯¥æ•°å€¼è¢«ç§°ä¸ºåˆ†å¸ƒ ğ‘ƒ çš„ **ç†µ entropy**
- å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹ç¨‹å¾—åˆ°ï¼š
	- $ğ»[ğ‘ƒ] = âˆ‘ğ‘—âˆ’ğ‘ƒ(ğ‘—)logğ‘ƒ(ğ‘—)$

- ä¿¡æ¯è®ºçš„åŸºæœ¬å®šç†ä¹‹ä¸€æŒ‡å‡ºï¼Œä¸ºäº†å¯¹ä»åˆ†å¸ƒğ‘ï¿½ä¸­éšæœºæŠ½å–çš„æ•°æ®è¿›è¡Œç¼–ç ï¼Œ æˆ‘ä»¬è‡³å°‘éœ€è¦ğ»[ğ‘ƒ]ï¿½[ï¿½]â€œçº³ç‰¹ï¼ˆnatï¼‰â€å¯¹å…¶è¿›è¡Œç¼–ç ã€‚ â€œçº³ç‰¹â€ç›¸å½“äº_æ¯”ç‰¹_ï¼ˆbitï¼‰ï¼Œä½†æ˜¯å¯¹æ•°åº•ä¸ºğ‘’ï¿½è€Œä¸æ˜¯2ã€‚å› æ­¤ï¼Œä¸€ä¸ªçº³ç‰¹æ˜¯1log(2)â‰ˆ1.441logâ¡(2)â‰ˆ1.44æ¯”ç‰¹ã€‚

### ä¿¡æ¯é‡[](http://localhost:8888/notebooks/Py_Code/d2l-zh/pytorch/chapter_linear-networks/softmax-regression.ipynb#%E4%BF%A1%E6%81%AF%E9%87%8F)

å‹ç¼©ä¸é¢„æµ‹æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè¦å‹ç¼©çš„æ•°æ®æµã€‚ å¦‚æœæˆ‘ä»¬å¾ˆå®¹æ˜“é¢„æµ‹ä¸‹ä¸€ä¸ªæ•°æ®ï¼Œé‚£ä¹ˆè¿™ä¸ªæ•°æ®å°±å¾ˆå®¹æ˜“å‹ç¼©ã€‚ ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ ä¸¾ä¸€ä¸ªæç«¯çš„ä¾‹å­ï¼Œå‡å¦‚æ•°æ®æµä¸­çš„æ¯ä¸ªæ•°æ®å®Œå…¨ç›¸åŒï¼Œè¿™ä¼šæ˜¯ä¸€ä¸ªéå¸¸æ— èŠçš„æ•°æ®æµã€‚ ç”±äºå®ƒä»¬æ€»æ˜¯ç›¸åŒçš„ï¼Œæˆ‘ä»¬æ€»æ˜¯çŸ¥é“ä¸‹ä¸€ä¸ªæ•°æ®æ˜¯ä»€ä¹ˆã€‚ æ‰€ä»¥ï¼Œä¸ºäº†ä¼ é€’æ•°æ®æµçš„å†…å®¹ï¼Œæˆ‘ä»¬ä¸å¿…ä¼ è¾“ä»»ä½•ä¿¡æ¯ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œâ€œä¸‹ä¸€ä¸ªæ•°æ®æ˜¯xxâ€è¿™ä¸ªäº‹ä»¶æ¯«æ— ä¿¡æ¯é‡ã€‚

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä¸èƒ½å®Œå…¨é¢„æµ‹æ¯ä¸€ä¸ªäº‹ä»¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬æœ‰æ—¶å¯èƒ½ä¼šæ„Ÿåˆ°"æƒŠå¼‚"ã€‚ å…‹åŠ³å¾·Â·é¦™å†œå†³å®šç”¨ä¿¡æ¯é‡log1ğ‘ƒ(ğ‘—)=âˆ’logğ‘ƒ(ğ‘—)logâ¡1ï¿½(ï¿½)=âˆ’logâ¡ï¿½(ï¿½)æ¥é‡åŒ–è¿™ç§æƒŠå¼‚ç¨‹åº¦ã€‚ åœ¨è§‚å¯Ÿä¸€ä¸ªäº‹ä»¶ğ‘—ï¿½æ—¶ï¼Œå¹¶èµ‹äºˆå®ƒï¼ˆä¸»è§‚ï¼‰æ¦‚ç‡ğ‘ƒ(ğ‘—)ï¿½(ï¿½)ã€‚ å½“æˆ‘ä»¬èµ‹äºˆä¸€ä¸ªäº‹ä»¶è¾ƒä½çš„æ¦‚ç‡æ—¶ï¼Œæˆ‘ä»¬çš„æƒŠå¼‚ä¼šæ›´å¤§ï¼Œè¯¥äº‹ä»¶çš„ä¿¡æ¯é‡ä¹Ÿå°±æ›´å¤§ã€‚ åœ¨ :eqref:`eq_softmax_reg_entropy`ä¸­å®šä¹‰çš„ç†µï¼Œ æ˜¯å½“åˆ†é…çš„æ¦‚ç‡çœŸæ­£åŒ¹é…æ•°æ®ç”Ÿæˆè¿‡ç¨‹æ—¶çš„_ä¿¡æ¯é‡çš„æœŸæœ›_ã€‚

### é‡æ–°å®¡è§†äº¤å‰ç†µ[](http://localhost:8888/notebooks/Py_Code/d2l-zh/pytorch/chapter_linear-networks/softmax-regression.ipynb#%E9%87%8D%E6%96%B0%E5%AE%A1%E8%A7%86%E4%BA%A4%E5%8F%89%E7%86%B5)

å¦‚æœæŠŠç†µğ»(ğ‘ƒ)ï¿½(ï¿½)æƒ³è±¡ä¸ºâ€œçŸ¥é“çœŸå®æ¦‚ç‡çš„äººæ‰€ç»å†çš„æƒŠå¼‚ç¨‹åº¦â€ï¼Œé‚£ä¹ˆä»€ä¹ˆæ˜¯äº¤å‰ç†µï¼Ÿ äº¤å‰ç†µ_ä»_ğ‘ƒï¿½_åˆ°_ğ‘„ï¿½ï¼Œè®°ä¸ºğ»(ğ‘ƒ,ğ‘„)ï¿½(ï¿½,ï¿½)ã€‚ æˆ‘ä»¬å¯ä»¥æŠŠäº¤å‰ç†µæƒ³è±¡ä¸ºâ€œä¸»è§‚æ¦‚ç‡ä¸ºğ‘„ï¿½çš„è§‚å¯Ÿè€…åœ¨çœ‹åˆ°æ ¹æ®æ¦‚ç‡ğ‘ƒï¿½ç”Ÿæˆçš„æ•°æ®æ—¶çš„é¢„æœŸæƒŠå¼‚â€ã€‚ å½“ğ‘ƒ=ğ‘„ï¿½=ï¿½æ—¶ï¼Œäº¤å‰ç†µè¾¾åˆ°æœ€ä½ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»ğ‘ƒï¿½åˆ°ğ‘„ï¿½çš„äº¤å‰ç†µæ˜¯ğ»(ğ‘ƒ,ğ‘ƒ)=ğ»(ğ‘ƒ)ï¿½(ï¿½,ï¿½)=ï¿½(ï¿½)ã€‚

ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸¤æ–¹é¢æ¥è€ƒè™‘äº¤å‰ç†µåˆ†ç±»ç›®æ ‡ï¼š ï¼ˆiï¼‰æœ€å¤§åŒ–è§‚æµ‹æ•°æ®çš„ä¼¼ç„¶ï¼›ï¼ˆiiï¼‰æœ€å°åŒ–ä¼ è¾¾æ ‡ç­¾æ‰€éœ€çš„æƒŠå¼‚ã€‚

### 3.å›¾ç‰‡åˆ†ç±»æ•°æ®é›†

#### 3.1 Fashion-MNISTæ•°æ®é›†ï¼š

- è¯»å–æ•°æ®é›†

```python

trans=transforms.ToTensor()

mnist_train=torchvision.datasets.FashionMNIST(root="../data",train=True, transform=trans,download=True)

mnist_test=torchvision.datasets.FashionMNIST(root="../data",train=False, transform=trans,download=True)

```

- æ•°æ®é›†å†…å›¾ç‰‡å¤§å°

```python

mnist_train[0][0].shape

torch.Size([1, 28, 28])

```

è¡¨ç¤ºå›¾ç‰‡ä¸ºå•é€šé“ï¼ˆé»‘ç™½ï¼‰çš„28X28çš„å›¾ç‰‡

- æ˜¾ç¤ºæ•°æ®é›†å›¾åƒ

```

X,y = next(iter(data.DataLoader(mnist_train,batch_size=18)))

show_images(X.reshape(18,28,28),2,9,titles=get_fashion_mnist_labels(y))

```

<div align="center">

<img src="../imgs/09/09-08.png" alt="image" align="center"width="500"/>

</div>

### 4.ä»é›¶å®ç°softmaxå›å½’

#### softmax:

$$

softmax(X)_{ij}=\frac{exp(X_{ij})}{\sum_{k} exp(X_{ik})}

$$

```python

def softmax(X):

X_exp = torch.exp(X)

partition = X_exp.sum(1, keepdim=True)

return X_exp / partition

```

1. å°†å›¾åƒå±•å¹³ï¼Œæ¯ä¸ªå›¾åƒçœ‹åšé•¿åº¦ä¸º784çš„å‘é‡ï¼Œå› ä¸ºæ•°æ®é›†æœ‰åä¸ªç±»åˆ«ï¼Œæ‰€ä»¥ç½‘ç»œè¾“å‡ºç»´åº¦ä¸º10ã€‚ä»¥æ­¤è®¾å®šå‚æ•°å¤§å°å¹¶åˆå§‹åŒ–ï¼š

```python

num_inputs = 784

num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)

b = torch.zeros(num_outputs, requires_grad=True)

```

2. å®ç°softmaxå›å½’æ¨¡å‹ï¼š

```python

def net(X):

return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)

```

3. å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š

```python

def cross_entropy(y_hat, y):

return - torch.log(y_hat[range(len(y_hat)), y])

```

4. è®¡ç®—æ­£ç¡®ç‡ï¼š

```python

def accuracy(y_hat, y):

"""è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡"""

if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:

y_hat = y_hat.argmax(axis=1)

cmp = y_hat.type(y.dtype) == y

return float(cmp.type(y.dtype).sum())

```

5. è¯„ä¼°netç²¾åº¦

```python

def evaluate_accuracy(net, data_iter):

"""è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""

if isinstance(net, torch.nn.Module):

net.eval()

metric = Accumulator(2)

with torch.no_grad():

for X, y in data_iter:

metric.add(accuracy(net(X), y), y.numel())

return metric[0] / metric[1]

```

```python

class Accumulator:

"""åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """

def __init__(self, n):

self.data = [0.0] * n

def add(self, *args):

self.data = [a + float(b) for a, b in zip(self.data, args)]

def reset(self):

self.data = [0.0] * len(self.data)

def __getitem__(self, idx):

return self.data[idx]

```

6. å®šä¹‰è®­ç»ƒæ¨¡å‹ï¼š

```python

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):

"""è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""

animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],

legend=['train loss', 'train acc', 'test acc'])

for epoch in range(num_epochs):

train_metrics = train_epoch_ch3(net, train_iter, loss, updater)

test_acc = evaluate_accuracy(net, test_iter)

animator.add(epoch + 1, train_metrics + (test_acc,))

train_loss, train_acc = train_metrics

assert train_loss < 0.5, train_loss

assert train_acc <= 1 and train_acc > 0.7, train_acc

assert test_acc <= 1 and test_acc > 0.7, test_acc

```

7. é¢„æµ‹ï¼š

```python

def predict_ch3(net, test_iter, n=6):

"""é¢„æµ‹æ ‡ç­¾ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""

for X, y in test_iter:

break

trues = d2l.get_fashion_mnist_labels(y)

preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))

titles = [true +'\n' + pred for true, pred in zip(trues, preds)]

d2l.show_images(

X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)

```

<div align="center">

<img src="../imgs/09/09-09.png" alt="image" align="center"width="500"/>

</div>

### 5.softmaxçš„ç®€æ´å®ç°

> è°ƒç”¨torchå†…çš„ç½‘ç»œå±‚

```python

import torch

from torch import nn

from d2l import torch as d2l

batch_size=256

train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)

net=nn.Sequential(nn.Flatten(),nn.Linear(784,10))

def init_weights(m):

if type(m) == nn.Linear:

nn.init.normal_(m.weight,std=0.01)

net.apply(init_weights)

loss=nn.CrossEntropyLoss()

trainer=torch.optim.SGD(net.parameters(),lr=0.1)

num_epochs=10

d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,trainer)

```

### 6.softmaxå›å½’Q&A

**Q1:softlabelè®­ç»ƒç­–ç•¥ä»¥åŠä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

> softmaxç”¨æŒ‡æ•°å¾ˆéš¾é€¼è¿‘1ï¼Œsoftlabelå°†æ­£ä¾‹å’Œè´Ÿä¾‹åˆ†åˆ«æ ‡è®°ä¸º0.9å’Œ0.1ä½¿ç»“æœé€¼è¿‘å˜å¾—å¯èƒ½ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å°æŠ€å·§ã€‚

##### Q2:softmaxå›å½’å’Œlogisticå›å½’ï¼Ÿ

> logisticå›å½’ä¸ºäºŒåˆ†ç±»é—®é¢˜ï¼Œæ˜¯softmaxå›å½’çš„ç‰¹ä¾‹

##### Q3:ä¸ºä»€ä¹ˆä½¿ç”¨äº¤å‰ç†µï¼Œè€Œä¸ç”¨ç›¸å¯¹ç†µï¼Œäº’ä¿¡æ¯ç†µç­‰å…¶ä»–åŸºäºä¿¡æ¯é‡çš„åº¦é‡ï¼Ÿ

> å®é™…ä¸Šä½¿ç”¨å“ªä¸€ç§ç†µçš„æ•ˆæœåŒºåˆ«ä¸å¤§ï¼Œæ‰€ä»¥å“ªç§ç®€å•å°±ç”¨å“ªç§

##### Q4:![](http://latex.codecogs.com/gif.latex?\\y*log\hat{y}) ä¸ºä»€ä¹ˆæˆ‘ä»¬åªå…³å¿ƒæ­£ç¡®ç±»ï¼Œè€Œä¸å…³å¿ƒä¸æ­£ç¡®çš„ç±»å‘¢ï¼Ÿ

> å¹¶ä¸æ˜¯ä¸å…³å¿ƒï¼Œè€Œæ˜¯ä¸æ­£ç¡®çš„çš„ç±»æ ‡å·ä¸ºé›¶ï¼Œæ‰€ä»¥ç®—å¼ä¸­ä¸ä½“ç°ï¼Œå¦‚æœä½¿ç”¨softlabelç­–ç•¥ï¼Œå°±ä¼šä½“ç°å‡ºä¸æ­£ç¡®çš„ç±»ã€‚

##### Q5:ä¼¼ç„¶å‡½æ•°æ›²çº¿æ˜¯æ€ä¹ˆå¾—å‡ºæ¥çš„ï¼Ÿæœ‰ä»€ä¹ˆå‚è€ƒæ„ä¹‰ï¼Ÿ

> æœ€å°åŒ–æŸå¤±å‡½æ•°ä¹Ÿæ„å‘³ç€æœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°ï¼Œä¼¼ç„¶å‡½æ•°è¡¨ç¤ºç»Ÿè®¡æ¦‚ç‡å’Œæ¨¡å‹çš„æ‹Ÿåˆç¨‹åº¦ã€‚

##### Q6:åœ¨å¤šæ¬¡è¿­ä»£ä¹‹åæ¬§å¦‚æœæµ‹è¯•ç²¾åº¦å‡ºç°ä¸Šå‡åå†ä¸‹é™æ˜¯è¿‡æ‹Ÿåˆäº†å—ï¼Ÿå¯ä»¥æå‰ç»ˆæ­¢å—ï¼Ÿ

> å¾ˆæœ‰å¯èƒ½æ˜¯è¿‡æ‹Ÿåˆï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒæ¥è§‚å¯Ÿæ˜¯å¦æŒç»­ä¸‹é™

##### Q7:cnnç½‘ç»œä¸»è¦å­¦ä¹ åˆ°çš„æ˜¯çº¹ç†è¿˜æ˜¯è½®å»“è¿˜æ˜¯æ‰€æœ‰å†…å®¹çš„ç»¼åˆï¼Ÿ

> ç›®å‰è®¤ä¸ºä¸»è¦å­¦ä¹ åˆ°çš„æ˜¯çº¹ç†ä¿¡æ¯

##### Q8:softmaxå¯è§£é‡Šå—ï¼Ÿ

> å•çº¯softmaxæ˜¯å¯è§£é‡Šçš„ï¼Œå¯ä»¥åœ¨ç»Ÿè®¡ä¹¦ç±ä¸­æ‰¾åˆ°ç›¸å…³çš„è§£é‡Šã€‚
