>  上一节描述了过拟合的问题，本节将介绍一些正则化模型的技术。 
>  我们总是可以通过去收集更多的训练数据来缓解过拟合。 但这可能成本很高，耗时颇多，或者完全超出 我们的控制，因而在短期内不可能做到。
>  假设我们已经拥有尽可能多的高质量数据，我们便可以将重点放在正则化技术上。
>  
>  **权重衰退 weight decay** 是最广泛使用的**正则化的技术**之一


* [使用均方范数](#使用均方范数)
    * [硬性限制/直观理解](#硬性限制直观理解)
  * [柔性限制/实际应用](#柔性限制实际应用)
* [参数更新法则](#参数更新法则)
  * [计算梯度](#计算梯度)
    * [补：偏置项](#补偏置项)
* [权重衰退](#权重衰退)
    * [为什么先使用 𝐿2 范数，而不是 𝐿1 范数](#为什么先使用-𝐿2-范数而不是-𝐿1-范数)
* [权重衰减的代码实现](#权重衰减的代码实现)
* [总结](#总结)
* [权重衰退 Q&A](#权重衰退-qa)


# 使用均方范数

- 上一节讲到如何控制模型容量
	- 限制模型参数
	- 限制参数值的选择范围，正是 权重衰退 的方法

### 硬性限制/直观理解

- 优化目标仍然是 $min \enspace \mathscr{l}(\textbf w, b)$，添加一个限制条件 subject to $||w||^2 \leq 0$
	- 即权重的各项平方和小于一个特定的常数 $\theta$
	- 那么 $\textbf w$ 的每一个值都要小于 $\sqrt \theta$
	- 如果 $\theta$ 选值很小，那么 $\textbf w$ 的值也都会很小
	- 小的 $\theta$ 意味着更强的正则项
		- 最强的是 $\theta = 0$，那意味着 $\textbf w$ 都没有了，只有 $b$
		- 一般 $\theta = 1 \enspace 0.1 \enspace 0.01...$

- 通常不会限制偏移 b，理论上讲 b 表示整个数据在零点上的偏移，因此是不应该限制的
- 实际上，限不限制对结果都没什么影响

- 补： **吴恩达课程中对这一现象的解释是w是高维向量，已经包含了绝大多数参数足以表达高方差问题，b 作为单个数字对结果的影响就会很小.** [转载见](https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/blob/main/notes/12-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80.md)


## 柔性限制/实际应用

- 对于每个 $\theta$，都可以找到 $\lambda$ 使得之前的目标函数等价于下面
	- $min \enspace \mathscr l(\textbf w, b) + \frac{\lambda}{2}||w||^2$
	- 可以通过拉格朗日乘子证明 这个式子等价于硬性限制的目标函数的式子

-  本质是将原来的训练目标 _最小化训练标签上的预测损失_， 调整为 _最小化预测损失和惩罚项之和_ 。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数 $‖𝐰‖^2$

- 其中 $\frac{\lambda}{2}||w||^2$  被称为 *罚 penalty*
	-  这里我们除以 2 是因为当取一个二次函数的导数时， 2 和 1/2 会抵消，以确保更新表达式看起来既漂亮又简单

- $\lambda$ 是超参数，控制了正则项的重要程度
	- 当 $\lambda = 0$  无作用，恢复了原来的损失函数
	- 当 $\lambda \rightarrow \infty$  最优解 $\textbf w* \rightarrow 0$
	- 也就是说  $\lambda$  越大，模型复杂度就被控制的越低
- 回想一下，也就是说，为了惩罚权重向量的大小， 我们必须以某种方式在损失函数中添加 $‖𝐰‖^2$ 
- 但是模型应该如何平衡这个新的额外惩罚的损失？ 实际上，我们通过 _正则化常数 𝜆_ 来描述这种权衡

- 演示对最优解的影响

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230324162238.png)


-  绿线是原本损失函数函数值 $\mathscr l$ 的“等高线”，需要优化它
-  黄线可以看作是正则项 $\frac{\lambda}{2}||w||^2$ 对应函数值的“等高线” ，它以原点为中心
-  使用权重衰减后需要优化的损失函数相当于图中两组等高线叠加
-  原始最优解 $\widetilde{ \textbf w}*$ 位于绿色中心，现在这一位置在对于正则项（黄线）有很高的损失，而正则项最小值位于原点，因此罚的作用力会大于 $\mathscr l$，这样会往下拉直到平衡点，现在的最终优化解会更靠近原点
-  而当所有参数都更靠近原点时模型复杂度变低，规模也就更小

# 参数更新法则

## 计算梯度

- 回忆一下， $\frac{\partial{}}{\partial{\textbf {w}}}(\ell(\textbf{w}, b) + \frac{\lambda}{2}||\textbf{w}||^2) = \frac{\partial{\ell(\textbf{w},b)}}{{\partial{\textbf{w}}}} + \lambda\textbf{w})$

- 时间 t 更新参数
	- 已知  $w_{t+1} = w_t - \eta \frac{\partial}{\partial \textbf w_t}$
	- 带入更新参数公式
		- $\textbf w_{t + 1} = (1 - \eta\lambda)\textbf {w}_{t} - \eta\frac{\partial{\ell(\textbf w_t, b_t)}}{\partial{\textbf w_t}}$

- 注意到这个公式中后一项与原来更新参数的公式没有区别，仅仅是在前一项 $\textbf{w}_{t}$ 上加了一个系数 $(1-\eta\lambda)$
-  较小的 𝜆 值对应较少约束的 𝐰 ， 而较大的 𝜆 值对 𝐰 的约束更大

### 补：偏置项
- 是否对相应的偏置 $𝑏^2$ 进行惩罚在不同的实践中会有所不同， 在神经网络的不同层中也会有所不同。 通常，网络输出层的偏置项 **不会被正则化**。

# 权重衰退

- 通常 $\eta\lambda<1$ ，如果大于 1 抖动会比较大
	- 也就是说由于引入了 $\lambda$，每次更新参数前先给待更新参数乘上一个小于 1 的权重再更新
	- 即我们根据估计值与观测值之间的差异来更新 𝐰 。 然而，我们同时也在试图将 𝐰 的大小缩小到零，*权重衰退* 由此得名
	- 我们仅考虑惩罚项，优化算法在训练的每一步 衰退权重

- 权重衰退 是最广泛使用的正则化的技术之一， 它通常也被称为 𝐿2 正则化 

### 为什么先使用 𝐿2 范数，而不是 𝐿1 范数

- 事实上，这个选择在整个统计领域中都是有效的和受欢迎的。 
	- 𝐿2 正则化线性模型构成经典的 _岭回归ridge regression_ 算法
	- 𝐿1 正则化线性回归是统计学中类似的基本模型， 通常被称为 _套索回归 lasso regression_ 

- 使用 𝐿2 范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。
- 在实践中，这可能使它们对单个变量中的观测误差更为稳定。
- 相比之下，𝐿1 惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为 _特征选择feature selection_ ，这可能是其他场景下需要的

# 权重衰减的代码实现

[12 权重衰退的代码实现](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12%20%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80%E7%9A%84%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0.ipynb)

# 总结

- 本章介绍了均方范数的硬性限制和柔性限制
- 介绍了通过参数更新法则，计算梯度得到的值与原来更新参数的公式没有大的区别，仅仅加了一个系数 $(1-\eta\lambda)$，引出了 超参数 $\lambda$
- 介绍了权重衰退，它通过L2正则项使得模型参数不会过大，从而控制模型复杂度
- 介绍了为什么使用 L2 范数
- 介绍了权重衰退的代码从零实现和简洁实现

# 权重衰退 Q&A

- `Pytorch是否支持复数神经网络？``
>  应该是不支持，但是，复数可以看作是二维的数，可以尝试将对应结构变成二维来实现需要的效果。

- `为什么参数不过大复杂度就低呢？``
>  确切的说是限制整个模型优化时只能在很小范围内取参数，整个模型的控件变小，会使模型复杂度降低
>  参数选择范围大时可拟合出很复杂、非常不平滑的曲线，限制后只能学到更平滑的曲线/选择更简单的模型，那么意味着模型复杂度就变低了，无法学习复杂的模型

- `如果使用L1范数如何更新权重？``
>  一样的，就是编写代码时只需把罚项改成 w 的 L1 范数

>  老师解答就到这里，但实操不应该只改罚项函数，还需重新定义带正则项的损失函数并求导化简。
>  $\frac{\partial{}}{\partial{\mathbf{w}}}(\ell(\mathbf{w}, b)+\lambda||\mathbf{w}||_1) = \frac{\partial{\ell(\mathbf{w}, b)}}{{\partial{\mathbf{w}}}} + I'\lambda$
>  其中 $I'=(a_1,...,a_n))$ ,当 $\mathbf{w}$ 中第 $i$ 个元素为正时 $a_i=1$，反之 $a_i=-1$ ，=0时随意
>  代入公式化简化得 $\mathbf{w}_{t + 1} = \mathbf{w}_{t} - \eta\frac{\partial{\ell(\mathbf{w}_t, b_t)}}{{\partial{\mathbf{w}_{t}}}} - I'\eta\lambda$
>  从这个式子可以看出使用 L1 正则化时只能对所有同号的参数施加一个相同大小的正则项（增减一个定值），而反观L2正则化对参数的影响是与参数本身的值有关的（乘上一个系数）似乎是更好的选择。
>  不过L1正则化在特征提取上会有用处。

- `实践中权重衰减的值设置为多少好？跑代码时感觉效果不明显`
>  一般取 1e-2, 1e-3, 1e-4，权重衰退的效果确实有限，之后还会讲解更多方法。
>  如果模型真的很复杂那么权重衰退一般不会带来特别好的效果。

- `关于L2范数的记法`
>  完整的写法是 $||\mathbf{w}||^2_2$ ，上标的 2 表示平方，下标的 2 表示是 L2 范数，下标有时省略。

- `为什么要把 w 往小拉？如果最优解的 w 本来就较大，权重衰减是否会起反作用？/正则项使得  w 变得更平均,没有突出的值为什么可以拟合的更好呢？`
>  实际上，训练的数据都是有噪音的，，而这些噪音可能会被拟合进去使得我们实际求解时得不到数学上的最优解，模型很可能学到所有噪音，这样最优解很可能学得特别大，正则化起到将结果拉向最优解的作用。
>  需要控制 $\lambda$ 大小，来告诉把最优解拉回来多少，假设 $\lambda$ 选取过大 / 过小 都可能拉偏
>  这里的最优解是数学的最优解，实际上得不到，如果没有过拟合那权重衰减就不起作用。

- `噪音大会使得 w 较大是经验所得还是可以证明？
>  可以证明，但本课程中不讲，可以自己尝试。

- `怎样调整 lambda？``
> 不能确定什么时候是最优，但可以用前面讲的验证集/ k折交叉验证，先取 $\lambda=0$ 看训练结果，再改变 $\lambda$ 看是否有改善


