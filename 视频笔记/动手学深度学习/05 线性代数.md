## 05 线性代数

### 线性代数基础知识

#### 标量
- 简单操作
> $c = a+b$
> $c = a \cdot b$
> $c = \sin a$

- 长度
> $|a| = \left\{\begin{matrix}  a \enspace if \enspace a>0 \\    -a \enspace otherwise \end{matrix}\right.$
> $|a+b| \leq |a| + |b|$
> $|a \cdot b| = |a| \cdot |b|$

#### 向量
- 简单操作
> $c = a+b$  where $c_i = a_i + b_i$
> $c = \alpha \cdot b$  where $c = \alpha b_i$
> $c = \sin a$  where $c_i = \sin a_i$

- 长度
> $||a||_2 = [\sum\limits_{i=1}^m a_i^2]^\frac{1}{2}$  $||a|| \geq 0$ for all a
> $||a+b|| \leq ||a|| + ||b||$
> $||a \cdot b|| = |a| \cdot ||b||$

- 点乘
> $a^Tb = \sum\limits_ia_ib_i$

- 正交
> $a^Tb = \sum\limits_ia_ib_i = 0$

#### 矩阵

##### 矩阵的操作

- 简单操作
> $C = A + B$  where $C_{ij} = A_{ij} + B_{ij}$
> $C = \alpha \cdot B$  where $C_{ij} = \alpha B_ij$
> $C = \sin A$  where $C_{ij} = \sin A_{ij}$

- 乘法
	- 矩阵乘以向量
	> $c = Ab$   where $C_i = \sum\limits_jA_{ij}b_j$
	- 矩阵乘以矩阵
	> $C = AB$  where $C_{ik} = \sum\limits_jA_{ij}B_{jk}$
	
- 范数
> $c = Ab$  hence  $||c|| \leq ||A|| \cdot ||b||$
	- 取决于如何衡量 b 和 c 的长度

- 常见范数 
	- 矩阵范数：最小的满足的上面公式的值
	- Frobenius 范数：$||A||_{Frob} = [\sum\limits_{ij}A_{ij}^2]^{\frac{1}{2}}$
- 矩阵范数麻烦且不常用，一般用 F 范数

##### 特殊矩阵

- 对称和反对称
> $A_{ij} = A_{ji}$  and  $A_{ij} = -A_{ji}$

- 正定
> $||x||^2 = x^Tx \geq 0$  generalizes to  $x^TAx \geq 0$

- 正交矩阵
	- 所有行都相互正交
	- 所有行都有单位长度  $U$ with $\sum\limits_j U_{ij}U_{kj} = \delta_{ik}$
	- 可以写成 $UU^T = 1$

- 置换矩阵
	- $P$ where $P_{ij} = 1$ if and only if $j = \pi (i)$
	- 置换矩阵是正交矩阵

- 深度学习里基本不会涉及到正定、置换矩阵，这里明确个概念就行

##### 特征向量和特征值

- 数学定义：设 $A$ 是 $n$ 阶方阵，如果存在常数 $\lambda$ 及非零向量 $x$，使得 $Ax = \lambda x$，则称 $\lambda$ 是矩阵 $A$ 的特征值，$x$ 是 $A$ 属于特征值 $\lambda$ 的特征向量
- 直观理解：不被矩阵 $A$ 改变方向的向量 $x$ 就是 $A$ 的一个特征向量
- 矩阵不一定有特征向量，但是对称矩阵总是可以找到特征向量

### 线性代数实现

这部分主要是应用pytorch实现基本矩阵操作，同样由标量过渡到向量最后拓展到矩阵

#### 标量
- 是由只有一个元素的张量表示

```python
import torch # 应用pytorch框架

# 标量由只有一个元素的张量表示
>>> x = torch.tensor([3.0]) # 单独一个数字表示标量也可以
>>> y = torch.tensor([2.0]) # 单独一个数字表示标量也可以
>>> print(x + y) # tensor([5.])
>>> print(x * y) # tensor([6.])
>>> print(x / y) # tensor([1.5000])
>>> print(x ** y) # tensor([9.]) 指数运算
```

#### 向量
- 向量可以被视为标量值组成的列表
- 这些标量值被称为向量的 _元素_（element）或 _分量_（component）
- 人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制

```python
# 生成[0, 4)范围内所有整数构成的张量tensor
>>> x = torch.arange(4) # tensor([0, 1, 2, 3])

# 和列表相似，通过张量的索引访问元素
>>> print(x[3]) # tensor(3)

# 获取张量x的长度
>>> print(len(x)) # 4

# 获取张量形状，这里x是只有一个轴的张量因此形状只有一个元素
>>> print(x.shape) # torch.Size([4])
```

#### 矩阵
- 正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶
- 对于任意 $𝐀∈ℝ^{𝑚×𝑛}$， $𝐀$ 的形状是（𝑚,𝑛）或 𝑚×𝑛
- 当矩阵具有相同数量的行和列时，其形状将变为正方形； 因此，它被称为 _方阵_（square matrix）

- 通过指定两个分量 m 和 n 来创建一个形状为 m x n 的矩阵

```python
>>> A = torch.arange(20).reshape(5,4)
>>> A

tensor([[ 0, 1, 2, 3],
	    [ 4, 5, 6, 7],
	    [ 8, 9, 10, 11],
	    [12, 13, 14, 15],
	    [16, 17, 18, 19]])
```

- 矩阵转置 transpose

```python
>>> A.T

tensor([[ 0, 4, 8, 12, 16],
	    [ 1, 5, 9, 13, 17],
	    [ 2, 6, 10, 14, 18],
	    [ 3, 7, 11, 15, 19]])
```

- 作为方阵的一种特殊类型，对称矩阵 symemetric matrix  $A$
	- 等于其转置  $A = A^T$

```python
>>> B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
>>> B

tensor([[ 1, 2, 3],
	    [ 2, 0, 4],
	    [ 3, 4, 5]])
```

```python
>>> B == B.T

tensor([[ True, True, True],
	    [ True, True, True],
	    [ True, True, True]])
```

#### 张量

- 就像向量是标量的推广，矩阵是向量推广一样，我们可以构建具有更多轴的数据结构
- 张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的 𝑛 维数组的通用方法
	- 向量是一阶张量，矩阵是二阶张量。
- 张量用特殊字体的大写字母表示（例如，𝖷、𝖸和𝖹）， 它们的索引机制与矩阵类似。
- 当我们开始处理图像时，张量将变得更加重要，图像以 𝑛 维数组形式出现， 其中3个轴对应于高度、宽度，以及一个_通道_（channel）轴， 用于表示颜色通道（红色、绿色和蓝色）

```python
>>> X = torch.arange(24).reshape(2, 3, 4)
>>> X

tensor([[[ 0, 1, 2, 3],
	    [ 4, 5, 6, 7],
	    [ 8, 9, 10, 11]],
	    
	    [[ 12, 13, 14, 15],
	     [ 16, 17, 18, 19],
	     [ 20, 21, 22, 23]]])
```

##### 张量算法的基本性质
- 给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量

```python
>>> A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
>>> B = A.clone()  #  通过分配新内存，将A的一个副本分配给B
>>> A, A + B

(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
```

- 两个矩阵的按元素乘法 称为 _哈达玛积 Hadamard product_  数学符号 $\odot$

```python
>>> A * B

tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```

- **将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘**

```python
>>> a = 2
>>> X = torch.arange(24).reshape(2, 3, 4)
>>> a + X, (a * X).shape

(tensor([[[ 2,  3,  4,  5],
          [ 6,  7,  8,  9],
          [10, 11, 12, 13]],
 
         [[14, 15, 16, 17],
          [18, 19, 20, 21],
          [22, 23, 24, 25]]]),
 torch.Size([2, 3, 4]))
```

- 计算其元素的和

```python
>>> x = torch.arange(4, dtype=torch.float32)
>>> x, x.sum()

(tensor([0., 1., 2., 3.]), tensor(6.))
```

#### 降维
- 表示任意形状张量的元素和

```python
>>> A.shape, A.sum()

(torch.Size([5, 4]), tensor(190.))
```

- **指定张量沿哪一个轴来通过求和降低维度**
- 默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量
- axis = 0 以行为轴，留列；axis = 1 以列为轴，留行。axis = 去掉那一维

```python
>>> A = torch.arange(20*2).reshape(2, 5, 4)
>>> A.shape, A.sum()

(torch.Size([2, 5, 4]), tensor(780))
```

```python
>>> A_sum_axis0 = A.sum(axis=0)
>>> A_sum_axis0, A_sum_axis0.shape

(tensor([[20, 22, 24, 26],
         [28, 30, 32, 34],
         [36, 38, 40, 42],
         [44, 46, 48, 50],
         [52, 54, 56, 58]]),
 torch.Size([5, 4]))
```

```python
>>> A_sum_axis1 = A.sum(axis=1)
>>> A_sum_axis1, A_sum_axis1.shape

(tensor([[ 40,  45,  50,  55],
         [140, 145, 150, 155]]),
 torch.Size([2, 4]))
```

```python
>>> A.sum(axis=[0, 1]).shape

torch.Size([4])
```

- 一个与求和相关的量是 平均值 mean/average

```python
>>> A.mean(), A.sum() / A.numel()

(tensor(9.5000), tensor(9.5000))
```

- **计算平均值的函数也可以沿指定轴降低张量的维度**

```python
>>> A.mean(axis=0), A.sum(axis=0) / A.shape[0]

(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))
```

#### 非降维求和
- **计算总和或均值时保持轴数不变**

```python
>>> sum_A = A.sum(axis=1, keepdims=True)
>>> sum_A

tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```

- 由于`sum_A`在对每行进行求和后仍保持两个轴，我们可以**通过广播将`A`除以`sum_A`**

```python
>>> A / sum_A

tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```

- **某个轴计算`A`元素的累积总和**
- 可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度
- 累计总和就是第一行不变，第二行变为1+2，第三行1+2+3

```python
>>> A.cumsum(axis=0)

tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

#### 点积（Dot Product）
- 点积是相同位置的按元素乘积的和

```python
>>> y = torch.ones(4, dtype = torch.float32)
>>> x, y, torch.dot(x, y)

(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))
```

- 可以通过执行按元素乘法，然后进行求和来表示两个向量的点积

```python
>>> torch.sum(x * y)

tensor(6.)
```

- 点积在很多场合都很有用
- 例如，给定一组由向量 $𝐱∈ℝ^𝑑$ 表示的值， 和一组由 $𝐰∈ℝ^𝑑$ 表示的权重。 𝐱 中的值根据权重 𝐰 的加权和， 可以表示为点积 $𝐱^⊤𝐰$
- 当权重为非负数且和为1（即 $∑^𝑑_{𝑖=1}𝑤_𝑖=1$）时， 点积表示 _加权平均_（weighted average）
- 将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。

#### 矩阵-向量积 matrix-vector product

- 矩阵向量积 𝐀𝐱 是一个长度为 𝑚 的列向量， 其第 𝑖 个元素是点积 $𝐚^⊤_𝑖𝐱$
- 在代码中使用张量表示矩阵-向量积，使用 `mv` 函数。 当矩阵 `A` 和向量 `x` 调用 `torch.mv(A, x)` 时，会执行矩阵-向量积。 注意，`A` 的列维数（沿轴1的长度）必须与 `x` 的维数（其长度）相同

```python
>>> A.shape, x.shape, torch.mv(A, x)

(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))
```

#### 矩阵-矩阵乘法 matrix-matrix multiplication

- 可以将矩阵-矩阵乘法 𝐀𝐁 看作简单地执行 𝑚 次矩阵-向量积，并将结果拼接在一起，形成一个 𝑛×𝑚 矩阵

```python
>>> B = torch.ones(4, 3)
>>> torch.mm(A, B)

tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
```

#### 范数

- 线性代数中最有用的一些运算符是 _范数_（norm）
- 非正式地说，向量的 _范数_ 是表示一个向量有多大。 这里考虑的 _大小_（size）概念不涉及维度，而是分量的大小。

- 在线性代数中，向量范数是将向量映射到标量的函数 𝑓 
- 给定任意向量 𝐱 ，向量范数要满足一些属性
	- 第一个性质是：如果按常数因子 𝛼 缩放向量的所有元素， 其范数也会按相同常数因子的 _绝对值_ 缩放：𝑓(𝛼𝐱)=|𝛼|𝑓(𝐱)
	- 第二个性质是熟悉的三角不等式:  𝑓(𝐱+𝐲)≤𝑓(𝐱)+𝑓(𝐲)
	- 第三个性质简单地说范数必须是非负的:  𝑓(𝐱)≥0
		- 这是有道理的。因为在大多数情况下，任何东西的最小的 _大小_ 是 0。
	- 最后一个性质要求  范数最小为 0，当且仅当向量全由 0 组成。
		- ∀𝑖, $[𝐱]_𝑖$ = 0 ⇔ 𝑓(𝐱) = 0

- 范数听起来很像距离的度量。 欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。 
- 事实上，欧几里得距离是一个 𝐿2 范数： 假设 𝑛 维向量 𝐱 中的元素是 $𝑥_1,…,𝑥_𝑛$，其 𝐿2范数 是向量元素平方和的平方根
	- $‖𝐱‖_2 = \sqrt{∑_{𝑖=1}^𝑛 𝑥^2_𝑖}$
- 其中，在 𝐿2 范数中常常省略下标 2，也就是说 ‖𝐱‖ 等同于 $‖𝐱‖_2$ 

- 按如下方式计算向量的 𝐿2 范数。

```python
>>> u = torch.tensor([3.0, -4.0])
>>> torch.norm(u)

tensor(5.)
```

- 𝐿1范数，它表示为向量元素的绝对值之和
- 与 𝐿2 范数相比，𝐿1 范数受异常值的影响较小

```python
>>> torch.abs(u).sum()

tensor(7.)
```

- 矩阵 $𝐗∈ℝ^{𝑚×𝑛}$ 的 _Frobenius范数_ Frobenius norm 是矩阵元素平方和的平方根
	- $‖𝐗‖_𝐹 = \sqrt{∑_{𝑖=1}^𝑚∑_{𝑗=1}^𝑛 𝑥^2_{𝑖𝑗}}$
- Frobenius 范数满足向量范数的所有性质，它就像是矩阵形向量的 𝐿2 范数

```python
>>> torch.norm(torch.ones((4, 9)))

tensor(6.)
```

##### 范数和目标

- 在深度学习中，我们经常试图解决优化问题： 
	- _最大化_ 分配给观测数据的概率; 
	- _最小化_ 预测和真实观测之间的距离。 
- 用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。 
- 目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。


## 线性代数Q&A
- **这么转换有什么负面影响嘛？比如数值变得稀疏**
>是会变得稀疏，可以用稀疏矩阵来承载，稀疏对深度学习没有大的影响。其他负面影响应该是没有的

- **为什么深度学习要用张量来表示？**
>整个机器学习都是用张量或是数值矩阵来表示，因为这也有着历史的原因。机器学习的统计版本会更偏数学一点。

- **求copy 和 clone 的区别（是关于内存吗）？**
>copy分为深拷贝和浅拷贝。clone是一定会复制内存

- **torch不区分行向量和列向量吗？**
>向量只能是行向量，列向量可以被看作是一个矩阵，一维数组

- **sum(axis=[0,1])怎么求？**
>想象成RGB三通道，先对channel=0求和，再对channel=1求和

- **请问老师 病理图片的SVS格式 和医生勾画的区域XML格式的文件 怎么进行预处理？**
>1. 就当成图片，用像素级解决
>2. 也可以当成结构化输入，NLP CV解决
