## 08 线性回归

### 线性回归

-  _回归 regression_  是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。
-  _线性回归 linear regression_  可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行
-  举例：房价

#### 线性回归的基本元素

-  线性回归基于几个简单的假设： 
	-  首先，假设自变量 𝐱 和因变量 𝑦 之间的关系是线性的， 即 𝑦 可以表示为 𝐱 中元素的加权和
		-  这里通常允许包含观测值的一些噪声
	-  其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

-  为了解释 _线性回归_，我们举一个实际的例子
-  我们希望根据房屋的面积（平方英尺）和 房龄（年）来估算房屋价格（美元）。 
-  为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 
-  在机器学习的术语中，该数据集称为 **训练数据集 training data set**  或 **训练集 training set**
 -  每行数据（比如一次房屋交易相对应的数据）称为 **样本 sample**， 也可以称为 **数据点 data point ** 或 **数据样本 data instance** 
 -  把试图预测的目标（比如预测房屋价格）称为 **标签 label** 或 **目标 target**
 -  预测所依据的自变量（面积和房龄）称为 **特征 feature** 或 **协变量 covariate**

-  通常，我们使用 𝑛 来表示数据集中的样本数
-  对索引为 𝑖 的样本，其输入表示为 $𝐱^{(𝑖)}=[𝑥^{(𝑖)}_1, 𝑥^{(𝑖)}_2]^⊤$  其对应的标签是 $𝑦^{(𝑖)}$

#### 线性模型

-  给定 **$n$ 维输入**：$x=[x_1,x_2,...,x_n]^T$

-  线性模型需要确定一个n维权重和一个标量偏差
	-  $\omega=[\omega_1,\omega_2,...,\omega_n]^T , \enspace b$
	-  𝑤 称为 _权重 weight_ 
		-  权重决定了每个特征对我们预测值的影响
	-   𝑏 称为 _偏置 bias_、_偏移量 offset_ 或 _截距 intercept_
		-  偏置是指当所有特征都取值为 0 时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。 如果没有偏置项，我们模型的表达能力将受到限制

-  输出 ：$y = \omega_1x_1 + \omega_2x_2 + ... + \omega_nx_n + b$，
	- 向量版本的是 $y=<\omega,x> + b   = 𝐰^⊤𝐱 + 𝑏$
	- 输出是输入的加权和
-  严格来说， 是输入特征的一个 _仿射变换 affine transformation_
-  仿射变换的特点是通过加权和对特征进行 _线性变换 linear transformation_ ， 并通过偏置项来进行 _平移translation_

-  对于特征集合 𝐗 ，预测值 $𝐲̂ ∈ℝ^𝑛$ 可以通过矩阵-向量乘法表示为：
	- 𝐲̂ =𝐗𝐰+𝑏

-  **Goal**
	-  给定训练数据特征 𝐗 和对应的已知标签 𝐲 ， 线性回归的目标是找到一组权重向量 𝐰 和偏置 𝑏  
	-  当给定从 𝐗 的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小

-  虽然我们相信给定 𝐱 预测 𝑦 的最佳模型会是线性的， 但我们很难找到一个有 𝑛 个样本的真实数据集，其中对于所有的 1 ≤ 𝑖 ≤ 𝑛，$𝑦^{(𝑖)}$ 完全等于 $𝐰^⊤𝐱^{(𝑖)} + 𝑏$ 。 
-  无论我们使用什么手段来观察特征 𝐗 和标签 𝐲 ， 都可能会出现少量的观测误差。
-  因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。

-  在开始寻找最好的 _模型参数 model parameters_  𝐰 和 𝑏 之前， 还需要两个东西： 
  1.  一种模型质量的度量方式
  2.  一种能够更新模型以提高模型预测质量的方法。

-  线性模型可以看作是单层神经网络

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230318145155.png)

>- 神经网络源于神经科学
>- 最早的神经网络是源自神经科学的，但是时至今日，很多神经网络已经远远高于神经科学，可解释性也不是很强，不必纠结
-  神经科学术语
	- 输入  -  Dendrite  树突
	- 计算发生处  -  Cell body  细胞体  Nucleus  神经核
	- 输出  -  Axon 轴突
	- 输出到下一层  -  Myelin Sheath  髓鞘  Schwann Cell  施万细胞

#### 衡量估计质量

-  我们需要估计模型的预估值和真实值之间的差距，例如房屋售价和估价
-  _损失函数 loss function_ 能够量化目标的 _实际_ 值与 _预测_ 值之间的差距
-  通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。
-  回归问题中最常用的损失函数是平方误差函数
-  假设 $\cal y$ 是真实值，$\tilde{y}$ 是估计值，可以比较
	- **平方损失**  -  $l(y,\tilde{y})=\frac{1}{2}(y-\tilde{y})^2$
		- $\frac{1}{2}$ 作用是方便求导

#### 训练数据

- 收集一些数据点来决定参数值（权重$\omega$和偏差$b$），例如6个月内被卖掉的房子
- 这被称之为**训练数据**
- 通常越多越好
	- 需要注意的是，现实世界的数据都是有限的，但是为了训练出精确的参数往往需要训练数据越多越好，当训练数据不足的时候，我们还需要进行额外处理。

- 假设我们有 $n$ 个样本，记为
	- ${X} = [𝐱_1, 𝐱_2, ..., 𝐱_n]^T, y = [y_1, y_2, ..., y_n]^T$
	- $X$ 的每一行是一个样本，$y$ 的每一行是一个输出的实数值。

#### 参数学习

- **训练损失**
	- 训练参数的时候，需要定义一个损失函数来衡量参数的好坏
	- 由于平方误差函数中的二次方项， 估计值 $𝑦̂ ^{(𝑖)}$ 和观测值 $𝑦^{(𝑖)}$ 之间较大的差异将导致更大的损失。 
	- 为了度量模型在整个数据集上的质量，我们需计算在训练集 𝑛  个样本上的损失均值（也等价于求和）
	- 应用前文提过的平方损失有公式：
		- ​ $l(X, y,\omega, b) = \frac{1}{2n}\sum\limits_{i = 1}^n(y_i -<𝐱_i, w> - b)^2 = \frac{1}{2n} ||y - X\omega - b||^2$
- **最小化损失来学习参数**
	- 训练参数的目的就是使损失函数的值尽可能小（这意味着预估值和真实值更接近）。
	- 最后求得的参数值可表示为：
	- $\omega^*, b^* = \mathop{argmin}\limits_{\omega,b} \enspace \mathcal{l}(X, y, \omega, b)$

#### 显示解 analytical solution
-  线性回归有显示解，即可以直接矩阵数学运算，得到参数 $w$ 和 $b$ 的最优解
-  而不是用梯度下降，牛顿法等参数优化方式一点点逼近最优解。

##### 推导过程
-  为了方便矩阵表示和计算，将偏差加入权重
	- $X\gets[X,1], \omega \gets \begin{bmatrix} \omega \\ b \end{bmatrix}$
-  $l(X, y,\omega) = \frac{1}{2n}||y - Xw||^2$ 
-  $\frac{\partial}{\partial\omega}l(X, y,\omega)= \frac{1}{n} (y - X\omega)^TX$

-  损失函数是凸函数，最优解满足导数为 0，可解出显示解
	-  令$\frac{\partial}{\partial\omega} l(X,y,\omega)=0$
	-  有$\frac{1}{n}(y - X\omega)^TX = 0$
	-  解得 $\omega^* = (X^TX)^{-1}X^Ty$

#### 小结

-  线性回归是对 $n$ 维输入的加权和，外加偏差
-  使用 *平方损失* 来衡量预测值和真实值之间的误差
-  *线性回归有显示解*
-  线性回归可以看作单层神经网络

### 基础优化算法

#### 梯度下降 gradient descent
-  当模型没有显示解的时候，应用梯度下降法逼近最优解。
-  这种方法几乎可以优化所有深度学习模型
-  它通过不断地在损失函数递减的方向上更新参数来降低误差

-  梯度下降法的具体步骤：
	- 挑选一个随机初始值 $\omega_0$
	- 重复迭代参数 $t =1, 2, 3$
		-  迭代公式为：$\omega_t=\omega_{t-1}-\lambda\frac{\partial l}{\partial\omega_{t-1} }$
		-  沿梯度方向将增加损失函数值
		-  学习率：步长的超参数
-  $-\frac{\partial l}{\partial\omega_{t-1}}$ 为函数值下降最快的方向

-  选择学习率
	-  学习率 $\lambda$ 为学习步长，代表了沿负梯度方向走了多远
		-  这是超参数（人为指定的的值，不是训练得到的）
	-  学习率不能太大，也不能太小，需要选取适当。

#### 小批量随机梯度下降 minibatch stochastic gradient descent

-  在整个训练集上算梯度太贵了
-  在实际应用中，很少直接应用梯度下降法，这是因为每次更新都需要计算训练集上所有的样本，耗费时间太长
-  一个深度神经网络模型，迭代一次可能需要数分钟甚至数小时

-  为了减少运算代价，我们可以==随机采样== $b$ 个样本 $i_1, i_2, ..., i_b$ 来近似损失，损失函数为：
	-  $\frac{1}{b}\sum\limits_{i\in I_b} l(x_i, y_i, \omega)$ ,
		-  其中 b  -  批量大小 batch size ，也是超参数

- 算法的步骤如下： 
	  1. 初始化模型参数的值，如随机初始化
	  2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。
-  对于平方损失和仿射变换，我们可以明确地写成如下形式
	-  $\textbf{w} ←  \textbf{w} - \frac{\eta}{|\mathcal{B}|} \sum\limits_{𝑖∈\mathcal{B}}\partial_wl^{(i)}(\textbf{w}, b) = \textbf{w} - \frac{\eta}{|\mathcal{B}|} \sum\limits_{𝑖∈\mathcal{B}}\textbf{x}^{(i)}(\textbf{w}^T\textbf{x}^{(i)} + b - y^{(i)})$ 
	- $b ←  \textbf{w} - \frac{\eta}{|\mathcal{B}|} \sum\limits_{𝑖∈\mathcal{B}}\partial_bl^{(i)}(\textbf{w}, b) = b - \frac{\eta}{|\mathcal{B}|} \sum\limits_{𝑖∈\mathcal{B}}(\textbf{w}^T\textbf{x}^{(i)} + b - y^{(i)})$ 
- _调参 hyperparameter tuning_ 是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的_验证数据集_（validation dataset）上评估得到的。
-  选择批量大小
	-  $b$ 也不能太大：内存消耗增加；浪费计算资源，一个极端的情况是可能会重复选取很多差不多的样本，浪费计算资源
	-  $b$ 也不能太小：每次计算量太小，很难以并行，不能最大限度利用GPU资源

-  在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为 $\hat{𝐰},\hat{𝑏}$
-  但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

- 线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。
-  深度学习实践者很少会去花费大力气寻找这样一组参数，使得在 _训练集_ 上的损失达到最小
-  事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为 _泛化 generalization_

## 线性回归的从零实现

[08 线性回归从零实现.ipynb](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/08%20%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0.ipynb)

#### 总结

-  本节介绍了线性回归。线性模型的输入、输出、权重、偏置及噪声
-  介绍了一种模型质量的度量方式：损失函数，有很多种类损失函数，在线性回归我们采用均方误差。把问题转换为求最小化损失函数的优化问题
-  介绍了一种能够更新模型以提高模型预测质量的方法，基础的优化算法：梯度下降法和小批量梯度随机下降法及其算法步骤
-  介绍了线性回归的显示解及推导过程，它也是本次课程唯一有显示解的模型
-  介绍了 用代码实现线性回归的从零开始
-  介绍了 用代码实现线性回归

## 08 线性回归 Q&A

- `为什么使用平方损失而不是绝对差值？`
> 其实差别不大，最开始使用平方损失是因为它可导，现在其实都可以使用。

- `损失为什么要求平均？`
> 本质上没有关系，但是如果不求平均，梯度的数值会比较大，这时需要学习率除以 n。
> 如果不除以 n，可能会随着样本数量的增大而让梯度变得很大。

- `不管是梯度下降还是随机梯度下降，怎么找到合适的学习率？`
> 选择对学习率不敏感的优化方法，比如 Adam
> 合理参数初始化

- `物理实验中经常使用n-1代替n求误差请问这里求误差也能用n-1代替n吗？`
> 都可以，不会有影响的

- `batch_size是否会影响最终影响模型结果，batch_size过小是否可能导致最终累积的梯度计算不准确`
> 之后会讲到丢弃法，batch_size越小收敛越好
> 随机梯度下降理论上带来了噪音，噪音对神经网络是件好事情，因为现在深度神经网络过于复杂，适当的噪音保证不会走偏

- `训练过程中，过拟合和欠拟合情况下，学习率和batch_size应该如何调整？`
> 理论上 学习率 和 batch_size 对最后的拟合结果不会有影响

- `随机梯度下降中的随机是指批量大小是随机的吗？`
> 不是，批量大小是一样的，是128

- `深度学习上，设置损失函数的时候，需要考虑正则吗？`
> 会考虑，但是和损失函数是分开的，深度学习中正则没有太大的用处，有很多其他的技术可以有正则的效果。

- `如果样本大小不是批量数的整数倍，需要随机剔除多余的样本吗？`
> 就取多余的样本作为一个批次
> 直接丢弃
> 从下一个epoch里面补少的样本
