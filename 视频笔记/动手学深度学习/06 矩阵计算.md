## 06 矩阵计算

- 在深度学习中，我们“训练”模型，不断更新它们，使它们在看到越来越多的数据时变得越来越好。
- 通常情况下，变得更好意味着最小化一个 _损失函数_（loss function）， 即一个衡量“模型有多糟糕”这个问题的分数。 
- 最终，我们真正关心的是生成一个模型，它能够在从未见过的数据上表现良好。 但“训练”模型只能将模型与我们实际能看到的数据相拟合。 
- 因此，我们可以将拟合模型的任务分解为两个关键问题：
	-   _优化_（optimization）：用模型拟合观测数据的过程；
	-   _泛化_（generalization）：数学原理和实践者的智慧，能够指导我们生成出有效性超出用于训练的数据集本身的模型。

### 导数的概念及几何意义
- 这是几乎所有深度学习优化算法的关键步骤。 在深度学习中，通常选择对于模型参数可微的损失函数。 
- 简而言之，对于每个参数， 如果把这个参数 _增加_ 或 _减少_ 一个无穷小的量，可以知道损失会以多快的速度增加或减少，


#### 标量导数
+ 导数是切线的斜率

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317160456.png)

- `a 不是 x 的函数`

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317160605.png)


#### 亚导数

+ 将导数拓展到不可微的函数
+ 在不可导的点的导数可以用一个范围内的数表示

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317161114.png)

#### 梯度

- 将导数拓展到向量

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317161310.png)

- 当 y 是标量，x 是列向量，结果是行向量
-  梯度 指向值变化最大的方向

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317161448.png)

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317162202.png)

#### 当 y 是 列向量，x 是标量，结果是跟 y 一样长的列向量

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317162301.png)

#### 都是向量，求导，结果是个矩阵

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317162606.png)

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317162721.png)

#### 拓展到矩阵

- **将求导推广到矩阵，由于矩阵可以看作由多个向量所组成，因此对矩阵的求导可以看作先对每个向量进行求导，然后再增加一个维度存放求导结果。**
- 例如当 F 为矩阵，input 为矩阵时，F 中的每个元素 f (标量）求导后均为一个矩阵（按照课上的展开方式），因此每个 **f**（包含多个 f（标量））求导后为存放多个矩阵的三维形状，再由于矩阵 F 由多个 **f** 组成，因此 F 求导后为存放多个 **f** 求导结果的四维形状。 **对于不同f和input求导后的维度情况总结如下图所示**

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317162950.png)

### 增: 函数与标量，向量，矩阵

- 该部分结合课程视频和参考文章进行总结
- 参考了知乎文章：[矩阵求导的本质与分子布局、分母布局的本质（矩阵求导——本质篇） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/263777564)
