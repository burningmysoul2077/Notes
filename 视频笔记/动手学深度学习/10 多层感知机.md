
# 感知机 Perceptron

## 定义

-  从现在的观点来看，感知机实际上就是神经网络中的一个神经单元

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322101942.png)

-  感知机能解决二分类问题 (-1 或 1)，但与线性回归和softmax回归有所区别：
	- 线性回归与softmax回归的输出均为实数，感知机输出一个离散的类
	- softmax回归的输出同时还满足概率公理，有n个类输出n个元素，感知机只能输出一个

## 训练感知机

	> initialize w = 0 and b = 0
	> repeat
	> 	# 此处表达式小于 0 代表预测结果错误
	> 	if y_i[<w,x_i>+b] <= 0 then
	> 		w <- w + y_ix_i and b <- b + yi
	> 	end if
	> until all classified correctly

-  可以看出这等价于使用如下损失函数的随机梯度下降（batch_size=1）:
	- $\ell(y, \textbf{x}, \textbf{w}) = max(0, -y<\textbf{w}, \textbf{x}>) = max(0, -y\textbf{w}^T\textbf{x})$
	- 分类正确的话，是会 >0 的，这样不会更新；

-  当预测错误时，偏导数为 $\frac{\partial \ell}{\partial \textbf{w}}=-y\cdot \textbf{x}$
- 注：此处为了方便计算，将偏置项 $b$ 归入 $w$ 中的最后一维，并在特征 $x$ 中相应的最后一维加入常数 1

## 收敛定理

- 假设数据在一个半径为 $r$ 的区域里面
	- 比如一个 半径为 $r$ 的圆
- 假设有一个余量 $\sigma$ 使得存在一个分截面 $\|\textbf w\|^2+b^2 \leq 1$，使得分截面能够对所有分类正确，$y(\textbf x^T\textbf w + b)\geq \rho$， 即$y(\textbf x^T\textbf w + b)$ 不仅大于0，而且还有一个余量
- 则感知机保证在 $\frac{r^2+1}{\rho ^2}$ 步内收敛，找到最优解
	- $r$  -  数据的大小
	- $\rho$  -  看数据质量，是不是相隔太近/太远

- [收敛性的证明](https://zhuanlan.zhihu.com/p/46762820)

## 线性模型的缺陷

- 在前面的课程中我们学习了softmax回归，线性回归，他们有将输入向量与一个权重向量做内积再与一个偏置相加得到一个值的过程：

$$
O =W^TX+b
$$

- 这个过程被称为仿射变换，它是一个带有偏置项的线性变换，它最终产生的模型被称为线性模型，线性模型的特点是只能以线性的方式对特征空间进行划分：

- 然而，这种线性划分依赖于线性假设，是非常不可靠的
- 线性假设意味着单调假设，这是不可靠的：
	- 对于人体的体温与健康情况的建模，人体在37℃时最为健康，过小过大均有风险，然而这不是单调的
- 线性假设意味着特征与预测存在线性相关性，这也是不可靠的：
	- 如果预测一个人偿还债务的可能性，那这个人的资产从0万元增至5万元和从100万元增至105万元对应的偿还债务的可能性的增幅肯定是不相等的，也就是不线性相关的
- 线性模型的评估标准是有位置依赖性的，这是不可靠的：
	- 如果需要判断图片中的动物是猫还是狗，对于图片中一个像素的权重的改变永远是不可靠的，因为如果将图片翻转，它的类别不会改变，但是线性模型不具备这种性质，像素的权重将会失效

### XOR 问题

- Minsky&Papert, 1969
- 感知机不能拟合 XOR 函数，它只能产生线性分割面

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322160038.png)

-  如上图，感知机没法分割 一三象限和二四象限，总有错的

## 小结

-  感知机是一个二分类模型，是最早的 AI模型之一
-  它的求解算法等价于使用批量大小为 1 的梯度下降
-  它不能拟合 XOR 函数，导致的第一次 AI 寒冬

# 多层感知机 MLP

## 学习XOR - XOR问题的多层次解决

-  仍以XOR问题为例，XOR问题的一个解决思路是分类两次
	-  先按 x 轴分类为 + 和 -，再按 y 轴分类为 + 和 -，最后将两个分类结果相乘
	- + 即为一三象限，- 即为二四象限：

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322161327.png)

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322161343.png)


-  这实际上将信息进行了多层次的传递：

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322161423.png)


- 其中
	- 蓝色  -  按 X 进行的 +-分类
	- 黄色  -  按 Y 进行的 +-分类
	- 灰色  -  将二者相乘
- 这就实现了用多层次的线性模型对非线性进行预测

## 单隐藏层

-  在网络中加入一个或多个隐藏层来克服线性模型的限制
-  最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。
-  我们可以把前 𝐿−1 层看作表示，把最后一层看作线性预测器。
-  这种架构/模型通常称为 **多层感知机 multilayer perceptron MLP**，如下图所示
	-  第一层 $x_1, x_2, x_3, x_4$ 称为输入
	-  之后的一层称为隐藏层，由5个感知机构成，他们均以前一层的信息作为输入
	-  最后是输出层，以前一层隐藏层的结果作为输入。
	-  除了输入的信息和最后一层的感知机以外，其余的层均称为隐藏层
	-  隐藏层的设置大小是重要的超参数，因为输入的大小无法更改，输出的大小是由类别决定，唯一可决定的就是隐藏层

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230322161655.png)

-  这个多层感知机有 4 个输入，3 个输出，其隐藏层包含 5 个隐藏单元。
-  输入层不涉及任何计算，因此使用此网络产生输出只需要实现隐藏层和输出层的计算。 因此，这个多层感知机中的层数为2。 
-  注意，这两个层都是全连接的。 每个输入都会影响隐藏层中的每个神经元， 而隐藏层中的每个神经元又会影响输出层中的每个神经元。
-  然而， 具有全连接层的多层感知机的参数开销可能会高得令人望而却步。 即使在不改变输入或输出大小的情况下， 可能在参数节约和模型有效性之间进行权衡 

### 单隐藏层 - 单分类

-  输入  $\textbf x \in \mathbb{R}^n$
-  隐藏层 $\textbf W_1 \in \mathbb{R}^{(m\times n)}, \textbf b_1 \in \mathbb{R}^m$  ，假设隐藏层大小是 $m$，有相应大小的偏移
	- $h = \sigma(\textbf W_1 \textbf x + \textbf b_1)$  ，作为输入进入输出层，$h$ 是一个长为 $m$ 的向量， $\sigma$ 是按元素的激活函数
-  输出层 $\textbf w_2 \in \mathbb{R}^{m}, b_2 \in \mathbb{R}^m$ ，单分类就是输出一个向量
	-  $o = \textbf w_2^T \textbf h + b_2$，得到一个标量

- 为什么需要非线性激活函数
	- 假设 $\sigma(x) = x$，将 $h$ 代入输出，还是得到一个线性模型，所以反过来说，$\sigma$ 不能是线性的

## 多类分类

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230323161104.png)

- 想要得到置信度，就放入到 softmax
- 就是在 softmax 种加入了一个隐藏层，就变成了多层感知机

-  输入  $\textbf x \in \mathbb{R}^n$
-  隐藏层 $\textbf W_1 \in \mathbb{R}^{m\times n}, \textbf b_1 \in \mathbb{R}^m$  ，假设隐藏层大小是 $m$，有相应大小的偏移
	- $h = \sigma(\textbf W_1 \textbf x + \textbf b_1)$  ，作为输入进入输出层，$h$ 是一个长为 $m$ 的向量， $\sigma$ 是按元素的激活函数
-  输出层 $\textbf w_2 \in \mathbb{R}^{m \times k}, b_2 \in \mathbb{R}^k$ ，多分类，输出要有 $k$ 个单元，就是输出层 $\textbf w_2$ 变成了 $m \times k$ 矩阵， $\textbf b_2$ 变成了 $k$ 维
	-  $\textbf o = \textbf w_2^T \textbf h + b_2$
	-  $\textbf y = softmax(\textbf o)$

### 多隐藏层

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230323161919.png)

-  $h = \sigma(\textbf W_1 \textbf x + \textbf b_1)$ 
-  $h = \sigma(\textbf W_2 \textbf h_1 + \textbf b_2)$ 
-  $h = \sigma(\textbf W_3 \textbf h_2 + \textbf b_3)$ 
-  $\textbf o = \textbf w_4^T \textbf h_3 + b_4$

-  超参数
	-  隐藏层数
	-  每层隐藏层的大小
- 一般会有一些经验，比如输入数据很大，128、256、512维，可以选择单隐藏层 $m_1$ 大一些；或者可以把模型做得深一点，相比较单隐藏层的 $m_1$ 小，后基层依次减小

## 通用近似定理

-   多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用， 这些神经元依赖于每个输入的值。 
-   我们可以很容易地设计隐藏节点来执行任意计算。 
	- 例如，在一对输入上进行基本逻辑操作，多层感知机是通用近似器。 即使是网络只有一个隐藏层，给定足够的神经元和正确的权重， 我们可以对任意函数建模，尽管实际中学习该函数是很困难的。 

-  神经网络有点像C语言。 C语言和任何其他现代编程语言一样，能够表达任何可计算的程序。 但实际上，想出一个符合规范的程序才是最困难的部分。

-  而且，虽然一个单隐层网络能学习任何函数， 但并不意味着应该尝试使用单隐藏层网络来解决所有问题。
- 事实上，通过使用更深（而不是更广）的网络，可以更容易地逼近许多函数。


# 激活函数 Activation Function

-  但上一节单分类中，证明了仅仅有线性变换是不够的，如果我们简单的将多个线性变换按层次叠加，由于线性变换的结果仍为线性变换，所以最终的结果等价于线性变换，与单个感知机并无区别，反而加大了模型，浪费了资源
-  为了防止这个问题，需要对每个单元（感知机）的输出通过激活函数进行处理再交由下一层的感知机进行运算，这些激活函数就是解决非线性问题的关键。

- **激活函数 activation function**  通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算。
	- 大多数激活函数都是非线性的

- 主要的激活函数有：

## ReLU函数

-  最受欢迎的激活函数是 *修正线性单元 Rectified linear unit，ReLU* 
-  因为它实现简单，同时在各种预测任务中表现良好。
-  **ReLU提供了一种非常简单的非线性变换**。给定元素 $x$，ReLU 函数被定义为该元素与 $0$ 的最大值：

$$
\operatorname{ReLU}(x) = \max(x, 0)
$$

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230323160412.png)

-  ReLU函数通过将相应的活性值设为 0，仅保留正元素并丢弃所有负元素。
-  正如从图中所看到，激活函数是分段线性的。

-  使用ReLU的原因是，它求导表现得特别好：
	- 要么让参数消失
	- 要么让参数通过
- 这使得优化表现的更好，并且 ReLU 减轻了困扰以往神经网络的梯度消失问题

## Sigmoid函数

- **对于一个定义域在$\mathbb{R}$中的输入，*sigmoid函数* 将输入变换为区间(0,1)上的输出**
- 因此，sigmoid 通常称为 *挤压函数 squashing function*
	- 它将范围$（-\infty, \infty）$中的任意输入压缩到区间（0,1）中的某个值：

$$
\operatorname{sigmoid}(x) = \frac{1}{1 + e^{-x}}.
$$

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230323155306.png)

-  在基于梯度的学习时，sigmoid 函数是一个自然的选择，因为它是一个平滑的、可微的阈值单元近似。
-  当我们想要将输出视作二元分类问题的概率时，sigmoid 仍然被广泛用作输出单元上的激活函数（可以将sigmoid视为softmax的特例）
-  然而，sigmoid在隐藏层中已经较少使用，它在大部分时候被更简单、更容易训练的 *ReLU* 所取代

##  Tanh函数

- 与sigmoid函数类似，**tanh(双曲正切)函数也能将其输入压缩转换到区间(-1,1)上**
- Tanh函数的公式如下：

$$
\operatorname{tanh}(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
$$

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230323155610.png)

-  当输入在 0 附近时，tanh 函数接近线性变换。 
-  函数的形状类似于 sigmoid 函数， 不同的是 tanh 函数关于坐标系原点中心对称

# 多层感知机从零实现

[10 多层感知机从零实现.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0.ipynb) 

# 多层感知机简洁实现

[10 多层感知机的简洁实现.ipynb](https://github.com/burningmysoul2077/Notes/blob/main/%E8%A7%86%E9%A2%91%E7%AC%94%E8%AE%B0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10%20%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0.ipynb) 

# 感知机 Q&A

-  `x>0, 输出为什么是 1， 通过设计 w 和 b吗？还是通过训练？`
>  不是一个x。是通过学习w和b

- `请问老师神经网络中的一层网络到底是指什么？是一层神经元经过线性变换后称为一层网络，还是一层神经元经过线性变化加非线性变换后称为一层`
>  一般来讲，一层包含 激活函数，带权重的一层。输入层不算一层。

- `老师，数据的区域r怎么测量或者统计？rho怎么设定？实际中我们确实想找到数据分布的区域，可以找到吗`
>  从统计上，rho是定义出来的，收敛定理是统计上的。但是机器学习，可以认为是统计的计算机分支，这里面不清楚rho怎么算。

- `请问老师神正式因为感知机只能产生XOR函数，所以当时人们才会使用SVM吗`
>  SVM是上世纪90年代出现，感知机早在60年代。可以说SVM替代了感知机，因为多层感知机需要人为选择超参数，而kernel SVM对超参数不敏感；而且SVM优化更简单，数学更好。但其实这两者实际效果上差不多

- `想问一下 XOR 函数有什么应用呢`
>  无

- `请问老师为什么神经网络要增加隐藏层的层数，而不是神经元的个数？不是有神经网络万有近似性质吗`
>  隐藏层有两个选择，更大或是更深，但是这两种模型复杂度是可以认为相等的。但是更大的模型不好训练，更容易过拟合，叫做 浅度学习。更深的叫做深度学习。

- `神经元和卷积核什么关系`
>  下次讲

- `老师，relu为什么管用，它在大于0的部分也就只是线性变换。为什么能促进学习呢，激活的本质是要做什么事？不是引入非线性？`
>  relu不是线性函数，线性函数一定是 f(x) = ax +b，一定是一根线。激活函数的本质就是引入非线性。

- `不同任务下的激活函数是不是都不一样？也是通过实验来确认吗？`
>  其实都差不多，尽量用relu

>- `老师，模型的深度和宽度哪个更影响性能，有理论指导吗？就是加深哪个更有效。怎么根据输入空间，选择最优的深度或者宽度？`
>  理论上没区别，实际上，深的更好。但是没有最优的。个人经验，举例做mlp 128维到2，一开始肯定从简单的开始，没有隐藏层；然后第二次试验加上隐藏层16个单元，慢慢加单元；假设效果不好，第三次加两个隐藏层

>- `请问为啥多层感知机公式哪里后面的W2、W3没有转置`
>   看定义

- `请问老师，怎么让感知机拟合所有函数，同时又保持动态性能？就是动态泛化，能打造动态神经网络吗？要不然训练完，参数永远是死的？`
>  是的，训练完，参数是固定的

# 总结

- 本节介绍了感知机、多层感知机、激活函数等。
- 介绍了感知机及其训练过程，收敛定理，XOR问题是线性模型的缺陷
- 介绍了解决XOR问题的多层感知机MLP，单隐藏层单分类
- 介绍了常用的激活函数：Sigmoid，Tanh，ReLU
- 介绍了使用softmax进行多分类、多隐藏层，通用近似订立
- 介绍了代码，多层感知机从零实现
- 介绍了代码，多层感知机简洁实现
