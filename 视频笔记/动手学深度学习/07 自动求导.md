

### 向量链式法则

- 标量链式法则

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191013.png)

- 拓展到向量
- 需要注意维数的变化
> 
>  下图三种情况分别对应：
>  1.  y为标量，x为向量
>  2.  y为标量，x为矩阵
>  3.  y、x为矩阵
    
![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191137.png)


- 例1 标量对向量求导
> 这里应该是用分子布局，所以是 X 转置

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191341.png)

- 例2 涉及到矩阵的情况
> X 是 mxn 的矩阵, w 为 n 维向量，y 为 m 维向量； z对 Xw-y 做 L2 norm, 为标量； 过程与 例一 大体一致；

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191715.png)


- **由于在神经网络动辄几百层，手动进行链式求导是很困难的，因此我们需要借助自动求导**


### 自动求导

-   含义：计算一个函数在指定值上的导数
-   自动求导有别于
    -   符号求导
    
        ![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191828.png)
    -   数值求导
        
        ![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317191856.png)

- 为了更好地理解自动求导，下面引入计算图的概念

#### 计算图

-   将代码分解成操作子
-   将计算表示成一个 **无环图**
    
> 下图自底向上其实就类似于链式求导过程

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317192002.png)

​
##### 计算图有两种构造方式

-  显示构造
> 可以理解为先定义公式再代值
> 
> Tensorflow/Theano/MXNet

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317192617.png)


-  隐式构造
> 系统将所有的计算记录下来
> 
> Pytorch/MXNet

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317192745.png)


#### 自动求导的两种模式

-  正向累积

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317192855.png)

-  反向累积（反向传递 back propagation）

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317192935.png)


#### 反向累积计算过程

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317193056.png)

- 反向累计总结
	- 构造计算图
	- 反向累积的正向过程：执行图，自底向上，需要存储中间结果
	- 反向累积的反向过程：自顶向下，可以去除不需要的枝（图中的 x 应为 w）

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230317193413.png)

#### 复杂度比较

-   反向累积
    -   时间复杂度：O(n), n 是操作子数
        -   通常正向和反向的代价类似
    -   空间复杂度：O(n)
        -   存储正向过程所有的中间结果

-   正向累积
    > 每次计算一个变量的梯度时都需要将所有节点扫一遍
    -   时间复杂度：O(n)
    -   空间复杂度：O(1)

### 自动求导实现

- 深度学习框架通过自动计算导数，即_自动微分_（automatic differentiation）来加快求导。 实际中，根据设计好的模型，系统会构建一个_计算图_（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，_反向传播_（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

#### 一个简单的例子

- 对 $y = 2𝐱^T𝐱$ 关于列向量 𝐱 求导

```python
import torch

>>> x = torch.arange(4.0)
>>> x

tensor([0., 1., 2., 3.])
```

- **在计算 𝑦 关于 𝐱 的梯度之前，需要一个地方来存储梯度**
- 重要的是，我们不会在每次对一个参数求导时都分配新的内存。
- 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。
- 注意，一个标量函数关于向量 𝐱 的梯度是向量，并且与 𝐱 具有相同的形状

```python
>>> x.requires_grad_(True)  # 等价于x = torch.arange(4.0,requires_grad=True)
>>> x.grad  # 默认值是None
```

- **现在计算 𝑦 

```python
>>> y = 2 * torch.dot(x, x)
>>> y

tensor(28., grad_fn=<MulBackward0>)
```

- PyTorch 隐式地构造计算图，grad_fn 用于记录梯度计算

- **通过调用反向传播函数来自动计算 𝑦 关于 𝐱 每个分量的梯度**

```python
>>> y.backward()
>>> x.grad

tensor([0., 4., 8., 12.])
```

```python
>>> x.grad == 4 * x  # 验证
>>> x.grad

tensor([True, True, True, True])
```

- **现在计算 `x` 的另一个函数**

```python
# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
>>> x.grad.zero_()
>>>
>>> y = x.sum()
>>> y.backward()
>>> x.grad

tensor([1., 1., 1., 1.])
```

- x.grad.zero_() 如果没有这一步结果就会加累上之前的梯度值，变为 [1,5,9,13]

#### 非标量变量的反向传播
- 当 `y` 不是标量时，向量 `y` 关于向量 `x` 的导数的最自然解释是一个矩阵。
- 对于高阶和高维的 `y` 和 `x`，求导的结果可以是一个高阶张量。

- 然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括 *深度学习中*）， 但当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里：
- **我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**

```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
>>> x.grad.zero_()
>>> y = x * x # 哈达玛积，对应元素相乘

# 等价于y.backward(torch.ones(len(x)))
>>> y.sum().backward()
>>> x.grad

tensor([0., 2., 4., 6.])
```

#### 分离计算
- **将某些计算移动到记录的计算图之外**

- 例如，假设 `y` 是作为 `x` 的函数计算的，而`z`则是作为 `y` 和 `x` 的函数计算的。 
- 想象一下，想计算 `z` 关于 `x` 的梯度，但由于某种原因，希望将 `y` 视为一个常数， 并且只考虑到 `x` 在 `y` 被计算后发挥的作用。

- 这里可以分离 `y` 来返回一个新变量 `u`，该变量与 `y` 具有相同的值， 但丢弃计算图中如何计算 `y` 的任何信息。
- 换句话说，梯度不会向后流经 `u` 到 `x`。
- 因此，下面的反向传播函数计算 `z=u*x` 关于 `x` 的偏导数，同时将 `u` 作为常数处理， 而不是 `z=x*x*x` 关于 `x` 的偏导数

```python
>>> x.grad.zero_()
>>> y = x * x
>>> u = y.detach() # 把y当作常数
>>> z = u * x

>>> z.sum().backward()
>>> x.grad == u

tensor([True, True, True, True])
```

- 由于记录了 `y` 的计算结果，可以随后在 `y` 上调用反向传播， 得到 `y=x*x` 关于的 `x` 的导数，即 `2*x`。

```python
>>> x.grad.zero_()
>>> y.sum().backward()
>>> x.grad == 2 * x

tensor([True, True, True, True])
```

#### Python控制流的梯度计算
- 使用自动微分的一个好处是： **即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），仍然可以计算得到的变量的梯度** 
- **这也是隐式构造的优势，因为它会存储梯度计算的计算图，再次计算时执行反向过程就可以**

- 在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。

```python
>>> def f(a):
>>>     b = a * 2
>>>     while b.norm()<1000:
>>>         b = b * 2
>>>     if b.sum() > 0:
>>>         c = b
>>>     else:
>>>         c = 100 * b
>>>     return c
>>> 
>>> a = torch.randn(size=(), requires_grad=True)
>>> d = f(a)
>>> d.backward()
```


## 07 自动求导 Q&A

- **`ppt上隐式构造和显式构造看起来为啥差不多？`**
> 显式和隐式的差别其实就是数学上求梯度和 python 求梯度计算上的差别，不用深究
> 显式构造就是我们数学上正常求导数的求法，先把所有求导的表达式选出来再代值

- **`需要正向和反向都算一遍吗？`**
> 在神经网络求梯度时，需要正向先算一遍，自动求导时只进行反向就可以，因为正向的结果已经存储

- **`为什么PyTorch会默认累积梯度`**
> 便于计算大批量；方便进一步设计,比如在不同模型之间 share

- **`为什么深度学习中一般对标量求导而不是对矩阵或向量求导`**
> loss一般都是标量

- 多个 loss 分别反向的时候是不是需要累计梯度`**
> 是的，如果神经网络有多个损失函数，是需要累积梯度的

- **`为什么获取.grad前需要backward`**
> 必须这样，相当于告诉程序需要计算梯度，因为计算梯度的代价很大，默认不计算

- **`pytorch或mxnet框架设计上可以实现矢量的求导吗`**
> 可以
