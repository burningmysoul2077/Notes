* [前言](#前言)
* [前向传播](#前向传播)
  * [举例：单隐藏层神经网络的机制](#举例单隐藏层神经网络的机制)
* [前向传播计算图](#前向传播计算图)
* [反向传播](#反向传播)
* [训练神经网络](#训练神经网络)
* [总结](#总结)



# 前言

> 我们已经学习了如何用小批量随机梯度下降训练模型。 
> 然而当实现该算法时，我们只考虑了通过 前向传播 forward propagation所涉及的计算。 在计算梯度时，我们只调用了深度学习框架提供的反向传播函数，而不知其所以然。
> 梯度的自动计算（自动微分）大大简化了深度学习算法的实现。 在自动微分之前，即使是对复杂模型的微小调整也需要手工重新计算复杂的导数， 学术论文也不得不分配大量页面来推导更新规则。
>  本节将通过一些基本的数学和计算图， 深入探讨 反向传播 的细节。 

------

# 前向传播

- _前向传播 forward propagation / forward pass_ ：按顺序计算和存储神经网络中每层的结果
	- 顺序：从输入层到输出层

## 举例：单隐藏层神经网络的机制

-  输入样本是               $𝐱∈ℝ^𝑑$
	- 为了简单起见，这样假设，并且隐藏层不包括偏置项。 

-  中间变量是：         $𝐳=𝐖^{(1)}𝐱$
	-  其中 $𝐖^{(1)}∈ℝ^{ℎ×𝑑}$  是隐藏层的权重参数

-  通过激活函数 𝜙 后   𝐡=𝜙(𝐳) 
	-  中间变量 $𝐳∈ℝ^ℎ$ 
	-  得到长度为 ℎ 的隐藏激活向量   
	-  隐藏变量 𝐡 也是一个中间变量

-  得到输出层变量：  $𝐨=𝐖^{(2)}𝐡$
	-  这里假设输出层的参数只有权重 $𝐖^{(2)} ∈ ℝ^{𝑞×ℎ}$ 
	-  它是一个长度为 𝑞 的向量

- 假设损失函数为 𝑙 ，样本标签为 𝑦 ，我们可以计算单个数据样本的损失项，
	-  𝐿 = 𝑙(𝐨, 𝑦) 

-  根据 $𝐿_2$ 正则化的定义，给定超参数 𝜆 ，正则化项为
	-    $𝑠 = \frac{𝜆}{2} (‖𝐖^{(1)}‖^2_𝐹 + ‖𝐖^{(2)}‖^2_𝐹)$
	-  其中矩阵的 Frobenius范数是将矩阵展平为向量后应用的 𝐿2 范数。 

- 最后，模型在给定数据样本上的正则化损失为：𝐽 = 𝐿 + 𝑠 

- 在下面的讨论中，我们将 𝐽 称为 _目标函数 objective function_ 。

# 前向传播计算图

- 绘制 _计算图_ 有助于我们可视化计算中操作符和变量的依赖关系。 

![image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Pasted%20image%2020230327154731.png)

-  这是与上述简单网络相对应的计算图
	-  其中正方形表示变量，圆圈表示操作符。 
	-  左下角表示输入，右上角表示输出。 
	-  注意显示数据流的箭头方向主要是向右和向上的。

# 反向传播

-  _反向传播 backward propagation或backpropagation_  指的是计算神经网络参数梯度的方法。
	-  简言之，该方法根据微积分中的 _链式规则_ ，按相反的顺序从输出层到输入层遍历网络。
	-  该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 
	-  假设我们有函数 $𝖸=𝑓(𝖷)$ 和 $𝖹=𝑔(𝖸)$  
		- 其中 输入 和 输出 $𝖷 , 𝖸 , 𝖹$ 是任意形状的张量。 
		- 利用链式法则，我们可以计算 𝖹 关于 𝖷 的导数
			- $\frac{∂𝖹}{∂𝖷} = \text {prod}(\frac{∂𝖹}{∂𝖸}, \frac{∂𝖸}{∂𝖷})$

-  在这里，我们使用 prod 运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。
-  对于向量，这很简单，它只是矩阵-矩阵乘法。 
-  对于高维张量，我们使用适当的对应项。 
-  运算符 prod 指代了所有的这些符号。

-  回想一下，在计算图中的单隐藏层简单网络的参数是 $𝐖^{(1)} , 𝐖^{(2)}$。 
-  反向传播的目的是计算梯度 $∂𝐽/∂𝐖^{(1)}$ 和 $∂𝐽/∂𝐖^{(2)}$ 。 
-  为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 
-  计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。
	1. 计算目标函数 𝐽 = 𝐿+𝑠 相对于 损失项 𝐿 和 正则项 𝑠 的梯度。
		- $\frac{∂𝐽}{∂𝐿} = 1 \enspace \text{and} \enspace \frac{∂𝐽}{∂𝑠} = 1$ 
	2. 根据链式法则计算目标函数关于 输出层变量 𝐨 的梯度：
		- $\frac{∂𝐽}{∂𝐨} = \text{prod}(\frac{∂𝐽}{∂𝐿}, \frac{∂𝐿}{∂𝐨}) = \frac{∂𝐿}{∂𝐨} ∈ ℝ^𝑞$
	3. 计算正则化项相对于两个参数的梯度：
		- $\frac{∂𝑠}{∂𝐖^{(1)}} = 𝜆𝐖^{(1)} \enspace \text {and} \enspace \frac{∂𝑠}{∂𝐖^{(2)}} = 𝜆𝐖^{(2)}$

-  现在我们可以计算最接近输出层的模型参数的梯度 $∂𝐽/∂𝐖^{(2)} ∈ ℝ^{𝑞×ℎ}$ 。 使用链式法则得出：
	- $\frac{∂𝐽}{∂𝐖^{(2)}} = \text{prod}(\frac{∂𝐽}{∂𝐨}, \frac{∂𝐨}{∂𝐖^{(2)}})  + \text{prod}(\frac{∂𝐽}{∂𝑠}, \frac{∂𝑠}{∂𝐖^{(2)}}) = \frac{∂𝐽}{∂𝐨}𝐡^⊤ + 𝜆𝐖^{(2)}$

-  为了获得关于 $𝐖^{(1)}$ 的梯度，我们需要继续沿着输出层到隐藏层反向传播。 
-  关于隐藏层输出的梯度 $∂𝐽/∂𝐡∈ℝ^ℎ$  由下式给出：
	- $\frac{∂𝐽}{∂𝐡} = \text{prod}(\frac{∂𝐽}{∂𝐨} , \frac{∂𝐨}{∂𝐡}) = 𝐖^{(2)}\frac{∂𝐽}{∂𝐨}$ 

-  由于激活函数 𝜙 是按元素计算的， 计算中间变量 𝐳 的梯度 $∂𝐽/∂𝐳∈ℝ^ℎ$ 需要使用按元素乘法运算符，我们用 ⊙ 表示：
	- $\frac{∂𝐽}{∂𝐳} = \text{prod}(\frac{∂𝐽}{∂𝐡}, \frac{∂𝐡}{∂𝐳}) = \frac{∂𝐽}{∂𝐡} ⊙ 𝜙′(𝐳)$

-  最后，我们可以得到最接近输入层的模型参数的梯度 $∂𝐽/∂𝐖(1)∈ℝ^{ℎ×𝑑}$。 
-  根据链式法则，我们得到：
	- $\frac{∂𝐽}{∂𝐖^{(1)}} = \text{prod}(\frac{∂𝐽}{∂𝐳}, \frac{∂𝐳}{∂𝐖^{(1)}}) + \text{prod}(∂𝐽∂𝑠,∂𝑠∂𝐖(1))= \frac{∂𝐽}{∂𝐳}𝐱^⊤ + 𝜆𝐖^{(1)}$

# 训练神经网络

-  在训练神经网络时，**前向传播和反向传播** 相互依赖。
-  对于前向传播，我们沿着依赖的方向遍历计算图并计算其路径上的所有变量。 然后将这些用于反向传播，其中计算顺序与计算图的相反。

-  以上述简单网络为例：
	-  一方面，在前向传播期间计算正则项 $s$ 取决于模型参数 $𝐖^{(1)}$ 和 $𝐖^{(2)}$ 的当前值。 它们是由优化算法根据最近迭代的反向传播给出的。 
	- 另一方面，反向传播期间参数 的梯度计算， 取决于由前向传播给出的隐藏变量 𝐡 的当前值。

-  因此，在训练神经网络时，在初始化模型参数后， 我们交替使用前向传播和反向传播，利用反向传播给出的梯度来更新模型参数。

-  注意，反向传播重复利用前向传播中存储的中间值，以避免重复计算。 
-  带来的影响之一是我们需要保留中间值，直到反向传播完成。 
-  这也是训练比单纯的预测需要更多的内存（显存）的原因之一。 

-  此外，这些中间值的大小与网络层的数量和批量的大小大致成正比。 
-  因此，使用更大的批量来训练更深层次的网络更容易导致 _内存不足 out of memory_ 错误。

------

# 总结

- 本节介绍了前向传播、前向传播计算图、反向传播和如何训练神经网络
- 介绍了前向传播按顺序从输入层到输出层的计算过程，得到计算函数
- 介绍了前向传播的可视化计算图
- 介绍了反向传播，用来计算神经网络参数梯度。利用前向传播的遍历和所有变量。
- 介绍了训练神经网络时，前向传播和反向传播相互依赖
