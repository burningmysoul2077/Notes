
- 机器学习的可行性，以及问题是否可以使用机器学习来解决。  

### Learning is Impossible
- 一个比较数学化的二分类例子
	- 输入特征 $\mathcal{X} = \{0, 1 \}^3$ 是二进制的、三维的，对应有8种输入
		- 其中训练样本 $\cal D$ 有 5 个
	- 根据训练样本对应的输出 $\mathcal{Y} = \{\circ, \times\}$
	- 假设有 8 个 hypothesis，这 8 个 hypothesis 在 $\cal D$ 上
		- 对 5 个训练样本的分类效果效果都完全正确
		- 但是在另外 3 个测试数据上，不同的 hypothesis 表现有好有坏
	- 在已知数据 $\cal D$ 上，$g ≈ f$
	- 但是在 $\cal D$  以外的未知数据上，$g≈f$ 不一定成立
- 而机器学习目的，恰恰是希望选择的模型能在未知数据上的预测与真实结果是一致的，而不是在已知的数据集 $\cal D$  上寻求最佳效果。
- 这个例子说明想要在 $\cal D$  以外的数据中更接近目标函数似乎是做不到的，只能保证对 $\cal D$  有很好的分类结果
- 这种特性被称为 **没有免费午餐 No Free Lunch 定理**
	- NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法
- 平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等

### Probability to the Rescue
- Idea: 是否有一些工具或者方法能够对未知的目标函数 $f$ 做一些推论，让模型能够变得有用
- 如果有一个装有很多（数量很大数不过来）橙色球和绿色球的罐子，如何推断橙色球的比例 $u$
	- 统计学上的做法是，从罐子中随机取出 $N$ 个球，作为样本
	- 计算这 $N$ 个球中橙色球的比例 $v$
	- 那么就估计出罐子中橙色球的比例约为 $v$

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316101342.png)

#### Possible versus Probable
- 其实这种随机抽取的做法 _不能_ 说明罐子里橙色球的比例一定是 $v$
- 但是从概率的角度来说，样本中的 $v$ 很有可能 $\approx$ 未知的 $u$

#### Hoeffding's Inequality
- 已知 $u$ 是罐子里橙色球的比例，$v$ 是 $N$ 个抽取的样本中橙色球的比例
	- 当 $N$ 足够大的时候，$v$ 接近于 $u$
- 这就是Hoeffding’s inequality：$P[|v - u| > \epsilon] \leq 2\exp(-2\epsilon^2N)$
	- Hoeffding不等式说明：当 $N$ 很大的时候，$v$ 与 $u$ 相差不会很大，差值被限定在 ϵ 之内。
- 结论 $v=u$ 称为 **probably approximately correct PAC**

### Connection to Learning
- 机器学习中 hypothesis $h(x)= f(x)$ 目标函数的可能性，类比于罐子中橙色球的概率问题
- 罐子里的一颗颗弹珠类比于机器学习样本空间的 $x \in \cal X$ ；
	- 橙色的弹珠类比于 $h(x)\neq f(x)$ 
	- 绿色的弹珠类比于 $h(x)= f(x)$ 
- 从罐子中抽取的 $N$ 个球类比于机器学习的训练样本 $\cal D$ ，且这两种抽样的样本与总体样本之间都是独立同分布的
- 所以，如果样本 $N$ 够大，且 $i.i.d$
	- 那从样本中 $h(x)≠f(x)$ 的概率就能推导在抽样样本外的所有样本中 $h(x)≠f(x)$ 的概率

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316102909.png)

#### Added Components
- 映射中最关键的是 抽样中橙球的概率理解为样本数据集 $\cal D$ 上 $h(x)$ 错误的概率，以此推算出在所有数据上 $h(x)\neq f(x)$ 的概率，这也是机器学习能够工作的本质
- 因为两者的错误率都是 PAC

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316103500.png)

- 这里引入两个值 $E_{in}(h)$ 和 $E_{out}(h)$ 
	- $E_{in}(h)$ 表示在抽样样本中，$h(x) \neq y_n$ 的概率
		- known $E_{in}(h) = \frac{1}{N}\sum\limits_{n=1}^N[h(x_n) \neq y_n]$
	- $E_{out}(h)$ 表示实际所有样本中，$h(x)\neq f(x)$ 的概率
		- unknown $E_{out}(h) = \frac{1}{N}\sum\limits_{n=1}^N[h(x_n) \neq y_n]$

#### The Formal Guarantee
- Hoeffding’s inequality 改为：$P[|E_{in}(h) - E_{out}(h)| > \epsilon] \leq 2\exp(-2\epsilon^2N)$
	- $E_{in}(h) = E_{out}(h)$ 也是 PAC 
	- 如果 $E_{in}(h) \approx E_{out}(h)$ and  $E_{in}(h)$ small  =>  $E_{out}(h)$ small 
		- 就是说在该数据分布 $P$ 下，$h$ 与 $f$ 非常接近，机器学习的模型比较准确

#### Verification of One h
- for any fixed $h$，$N$ 很大的时候，$E_{in}(h) = E_{out}(h)$，但是并不意味着 $g≈f$
- 因为 $h$ 是固定的，不能保证 $E_{in}(h)$ 足够小，即使 $E_{in}(h) = E_{out}(h)$，也可能使 $E_{out}(h)$ 偏大

#### The Verification Flow
- 所以，real learning 是通过算法 $\cal A$，选择最好的 $h$，使 $E_{in}(h)$ 足够小，从而保证 $E_{out}(h)$ 很小
- 使用新数据进行测试，验证其错误率

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316105013.png)


### Connection to Real Learning

#### Multiple $h$
- 假设现在有 M 个罐子（即有 M 个 hypothesis ），如果其中某个罐子抽样的球全是绿色，那是不是应该选择这个罐子
- 先来看这样一个例子：150 个人抛硬币，那么其中至少有一个人连续 5 次硬币都是正面朝上的概率是
	- $1 - (\frac{31}{32})^{150} > 99\%$
- 这个概率非常大，但是能否说明 5 次正面朝上的这个硬币具有代表性呢？答案是否定的！
- 并不能说明该硬币单次正面朝上的概率很大，其实都是0.5
- 一样的道理，抽到全是绿色球的时候也不能一定说明那个罐子就全是绿色球
- 当罐子数目很多或者抛硬币的人数很多的时候，可能引发 Bad Sample
	- Bad Sample 就是$E_{in}$ 和 $E_{out}$ 差别很大
- 即选择过多会恶化不好的情形。

#### BAD Sample and BAD Data
- BAD Sample
	- e.g.  $E_{out} = \frac{1}{2}$, but getting all heads ($E_{in} = 0$)
- BAD Dara for One $h$
	-  $E_{out}(h)$ and $E_{in}(h)$ far away
	-  e.g., $E_{out}$ big (far from $f$), but $E_{in}$ small

#### BAD Data for Many $h$
- 根据许多次抽样的到的不同的数据集 $\cal D$
	- Hoeffding’s inequality 保证了大多数的  $\cal D$ 都是比较好的情形（即对于某个h，保证 $E_{in}(h) \approx E_{out}(h)$）
	- 但是也有可能出现Bad Data，这是小概率事件

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316113056.png)

- 不同的数据集 $D_n$，对于不同的 hypothesis，有可能成为 Bad Data
	- 只要 $D_n$ 在某 hypothesis 上是 Bad Data，那么 $D_n$ 就是 Bad Data
	- 只有当 $D_n$ 在所有的 hypothesis 上都是好的数据，才说明 $D_n$ 不是Bad Data，可以自由选择算法 $\cal A$ 进行建模

#### Bound of BAD Data
- 根据 Hoeffding’s inequality，Bad Data 的上界可以表示为连级（union bound）的形式：

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316113339.png)

- 其中，$M$ 是hypothesis的个数，$N$ 是样本 $\cal D$ 的数量，$\epsilon$ 是参数

#### The Statistical Learning Flow
- Union bound表明，当 $M$ 有限，且 $N$ 足够大的时候，Bad Data出现的概率就更低了，即能保证 $\cal D$ 对于所有的 $h$ 都有 $E_{in}(h) \approx E_{out}(h)$，满足 PAC ，算法 $\cal A$ 的选择不受限制
- 满足这种 union bound 的情况，就是选取一个合理的算法 PLA/pocket ，选择使 $E_{in}$ 最小的 $h_m$ 作为 $g$，一般能够保证 $g ≈ f$，即有不错的泛化能力。
- 至此，就证明了机器学习是可行的

![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230316114000.png)

- 但是，如上面的学习流程图右下角所示，如果 M 是无数个，例如之前介绍的 PLA 直线有无数条，是否这些推论是否还成立

## 总结
- 本章介绍了机器学习的可行性
- 介绍了No Free Lunch 定理，说明机器学习无法找到一个 $g$ 能够完全和目标函数 $f$ 一样
- 介绍了统计概率上的假设，介绍了 Hoeffding 不等式、PAC 概念，引入 $E_{in}$ 代表抽样样本中错误概率、$E_{out}$代表实际所有样本中错误，证明对于某个 $h$，当 $N$ 足够大的时候，$E_{in}$ 和 $E_{out}$ 是PAC的
- 介绍了对于一个 $h$ 是可行的，对于若干个 $h$,引入BAD Sample 和 BAD Data 概念
- 介绍了对于若干个 $h$，引入连级，只要有 $h$ 个数 $M$ 是有限的，且 $N$ 足够大，就能保证 $E_{in}(h) \approx E_{out}(h)$，证明机器学习的可行性
