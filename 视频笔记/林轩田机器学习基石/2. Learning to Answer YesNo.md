
- 本节将继续深入探讨机器学习问题，介绍感知机Perceptron模型，并推导课程的第一个机器学习算法：Perceptron Learning Algorithm PLA

### A Simple Hypothesis Set: the Perceptron
- 举例：某银行要根据用户的年龄、性别、年收入等情况来判断是否给该用户发信用卡。
	- 现在有训练样本 $\cal D$，即之前用户的信息和是否发了信用卡。这是一个典型的机器学习问题
	- 要根据 $\cal D$，通过 $\cal A$，在 $\cal H$ 中选择最好的 $h$，得到 $g$，最接近目标函数 $f$，也就是根据先验知识建立是否给用户发信用卡的模型。
	- 用这个模型对以后用户进行判断：发信用卡（+1），不发信用卡（-1）。

#### Hypothesis Set
- 选择什么样的模型，很大程度上会影响机器学习的效果和表现

### 感知机 Perceptron
- For $x = (x_1, x_2, \cdots , x_d)$  -  features of customers 用户个人信息作为特征向量x，总共d个特征
- weight  -  每个特征赋予不同的权重w，表示该特征对输出的影响有多大
- 那所有特征的加权和的值 $\sum_{i=1}^d w_ix_i$ > threshold => +1; <threshold => -1
- $\cal y : \{+1, -1\}$, 0 ignored
- linear formula $h \in \cal H$ :  $h(x) = sign((\sum\limits_{i=1}^dw_ix_i) - threshold)$
- 这就是感知机模型，就是当特征加权和与阈值的差大于或等于0，则输出h(x)=1；当特征加权和与阈值的差小于0，则输出h(x)=-1，目的就是计算出所有权值w和阈值threshold

#### Vector Form of Perceptron Hypothesis
- $h(x) = sign((\sum\limits_{i=1}^dw_ix_i) - threshold)$
- 改写为  $h(x) = sign((\sum\limits_{i=1}^dw_ix_i) - (-threshold) \cdot \enspace (+1))$
	- -threshold  是 $w_0$
	- +1  是  $x_0$
- $h(x) = sign((\sum\limits_{i=0}^dw_ix_i)) = sign(w^Tx)$
- ![Image text](https://raw.githubusercontent.com/burningmysoul2077/Notes/main/ScreenShots/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/Pasted%20image%2020230313091458.png)
- 假设感知机在二维平面上  perceptrons <=> linear(binary) classifiers
  
### Perceptron Learning Algorithm PLA
- Select $g$ from $\cal H$,  $\cal H$ = all possible perceptrons
- Want  $g \approx f$, ideally $g(x_n)=f(x_n)=y_n$ but $\cal H$ is infinite
- Idea:  start from some $g_0(w_0)$, and correct its mistakes on $\cal D$
	- represent $g_0$ by its weight vector $w_0$
- ![[Pasted image 20230313093440.png]]
- PLA步骤：
	- 首先随机选择一条直线进行分类，找到第一个分类错误 $w_t$ 的点 $(x_{n(t)}, y_{n(t)})$
		- 如果这个点表示正类，被误分为负类，即 $w_t^Tx_{n(t)}<0$，那表示 $w$ 和 $x$ 夹角 >90度，其中 $w$ 是直线的法向量
			- 所以，$x$ 被误分在直线的下侧（相对于法向量，法向量的方向即为正类所在的一侧），修正的方法就是使 $w$ 和 $x$ 夹角 <90度。通常做法是 $w\leftarrow w+yx,\ y=1$，如图右上角所示，一次或多次更新后的 $w+yx$ 与 $x$ 夹角 <90度，能保证 $x$ 位于直线的上侧，则对误分为负类的错误点完成了直线修正。
		- 如果是误分为正类的点，即 $w_t^Tx_{n(t)}>0$，那表示 $w$ 和 $x$ 夹角 <90度，其中 $w$ 是直线的法向量
			- 所以，$x$ 被误分在直线的上侧，修正的方法就是使 $w$ 和 $x$ 夹角 >90度。通常做法是$w\leftarrow w+yx,\ y=-1$，如图右下角所示，一次或多次更新后的 $w+yx$ 与 $x$ 夹角 >90度，能保证 $x$ 位于直线的下侧，则对误分为正类的错误点也完成了直线修正。
	- 按照这种思想，遇到个错误点就进行修正，不断迭代 $w_{t+1} \leftarrow w_t + y_{n(t)}x_{x(t)}$, 直到 没有错误，那 last w called $w_{PLA}$ as $g$
	- 注意：每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）
- __Cyclic PLA__  -  实际操作中，可以一个点一个点地遍历，发现分类错误的点就进行修正，直到所有点全部分类正确

### Gurantee of PLA

#### Linear Separability
- 线性可分是PLA使前提条件
- 对于线性可分的情况，就存在这样一条直线，linear separable $\mathcal{D} \Leftrightarrow$ exists perfect $w_f$ s.t. $y_n = sign(w_f^Tx_n)$
	- $w_f$ perfect，代表对于每一个点 $x_n$，$y_{n(t)}w_f^Tx_{n(t)} \geq \mathop{\min}\limits_ny_nw_f^Tx_n> 0$
		- t 代表我们随机选中的某一点
	- by updating with any $(x_{n(t)}, y_{n(t)})$，$w_f^Tw_{t+1} = w_f^T(w_t+y_{n(t)}x_{n(t)})$
		- 根据上面的不等式，$w_f^Tw_{t+1} = w_f^Tw_t + w_f^Ty_{n(t)}x_{n(t)} \geq w_f^Tw_t + \mathop{\min}\limits_ny_nw_f^Tx_n > w_f^Tw_t + 0$
		- PLA会对错误的点进行修正，更新权重 $w_{t+1}$
		- 如果 $w_{t+1}$ 与 $w_f$ 越来越接近，数学运算上就是内积越大，表示 $w_{t+1}$ 是在接近目标权重 $w_f$，证明PLA是有学习效果的
- 以上的推导可以看出，$w_{t+1}$ 与 $w_f$ 的内积跟 $w_t$ 与 $w_f$ 的内积相比更大了。似乎说明了 $w_{t+1}$ 更接近 $w_f$，但是内积更大，可能是向量长度更大了，不一定是向量间角度更小

- Hence 还需要证明 $w_{t+1}$ 与 $w_t$ 向量长度的关系
- $w_t$ changed only when mistake $\Leftrightarrow {sign}(w_t^Tx_{n(t)}) \neq y_{n(t)} \Leftrightarrow y_{n(t)}w_t^Tx_{n(t)} \leq 0$
	- ![[Pasted image 20230314161546.png]]
		- $\mathop{max}\limits_n||y_nx_n||^2$ 是最远的那个点， $y_n$ 是 +1，-1
	- 事实上，$w_t$ 只会在分类错误的情况下update，最终得到的 $||w_{t+1}^2||$ 比 $||w_{t}^2||$ 的增量值 $\leq \mathop{max}\limits_n||x_n||^2$。这证明 $w_t$ 被限制了增长，$w_{t+1}$ 与 $w_t$ 向量长度不会相差很多
	
- __以上总结__:  如果令初始权值 $w_0=0$，那么经过 T 次错误修正后
	 - $\frac{W_f^T}{||w_f||}\frac{w_T}{||w_T||} \geq \sqrt{T} \cdot$ constant
	 - 不等式左边是 $w_T$ 与 $w_f$ 夹角的余弦值，随着 $T$ 增大，该余弦值越来越接近 1，即 $w_T$ 与 $w_f$ 越来越接近。
	 - 同时，$T−−√⋅constant≤1\sqrt T\cdot constant\leq 1$，迭代次数 $T$ 是有上界的
	 - 结论是：$w_{t+1}$ 与 $w_f$ 的是随着迭代次数增加，逐渐接近的。而且，PLA最终会停下来（因为 T 有上界），实现对线性可分的数据集完全分类

- 公式推导
	- $R^2 = \leq \mathop{max}\limits_n||x_n||^2$ - 半径的平方，最大的那个点的平方
	- $\rho = \mathop{\min}\limits_ny_n\frac{w_f^T}{||w_f||}x_n$  -  目标线的法向量跟每个点的内积，根据 y +1-1决定方向，如果线性可分，它一定 > 0
![[Pasted image 20230314165010.png]]
![[Pasted image 20230314165515.png]]
  

### Non-seperable Data
- 线性可分，PLA可以停下来并正确分类
- 非线性可分，$w_f$ 实际上不存在，那么之前的推导并不成立，PLA不一定会停
- 而且，即便PLA会停，但是因为 $\rho \enspace depends \enspace on \enspace w_f$ ，所以怎么停何时停，完全是未知

#### Learning with Noisy Data
- 非线性可分的情况，可以把它当成是数据集 $\cal D$ 中有 noise。机器学习流程如下：
![[Pasted image 20230314170103.png]]

- 在非线性情况下，可以把条件放松，即不苛求每个点都分类正确，而是容忍有错误点，取错误点的个数最少时的权重 $w$ 作为 $w_g$：
![[Pasted image 20230314170215.png]]
- 上面的解是 NP-hard 问题，无解

#### Pocket Algorithm
- 修改在线性可分类型中表现很好的 PLA，然后应用到非线性可分类型中，获得近似最好的 $g$，修改后的 PLA 就是 Packet Algorithm
- 思想是 keeping best weights in pocket
- 算法流程与 PLA 基本类似
	- 首先初始化pocket weights $\hat{w}$
	- 在这条初始化的直线中，随机找到一个错误  $(x_{n(t)}, y_{n(t)})$ 及其 $w_t$ ， 计算其分类错误点的个数
	- 然后对错误点进行修正，更新 $w_{t+1} \leftarrow w_t + y_{n(t)}x_{x(t)}$，得到一条新的直线，在计算其对应的分类错误点的个数，并与之前错误点个数比较，取个数较小的直线作为当前选择的分类直线。
	- 经过 $n$ 次迭代，if $w_{t+1}$ makes fewer mistakes than $\hat{w}$, replace $\hat{w}$ by $w_{t+1}$
	- until enough iterations, return $\hat w$ called $w_{POCKET}$ as $g$

- 如何判断数据集 $\cal D$ 是否线性可分
	- 二维数据，通常还是通过肉眼观察来判断的 

- Pocket Algorithm要比PLA速度慢

## 总结
- 本章介绍了线性感知机模型，以及解决这类感知机分类问题算法 - PLA
- 介绍 PLA 的前提条件 - 线性可分
- 介绍了对于线性可分问题，PLA 的整个推导过程及迭代次数
- 介绍了对于非线性可分问题，当作是训练集中加入了噪声，可以使用 PLA 的修正算法 Pocket Algorithm 
